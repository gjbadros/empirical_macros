%-------------------------------------------------------------------------
%  paper.tex -- The basic outline of the paper, includes all the sections
%
%   $Id$
%-------------------------------------------------------------------------


\documentclass{article}
\usepackage{fullpage}
%\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{personal}

\newcommand{\pcp}{\mbox{\textsf{PCP}$^3$}}
\newcommand{\pcppp}{\mbox{\textsf{PCppP}}}
\newcommand{\Cpp}{\mbox{\textsf{cpp}}}
\newcommand{\CPP}{\mbox{\textsf{C++}}}
\newcommand{\Perl}{\mbox{\textsf{Perl}}}
\newcommand{\C}{\mbox{\textsf{C}}}

\newcommand{\backcall}[3]{\item \texttt{#2} (\texttt{#3})
  \ifthenelse{\equal{#1}{}}{}{\textbf{returns} \texttt{#1}} \\ }

%\newcommand{\backcallobsoleted}[3]{\item \texttt{#2} (\texttt{#3})
%  \textbf{returns} \texttt{#1} \textsc{Obsoleted}\\ }

% Don't even print these out for the paper
\newcommand{\backcallobsoleted}[3]{}

\newcommand{\hook}[2]{\item \texttt{#1} (\texttt{#2}) \\ }

\newcommand{\hookobsoleted}[2]{}

\newcommand{\ppd}[1]{\texttt{\##1}}

\newcommand{\file}[1]{{\small \texttt{#1}}}
\newcommand{\email}[1]{{\small \texttt{#1}}}
\newcommand{\program}[1]{{\small \texttt{#1}}}
\newcommand{\syscall}[1]{{\textsf{#1}}}
\newcommand{\sectionref}[1]{section \ref{#1}, on page \pageref{#1}}
\newcommand{\ie}{i.e.,}
\newcommand{\eg}{e.g.,}
\newcommand{\etc}{etc\.}
\newcommand{\figref}[1]{Figure~\ref{#1}}

\title{\pcp{}: A \C{} Front End for \\ Preprocessor Analysis and Transformation}
\author{Greg J. Badros%
  \thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.}
  }

\begin{document}
\maketitle

\begin{abstract}
\label{sec:abstract}
Though the \C{} preprocessor provides necessary language features, it
does so in a completely unstructured way.  The lexical nature of \Cpp{}
creates numerous problems for software engineers and their tools, all
stemming from the chasm between the engineer's view of the source code
and the compiler's view.  The simplest way to reduce this problem is to
minimize use of the preprocessor.  In light of the data collected in a
prior empirical analysis, this paper considers some simple
transformations on uses of the preprocessor which could be converted
into language features. Existing tools for analyzing \C{} source in the
context of its preprocessor are unsuitable for such transformations, so
this work introduces a new approach: tightly integrating the
preprocessor with a \C{} language parser, permitting the code to be
analyzed at both the preprocessor and syntactic levels simultaneously.
The front-end framework, called \pcp{}, couples arbitrary \Perl{}
subroutine ``hooks'' invoked upon various preprocessor and parser events
and is thus general and flexible. \pcp{}'s strengths and weaknesses are
discussed in the context of several program understanding and
transformation tools, including a conservative analysis to support
replacing \Cpp{}'s \ppd{define} directives with \CPP{} language
features.

\end{abstract}
\bigskip

% Introduction, including problem statement, goals, etc
\section{Introduction}
\label{sec:intro}
More than twenty years ago, Dennis Ritchie designed the \C{} language to
include a textual macro preprocessor called
\Cpp{}~\cite[Ch.~3]{Harbison91}.  Given the simplicity of the language
and the state of the art in compiler technology in the mid-1970s, his
decision to provide some language features in this extra-linguistic tool
may have been justified.  For the last couple of decades, \C{} programs
have exploited \Cpp{}'s capabilities for everything from manifest
constants and type-less pseudo-inline functions, through modularization
and symbol generation.  Bjarne Stroustrup, the designer and original
implementor of \CPP{}, notes that ``without the \C{} preprocessor, \C{}
itself \ldots{} would have been stillborn''~\cite[p.~119]{Stroustrup94}.
Most certainly \Cpp{} contributes greatly to \C{}'s expressiveness and
portability, but perhaps at too large a cost.  Stroustrup recognizes
this tradeoff:

\begin{quotation}
\noindent Occasionally, even the most extreme uses of \Cpp{} are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders~\cite[p.~424]{Stroustrup94}.
\end{quotation}

%% FIXGJB: is \ldots too cute?
\subsection{Why \Cpp{} is good\ldots and bad}

The intrinsic problem with \Cpp{} is also its fundamental strength: the
conceptually distinct first pass of textual processing over \C{} source
code.  This introduces a giant chasm between the code that the
programmer sees and what the compiler-proper (\ie{} the \C{} compiler
separate from the preprocessor) ultimately compiles.\footnote{To avoid
  ambiguity, we will use \emph{preprocessed} to refer to the view of a
  translation unit after running \Cpp{} on it, and \emph{unprocessed} to
  refer to the original source code.} Consider the program in
\figref{fig:badmain} Though a legal ANSI \C{} program, its semantics are
undefined in light of \Cpp{}.  When compiled using \texttt{cc
  -Dprintf(x)=}, the program no longer outputs the familiar ``Hello
world'' message.\footnote{Adapted from an example given by
  Stroustrup~\cite[p.~423]{Stroustrup94}.}

\begin{figure}[hbt]
\label{fig:badmain}
\begin{center}
\begin{pseudocode}
#include <stdio.h>

int main(int argc, char *argv[]) {
  printf("Hello world");
  return 0;
}
\end{pseudocode}
\caption{Example of incomplete semantics of \C{} source.}
\end{center}
\end{figure}

Experienced and novice \C{} programmers alike have been frustrated by
similar misunderstandings of source code due to the arbitrary
transformations \Cpp{} performs before the language compiler ever is
invoked.\footnote{Some more modern \C{} compilers integrate the
  preprocessor with the lexical scanning phase, but, by necessity, it is
  still a separate first phase conceptually.}  Such confusions are
easily eliminated by allowing the software engineer to see the code
exactly as the compiler does.  Unfortunately, that view of the program
is a level of abstraction lower than the unprocessed source provides.
Well-known identifiers such as \texttt{stderr} appear as the far less
readable \texttt{(\_IO\_FILE*)(\&\_IO\_stderr\_)},\footnote{This is the
  output generated when preprocessed using the \texttt{gcc} compiler's
  standard include files.} and useful encapsulations such as
\texttt{assert} degenerate into sequences of symbols which are nonsense
to a human programmer.

\subsection{What this means for software tools}

Though software engineers are rarely encouraged to work directly with
preprocessed code, the majority of software engineering tools operate on
exactly that view of the source.  Debuggers, call graph extractors, data
flow analyzers, ASTLog [[FIXGJB: REF?]], and countless other tools
either run \Cpp{} as the first stage in their analysis, or use
representations derived from a compiler operating on the preprocessed
code.  Though the parser or abstract-syntax-tree based approach these
tools apply results in information that is precise for the input they
consider, their usefulness for human-targeted program understanding is
diminished due to the textual transformations from
preprocessing.\footnote{In contrast, this approach is exactly right for
  the compiler which has little need of preserving high level
  abstractions in generating object code.}  Additionally, parsing
requires a syntactically correct program and all header files to
exist---these constraints are not realistic during many software
maintenance activities (\eg{} porting, switching compilers, \etc{}).
Another significant disadvantage of preprocessing is that it eliminates
conditional compilation artifacts that are essential to the portability
and versatility of the source code (see~\cite{Krone94}).  Preprocessing
forces tools to limit their analysis to a single configuration of the
source code, instead of permitting global reasoning about the entire artifact.

Some tools choose a different tradeoff and operate instead on the
unprocessed source code exactly as the programmer sees it.  These tools
cannot use a straightforward parser or construct an accurate abstract
syntax tree because the source is not syntactically-correct \C{} code.
Lexical tools (\eg{} \texttt{etags}, LLSME~\cite{Murphy95}, and
\texttt{font-lock-mode} for \texttt{Emacs}) and approximate parsers
(\eg{} \texttt{a*} and \texttt{Genoa}~\cite{Griswold96},
\texttt{LCLint}~\cite{LCLint}, \etc{}) use this approach.  In general,
this leads to greater speed (especially for the lexical approach) and
improved robustness to syntax errors and language variants (approximate
parsers generally lose at worst the current top level construct, and
often can do better). Additionally, because the input is unprocessed,
the extracted information is presented to the human software engineer at
the same level of abstraction as the source with which she is working.
Unfortunately, by disregarding (or only partially honoring) the \Cpp{}
directives, the extracted model of the source code can only be an
approximation of the program's appearance to a full compiler.  \Cpp{}
macros can still wreak havoc by hiding arbitrary code in macro
expansions or customizing syntax of declarations or scoping constructs.
Griswold and Atkinson studied mistakes in call graph extraction using
various tools and found that macro expansion was a major cause of both
false positives and false negatives~\cite{Griswold96}.  Such tools
unfortunately are inappropriate for software tools that require exact or
conservative information.

An obvious solution to the problems \Cpp{} presents is to just avoid it,
and therefore \C{}, entirely.  By using a different language which
directly provides features such as modules, constants, inline functions,
generic functions, and other higher level abstractions that \C{}
emulates via its preprocessor, most of the above issues are avoided.
Casting away \C{} in favor of most modern languages would necessitate
discarding billions of lines of useful legacy code.  The one notable
exception is \CPP{}~\cite{CD2DraftStandard}. For compatibility with
\C{}, \CPP{} remains encumbered by \Cpp{}, yet \CPP{} does provide
language-level support for many higher level constructs, thus making
numerous \Cpp{} constructs redundant.\footnote{In fact, this was an
  explicit design goal for Stroustrup~\cite[p.~424]{Stroustrup94}.}
Migrating \C{} code to \CPP{} potentially provides a path to reduce
usage of the preprocessor to the benefit of tools and programmers.
Ideally, these transformations could be assisted by automated tools.

\subsection{Outline}

The following section highlights significant findings from an empirical
study of \Cpp{} use~\cite{EmpCpp-TR} with an emphasis on understanding
what fraction of preprocessor use in existing software artifacts can be
replaced by \CPP{} language features.  Section~\ref{sec:pcp3} describes
the tool the author developed to support accurate analysis of
unprocessed code without losing the high level abstractions expressed
with the preprocessor. Section~\ref{sec:results} illustrates some of the
analyses that \pcp{} does well, and discusses its support for automating
safe conversion of some simple uses of \Cpp{} into \CPP{} language
features. Section~\ref{sec:related} describes related work, and
section~\ref{sec:summary} discusses the contribution of this work, its
shortcomings, and some areas for possible future work.


%% Is C++ expressive enough?
% Background
\section{Feasibility of Transformation}
\label{sec:feasibility}
Many \Cpp{} constructs clearly have analogues in newer language
features.  For example, \ppd{define}s of simple numeric and string
constants can often be replaced with enumeration declarations or static
constant variable declarations (both newer features of ANSI
\C{}, not just of \CPP{}).
%FIXGJB: interesting that this language construct is used so little
Some function-like \ppd{define}s of pseudo-inline functions can
correspond to \CPP{}'s real \texttt{inline} functions (perhaps made
generic through use of a template).  An optimizing compiler performing
trivial dead code elimination can effect the same result as a
conditional compilation directive guarding debug-only code (e.g.,
replacing a syntactically correct block enclosed by \texttt{\ppd{ifndef}
  NDEBUG} and \ppd{endif} pair with an \texttt{if} statement).

%% Types of questions the analysis must support answering


% PCP^3
\section{The \pcp{} Infrastructure}
\label{sec:pcp3}
\pcp{} is built from three software components: a \texttt{P}arser, a \texttt{C}
\texttt{P}re\texttt{p}rocessor, and a \texttt{P}erl action
language.\footnote{Hence its name, \pcppp{}, shortened to \pcp{}.}

\subsection{Parser}

The first major component of \pcp{} is an ANSI \C{} compatible parser.
Choosing a parser was difficult as there are many freely available
parsers, often tightly coupled to their back-end, thus complicating
reuse.  Ultimately, the parser from \texttt{CTree}~\cite{CTree}, a
freely available \C{} front end which creates an abstract syntax tree
from a preprocessed input file, was embedded in \pcp{}.  The
\texttt{CTree} parser simply parses and creates an AST.  Its lexer and
parser both are mechanically generated from
\texttt{flex}~\cite{Flex,Levine92} and
\texttt{bison}~\cite{Bison,Levine92} (freely available implementations
of \texttt{lex} and \texttt{yacc}, respectively) specifications,
respectively.  \texttt{CTree} also implements a simply, but fully scoped
symbol table, relieving \pcp{} of another essential duty.

\subsection{Preprocessor}

As a software tool targeting \C{} code, the design of \pcp{} faced the
same difficulties as other tools as outlined in section~\ref{sec:intro}.
Disregarding the preprocessor is clearly not an option since analyzing
the preprocessor is integral to \pcp{}.  But approximating \Cpp{} is not
good enough either, as it is essential that the tool mimic \Cpp{}
exactly. Thus, the \C{} preprocessing library from the GNU \C{}
compiler's (\texttt{gcc}) well-tested (and slightly extended)
\Cpp{}~\cite{GCC} is embedded in \pcp{}.  By using \texttt{gcc}'s
\texttt{cpplib}, \pcp{} is able to ``see'' both the unprocessed and
the preprocessed views of source code.  Though \Cpp{} itself inserts
\ppd{line} directives in its output to provide a rudimentary mapping
between the two views, \pcp{} keeps a much finer mapping, tracking
what tokens came from source code directly, from expansions of macro
bodies, arguments of macros, \etc.\footnote{The tight mapping is similar
  to the approach the intentional programming group at Microsoft
  Research is using trying to recover preprocessor ``intentions'' from
  fully preprocessed code annotated with this mapping
  information~\cite{MSIPPersonal}.}  By connecting the annotated tokens
and exposing the state of the preprocessor to the parser and vice-versa,
the communication between \Cpp{} and the \C{} language parser is widened
from the thin straw of tokens passed via a Unix pipe that plagues other
tools choosing to operate on preprocessed code.

\subsection{Perl action language}

Griswold and Atkinson note that using a special-purpose interpreted
action language helped the scalability of various software tools in
extracting a call graph~\cite{Griswold96}.  Interpreted languages can
speed development time, especially when prototyping analyses.  For
\pcp{}, \Perl{}~\cite{Perl,Camel} was chosen because it interfaces
easily with \C{}, it is in widespread use, and the author is comfortable
with it.\footnote{Additionally, the tools used in the empirical study of
  \Cpp{} use~\cite{EmpCpp-TR}  are written in \Perl{}, thus
  presenting yet another opportunity for reuse.}

The \Perl{} interface of \pcp{} is split into two parts:

%%FIXGJB: did the numbers of hooks/backcalls change?
\begin{itemize}
\item Action ``hooks'' written in \Perl{} that \C{} code in \pcp{}
      invoke on various occurrences. Each hook is directly passed (via
      normal \Perl{} passing conventions) a set of parameters relevant
      to the current action.  Example actions include the scanning of
      preprocessor directives, the creation of a macro definition, the
      expansion (\ie{} use) of a macro name, and the parsing of a
      variable declaration.  There are forty-one hooks presently
      specified in \pcp{}. See \sectionref{sec:hooks} for details.
\item Subroutine ``backcalls'' written in \C{} (actually,
      \texttt{PerlXS} the dialect of \C{} used for \Perl{} extensions)
      that the \Perl{} hooks are free to call to access pre-specified
      \C{} data structures or actively interact with the Parser or
      \Cpp{} components of \pcp{}.  Example subroutines include getting
      the name of the currently-processed file, inserting arbitrary code
      for the preprocessor to parse, and instructing the parser to enter
      a new scope.  There are twenty-six backcalls presently permitted
      by \pcp{}.  See \sectionref{sec:backcalls} for details.
\end{itemize}

\noindent The \Perl{} hooks are read from a user-specifiable script
file.  That script is free to manage its local data structures
arbitrarily, and may import modules as an ordinary \Perl{} program
would.  However, the \C{} data structures are protected behind the hooks
and backcalls interfaces.

\subsection{Macro expansion mappings}
It is essential that \pcp{} maintain an accurate mapping between the
unprocessed source and the preprocessed source.  Macro expansions are
the most complicated aspect of this correspondence.  Macro arguments can
themselves be macros, and macros can expand to macros whose expansions
include other macros in need of expansion.\footnote{In ANSI \C{}'s
  preprocessor, recursion is prohibited; as a macro name is expanded,
  that name is disabled from future expansions generated by the original
  expansion.}  To effectively exploit the macro expansion hooks, the
details of the expanding and substituting process must be understood.
For \texttt{gcc}'s preprocessor, and consequently for \pcp{}, macro
expansion takes place as follows:\footnote{These list is necessarily
  implementation specific.  See \cite[Ch.~3]{Harbison91} for details of
  what is required by the standard}

\begin{enumerate}
\item The macro name is checked to see which arguments it uses.
\item Those arguments that are used in the expansion are expanded if and
      only if they contain macros.  Macros appearing in the expansions
      of the arguments do not get expanded yet--only one level of
      expansion is performed. Identical arguments are expanded
      independently; for example \texttt{FOO(BAR,BAR)} will expand
      \texttt{BAR} twice if the expansion of \texttt{FOO} uses both of
      its arguments.  They are expanded in the order that they are used
      in the body of the expansion, not left to right.
\item The body of the top level macro is copied left to right; arguments
      are replaced with the text from their expansions.  Macro names
      previously expanded are ``escaped'' (using a prefix of the
      distinguished symbols ``\texttt{@-}'') in this pseudo input buffer
      to prevent recursion
\item That entire text is rescanned, and un-escaped macros are expanded
      further.
\end{enumerate}

\noindent The \texttt{EXPAND\_MACRO} hook is called for each macro name as it is
expanded.  The parameters to the hook include the exact location of the
start and end of the macro invocation in the source code (or the
top-level macro which resulted in the expansion if the invocation does
not directly appear in the source).  Other parameters describe the
``nesting'' of the expansion, and a backcall
\texttt{MacroExpansionHistory} describes the current history of
expansions.  The nesting of an expansion is the trail through arguments
of other macros that led to this expansion, while the history expansion
is a list of macros that were expanded and led to this macro being expanded.

\begin{figure}[p]
  \begin{center}
    \leavevmode
    \includegraphics[width=0.5\linewidth]{figs/tree-expn.eps}
    \label{fig:tree-expn}
    \caption{Post-order traversal of an example macro expansion and the
      hooks that are called. Blue numbers are \texttt{EXPAND\_MACRO}
      actions, green numbers are \texttt{MACRO\_CLEANUP} actions; \eg{}
      the fifth hook \pcp{} invokes is \texttt{MACRO\_CLEANUP} for
        ``\texttt{FOO}''.  Note that between actions 11 and 12, numerous
        \texttt{TOKEN} actions occur.  In general, the tree need not be
      binary, but note that the leaves are the arguments in the order of
      appearance in expansion (not their order in the formals list).}
  \end{center}
\end{figure}

\begin{figure}[p]
  \begin{center}
    \leavevmode
    \includegraphics[width=0.5\linewidth]{figs/text-expn.eps}
    \label{fig:text-expn}
    \caption{Another view of an example macro expansion and the hooks
    that are called.  Red dotted lines are the initial consideration of
    text possibly subject to expansion.  Blue lines are
    \texttt{EXPAND\_MACRO} actions on the underline macro invocation,
    green lines are \texttt{MACRO\_CLEANUP}.}
  \end{center}
\end{figure}

Consider the example illustrated in figures \ref{fig:tree-expn} and
\ref{fig:text-expn}.  When the \texttt{MACRO\_EXPAND} hook is called for
\texttt{FOO} (marked with a asterisk star in the figures), we have:

\begin{verbatim}
   @nests                   == ( TA3#1; TA4#1 )
   MacroExpansionHistory()  == ( PA2#Body )
\end{verbatim}

\noindent The \texttt{@nests} tells us we are expanding the first
argument of macro \texttt{TA3}, which itself was the first argument of
macro \texttt{TA4}.  The \texttt{MacroExpansionHistory()} backcall
provides the remaining information about this expansion: that the
expansion came from the body of the earlier expansion of macro
\texttt{PA2}.

% Emacs support
\subsection{Textual annotations database in Emacs}

\subsection{Interactions among levels}

\subsection{Performance}

\section{Automated removal of \ppd{define}s of constants}
\label{sec:results}

% Summary & Future work
\section{Related Work}
\label{sec:related}

% Summary & Future work
\section{Summary and Future Work}
\label{sec:summary}

\appendix
\newpage

%%FIXGJB: order and organize these
\section{Action hooks called by \pcp}
\begin{footnotesize}
\label{sec:hooks}
\begin{itemize}
\sloppy
\input{pcp3-hooks.tex}
\fussy
\end{itemize}
\end{footnotesize}

\newpage

%%FIXGJB: order and organize these
\section{Subroutines provided by \pcp}
\label{sec:backcalls}
\begin{footnotesize}
\begin{itemize}
\sloppy
\input{backcalls.tex}
\fussy
\end{itemize}
\end{footnotesize}

% References
\newpage

\nocite{ARM}
\nocite{Dragon}
\nocite{Glickstein97} % Writing GNU Emacs Ext.
\nocite{Camel}        % Perl 5
\nocite{Perl}        % Perl 5
\nocite{Levine92}     % Lex & Yacc
\nocite{Harbison91}   % C Ref Man
\nocite{Stroustrup97} % C++, 3rd
\nocite{Stroustrup94} % C++, 2nd
\nocite{Kernighan88}  % C, 2nd
\nocite{Flanagan96}   % Java in a Nutshell
\nocite{BtYACC}
\nocite{EmpCpp-TR}
\nocite{GCC}
\nocite{CTree}
\nocite{TXL}
\nocite{Cordy92}
\nocite{BtYACC}
\nocite{Bison}
\nocite{Flex}
\nocite{Krone94}
\nocite{Griswold96}
\nocite{Atkinson96}
\nocite{CD2DraftStandard}

\bibliographystyle{alpha}
\bibliography{library,articles,pcp3}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
