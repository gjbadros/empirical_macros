%-------------------------------------------------------------------------
%  paper.tex -- The basic outline of the paper, includes all the sections
%
%   $Id$
%-------------------------------------------------------------------------


\documentclass{article}
\usepackage{fullpage}
%\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{personal}

\newcommand{\pcp}{\mbox{\textsf{PCp}$^3$}}
\newcommand{\pcppp}{\mbox{\textsf{PCppP}}}
\newcommand{\Cpp}{\mbox{\textsf{cpp}}}
\newcommand{\CPP}{\mbox{\textsf{C++}}}
\newcommand{\Perl}{\mbox{\textsf{Perl}}}
\newcommand{\C}{\mbox{\textsf{C}}}

\newcommand{\backcall}[3]{\item \texttt{#2} (\texttt{#3})
  \ifthenelse{\equal{#1}{}}{}{\textbf{returns} \texttt{#1}} \\ }

%\newcommand{\backcallobsoleted}[3]{\item \texttt{#2} (\texttt{#3})
%  \textbf{returns} \texttt{#1} \textsc{Obsoleted}\\ }

% Don't even print these out for the paper
\newcommand{\backcallobsoleted}[3]{}

\newcommand{\hook}[2]{\item \texttt{#1} (\texttt{#2}) \\ }

\newcommand{\hookobsoleted}[2]{}
\newcommand{\pphash}{\texttt{\#}}

\newcommand{\ppd}[1]{\texttt{\##1}}

\newcommand{\file}[1]{{\small \texttt{#1}}}
\newcommand{\email}[1]{{\small \texttt{#1}}}
\newcommand{\program}[1]{{\small \texttt{#1}}}
\newcommand{\syscall}[1]{{\textsf{#1}}}
\newcommand{\sectionref}[1]{section \ref{#1}, on page \pageref{#1}}
\newcommand{\appendixref}[1]{appendix \ref{#1}, on page \pageref{#1}}
\newcommand{\ie}{i.e.,}
\newcommand{\eg}{e.g.,}
\newcommand{\etc}{etc}  % trailing ".\ " needed at use; this just for
                        % italicizing optionally
\newcommand{\figref}[1]{Figure~\ref{#1}}
\setlength{\fboxsep}{.1in}

\title{\pcp{}: A \C{} Front End for \\ Preprocessor Analysis and Transformation}
\author{Greg J. Badros%
  \thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.}
  \\  \email{gjb@cs.washington.edu}}

\date{13 October 1997}


\begin{document}
\maketitle

\begin{abstract}
\label{sec:abstract}
Though the \C{} preprocessor provides necessary language features, it
does so in a completely unstructured way.  The lexical nature of \Cpp{}
creates numerous problems for software engineers and their tools, all
stemming from the chasm between the engineer's view of the source code
and the compiler's view.  The simplest way to reduce this problem is to
minimize use of the preprocessor.  In light of the data collected in a
prior empirical analysis, this paper considers some simple
transformations on uses of the preprocessor which could be converted
into language features. Existing tools for analyzing \C{} source in the
context of its preprocessor are unsuitable for such transformations, so
this work introduces a new approach: tightly integrating the
preprocessor with a \C{} language parser, permitting the code to be
analyzed at both the preprocessor and syntactic levels simultaneously.
The front-end framework, called \pcp{}, couples arbitrary \Perl{}
subroutine ``hooks'' invoked upon various preprocessor and parser events
and is thus general and flexible. \pcp{}'s strengths and weaknesses are
discussed in the context of several program understanding and
transformation tools, including a conservative analysis to support
replacing \Cpp{}'s \ppd{define} directives with \CPP{} language
features.

\end{abstract}
\bigskip

% Introduction, including problem statement, goals, etc
\section{Introduction}
\label{sec:intro}
More than twenty years ago, Dennis Ritchie designed the \C{} language to
include a textual macro preprocessor called
\Cpp{}~\cite[Ch.~3]{Harbison91}.  Given the simplicity of the language
and the state of the art in compiler technology in the mid-1970s, his
decision to provide some language features in this extra-linguistic tool
may have been justified.  For the last couple of decades, \C{} programs
have exploited \Cpp{}'s capabilities for everything from manifest
constants and type-less pseudo-inline functions, through modularization
and symbol generation.  Bjarne Stroustrup, the designer and original
implementor of \CPP{}, notes that ``without the \C{} preprocessor, \C{}
itself \ldots{} would have been stillborn''~\cite[p.~119]{Stroustrup94}.
Most certainly \Cpp{} contributes greatly to \C{}'s expressiveness and
portability, but perhaps at too large a cost.  Stroustrup recognizes
this tradeoff:

\begin{quotation}
\noindent Occasionally, even the most extreme uses of \Cpp{} are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders~\cite[p.~424]{Stroustrup94}.
\end{quotation}

%% FIXGJB: is \ldots too cute?
\subsection{Why \Cpp{} is good\ldots and bad}

The intrinsic problem with \Cpp{} is also its fundamental strength: the
conceptually distinct first pass of textual processing over \C{} source
code.  This introduces a giant chasm between the code that the
programmer sees and what the compiler-proper (\ie{} the \C{} compiler
separate from the preprocessor) ultimately compiles.\footnote{To avoid
  ambiguity, we will use \emph{preprocessed} to refer to the view of a
  translation unit after running \Cpp{} on it, and \emph{unprocessed} to
  refer to the original source code.} Consider the program in
\figref{fig:badmain} Though a legal ANSI \C{} program, its semantics are
undefined in light of \Cpp{}.  When compiled using \texttt{cc
  -Dprintf(x)=}, the program no longer outputs the familiar ``Hello
world'' message.\footnote{Adapted from an example given by
  Stroustrup~\cite[p.~423]{Stroustrup94}.}  What \emph{you} see is not what the
\emph{compiler} gets.

\begin{figure}[hbt]
\begin{center}
\begin{small}
\begin{pseudocode}[3in]
#include <stdio.h>

int main(int argc, char *argv[]) {
  printf("Hello world");
  return 0;
}
\end{pseudocode}
\end{small}
\caption{Example of incomplete semantics of \C{} source.}
\label{fig:badmain}
\end{center}
\end{figure}

Experienced and novice \C{} programmers alike have been frustrated by
similar misunderstandings of source code due to the arbitrary
transformations \Cpp{} performs before the language compiler ever is
invoked.\footnote{Some more modern \C{} compilers integrate the
  preprocessor with the lexical scanning phase, but, by necessity, it is
  still a separate first phase conceptually.}  Such confusions are
easily eliminated by allowing the software engineer to see the code
exactly as the compiler does.  Unfortunately, that view of the program
is a level of abstraction lower than the unprocessed source provides.
Well-known identifiers such as \texttt{stderr} appear as the far less
readable \texttt{(\_IO\_FILE*)(\&\_IO\_stderr\_)},\footnote{This is the
  output generated when preprocessed using the \texttt{gcc} compiler's
  standard include files.} and useful encapsulations such as
\texttt{assert} degenerate into sequences of symbols which are nonsense
to a human programmer.

%Unlike syntax-based macros~\cite{Weise93} which operate at the parser
%level, \Cpp{} provides macros and conditional inclusion mechanisms which
%operate at the lexical level.  The \C{} preprocessor also includes
%``stringization'' and pasting operators which manipulate symbols
%directly, altering their ultimate interpretation by a parser.  

\subsection{What this means for software tools}

Because of the preprocessor's textual foundations, \C{} source code
cannot be parsed directly.  Only after preprocessing is a correct \C{}
program in a syntactically usable form. Because of the relative ease in
parsing preprocessed code, the majority of software engineering tools
operate on exactly that view of the source, losing information by
disregarding the preprocessor directives.  Various tools including
source-level debuggers, ASTLOG~\cite{Crew97}, TXL~\cite{TXL}, and
countless others, either run \Cpp{} as the first stage in their
analysis, or use representations derived from a compiler operating on
the preprocessed code.  Though the parser or abstract-syntax-tree based
approach these tools apply results in information that is precise for
the input they consider, their usefulness for human-targeted program
understanding is diminished due to changes in the source artifact
resulting from preprocessing.\footnote{In contrast, this approach is
  exactly right for the compiler which has little need of preserving
  high level abstractions in generating object code.  However, in the
  presence of incorrect source code, the compiler itself is a
  program-understanding tool and preprocessing hinders its ability to
  accurately describe the problem.  The \ppd{line} directive is used to
  keep some rudimentary (but far too coarse) mapping to the original
  source to permit more reasonable errors and warnings.}  Additionally,
parsing requires a syntactically correct program and all header files to
exist---these constraints are not realistic during many software
maintenance activities (\eg{} porting, switching compilers, \etc.\ 
Another significant disadvantage of preprocessing is that it eliminates
conditional compilation artifacts that are essential to the portability
and versatility of the source code (see~\cite{Krone94}).  Preprocessing
forces tools to limit their analysis to a single configuration of the
source code, instead of permitting global reasoning about the entire
artifact.  Syntax errors can lurk in code hidden from the compiler by
\Cpp{} (in fact we found instances of this in \cite{EmpCpp}).

Some tools choose a different tradeoff and operate instead on the
unprocessed source code exactly as the programmer sees it.  These tools
cannot use a straightforward parser or construct an accurate abstract
syntax tree because the source is not syntactically-correct \C{} code.
Lexical tools (\eg{} \texttt{etags}, LLSME~\cite{Murphy95}, and
\texttt{font-lock-mode} for \texttt{Emacs}) and approximate parsers
(\eg{} \texttt{a*} and \texttt{Genoa}~\cite{Devanbu92},
\texttt{LCLint}~\cite{LCLint}, \etc.) use this approach.  In general,
this leads to greater speed (especially for the lexical approach) and
improved robustness to syntax errors and language variants (approximate
parsers generally lose at worst the current top level construct, and
often can do better). Additionally, because the input is unprocessed,
the extracted information is presented to the human software engineer at
the same level of abstraction as the source with which she is working.
Unfortunately, by disregarding (or only partially honoring) the \Cpp{}
directives, the extracted model of the source code can only be an
approximation of the program's appearance to a full compiler.  \Cpp{}
macros can still wreak havoc by hiding arbitrary code in macro
expansions or customizing syntax of declarations or scoping constructs.
Griswold and Atkinson studied mistakes in call graph extraction using
various tools and found that macro expansion was a major cause of both
false positives and false negatives~\cite{Griswold96}.  Such tools
unfortunately are inappropriate for software tools that require exact or
conservative information.

An obvious solution to the problems \Cpp{} presents is to just avoid it,
and therefore \C{}, entirely.  By using a different language which
directly provides features such as modules, constants, inline functions,
generic functions, and other higher level abstractions that \C{}
emulates via its preprocessor, most of the above issues are avoided.
Casting away \C{} in favor of most modern languages would necessitate
discarding billions of lines of useful legacy code.  The one notable
exception is \CPP{}~\cite{CD2DraftStandard}. For compatibility with
\C{}, \CPP{} remains encumbered by \Cpp{}, yet \CPP{} does provide
language-level support for many higher level constructs, thus making
numerous \Cpp{} constructs redundant.\footnote{In fact, this was an
  explicit design goal for Stroustrup~\cite[p.~424]{Stroustrup94}.}
Migrating \C{} code to \CPP{} potentially provides a path to reduce
usage of the preprocessor to the benefit of tools and programmers.  A
fundamental motivator in the design of \pcp{} is to automatically assist
in such transformations.

\subsection{Outline}

Section~\ref{sec:feasibility} highlights significant findings from an
empirical study of \Cpp{} use~\cite{EmpCpp} with an emphasis on
understanding what fraction of preprocessor use in existing software
artifacts can be replaced by \CPP{} language features.
Section~\ref{sec:pcp3} describes the tool the author developed to
support accurate analysis of unprocessed code without losing the high
level abstractions expressed with the preprocessor.  The two appendices
provide more details about the \pcp{} interfaces.
Section~\ref{sec:xform} discusses using the tool to assist automating
safe conversion of some simple uses of \Cpp{} into language constructs.
Section~\ref{sec:related} describes related work, and
section~\ref{sec:summary} discusses the contribution of this work, its
shortcomings, and some areas for possible future work.


%% Is C++ expressive enough?
% Background
\section{How pervasive and perverse is preprocessor use?}
\label{sec:feasibility}
Many \Cpp{} constructs clearly have analogues in newer language
features.  Carrol and Ellis list several preprocessor uses and explain
what \CPP{} features could be used instead~\cite{Carroll95}. For
example, \ppd{define}s of simple numeric and string constants can often
be replaced with enumeration declarations or static constant variable
declarations (both newer features of ANSI \C{}, not just of \CPP{}).
Some function-like \ppd{define}s of pseudo-inline functions can
correspond to \CPP{}'s real \texttt{inline} functions (perhaps made
generic through use of a template).  An optimizing compiler performing
trivial dead code elimination can effect the same result as a
conditional compilation directive guarding debug-only code (e.g.,
replacing a syntactically correct block enclosed by \texttt{\ppd{ifndef}
  NDEBUG} and \ppd{endif} pair with an \texttt{if} statement).

Over the last year, several colleagues and the author have investigated
how the \C{} preprocessor is used in a sample of 30 freely available
software packages~\cite{EmpCpp}.  Our findings show that the
preprocessor is used very heavily---almost 10\% of the
lines\footnote{Counts of lines always exclude lines which are blank or
  contain only comments, unless otherwise noted.} in the packages
analyzed are preprocessor directives.  About a third of these are macro
definitions, a bit more than a third are conditional compilation
directives, and the remainder are mostly \ppd{include}s.  Source code
lines can also rely on the preprocessor by having their inclusion be
controlled by a macro (\eg{} if the line is guarded by a conditional
compilation directive) or by containing macros which will be expanded.
About 28\% of lines expand one or more macros, and over 40\% of lines have some
dependence on the preprocessor.  These numbers confirm that \C{}
preprocessor is used extensively.

Though \Cpp{} is definitely used a great deal, not all uses of the
preprocessor are equally disorienting to tools and programmers.  For
example, a fairly common idiom uses a \texttt{\_\_P} macro to remove
function prototypes for non-ANSI \C{} compilers (see
figure~\ref{fig:prototype_example}).  Although 50\% of macro names are
used only twice or fewer (over 10\% are never used), this macro is used
frequently and can cause a parse problem or a lexical mismatch at each
function prototype.  The declaration no longer appears to be a function
prototype to tools operating on unprocessed source code.  As this is a
common case, tools can try to hack around the problem in a
package-dependent way (\eg{} the macro is not always called \texttt{\_\_P}).

%% Emacs font-lock-mode actually gets it wrong

\begin{figure}[hbt]
\begin{center}
\begin{small}
\begin{pseudocode}[4in]
#if defined (__STDC__)
#  if !defined (__P)
#    define __P(protos) protos
#  endif
#else /* !__STDC__ */
#  if !defined (__P)
#    define __P(protos) ()
#  endif
#endif

/* Below has no prototype for non-ANSI compilers */
int FooFunc __P((int i, char *sz));
\end{pseudocode}
\end{small}
\caption{Common use of macro for backward compatibility with non-ANSI
  \C{} compilers.  Note the double parenthesization of the argument to
  macro \texttt{\_\_P}; this permits in the entire formal parameter list
  to be used as the single argument to the macro. This example is
  adapted from code in the \texttt{bash} package.}
\label{fig:prototype_example}
\end{center}
\end{figure}

Because of the generality of the macro expansion mechanism, the problems
can be much worse.  A macro can expand to mismatched parentheses or
braces, or arbitrary symbols such as a semicolon.  They can even
manipulate their arguments as symbols using the \Cpp{} stringization
(\texttt{\#}) and pasting (\texttt{\#\#}) operators.
Figure~\ref{fig:worstcase} shows how damaging overuse of such features
can be to human program-understanding.  Also, approximate parsers operating on
the unprocessed code would have to expand many macros in order to make
sense of the code.

\begin{figure}[hbt]
\begin{center}
\begin{small}
\begin{pseudocode}[5.5in]
#define START_PARAMS (
#define END_PARAMS )
#define START_CALL (
#define END_CALL )
#define BEGIN {
#define END }
#define ARRAY []
#define NL ;
#define POINTER *
#define COMMA ,
#define STRING(x) #x

#include <stdio.h>

int main START_PARAMS int argc COMMA char POINTER argv ARRAY END_PARAMS BEGIN
  printf START_CALL STRING(Hello world\n) END_CALL NL
  return 0 NL
END
\end{pseudocode}
\end{small}
\caption{Legal ANSI \C{} program which is meaningful without
  preprocessing.  The question is: do people write code like this?  The
  answer, generally, is ``no.''}
\label{fig:worstcase}
\end{center}
\end{figure}

However, the good news from our study of how programmers actually use
\Cpp{} is that macros are often used in benign ways.  Over 43\% of macro
names simply expand to constants, and another 31\% expand to
expressions.  These types of macros generally provide few problems to
software tools using unprocessed source since unexpanded macros simply
appear to such tools as identifiers which are often transparently
replaceable by more general expressions.  Another 7\% are ``null
defines''---macros whose expansions are empty, usually used either in
conditional compilation directive guards (\eg{} NDEBUG), or as a means
of removing a keyword that might be unrecognizable to an older compiler
(\eg{} \_\_const).  The remaining 20\% of macro names expand to statements
(4\%), arbitrary symbols (10\%), involve types (1.4\%), or could not be
fit into any of the other categories.  Macro names that affect syntax in
unusual ways such as containing only punctuation or mismatched
parenthesis or braces account for only 0.3\% of the noise.

Unfortunately, there is some bad news: the syntactic and type-related
macros are used significantly more frequently than macro names with more
simple expansions.  While 90\% of all macro names are expanded twenty or
fewer times, only 55\% of the syntactic macros have that few uses.  In
fact, 15\% of them have 160 or more uses, each of which could cause
problems for a software tool operating on unprocessed source and not
expanding macros.\footnote{As previously mentioned, some of these reflect
  well-understood idioms that tools would be wise to handle specially.}

%% Types of questions the analysis must support answering
The tool used to analyze the packages approximates a parse of the
unprocessed source code of the entire package.  It makes multiple passes
over each file, and assumes that a \ppd{define} line occurring anywhere
in the source code will result in that identifier expanding anywhere
else in the source.  Such assumptions are wholly inappropriate 

% PCP^3
\section{The \pcp{} Infrastructure}
\label{sec:pcp3}
\pcp{} is built from three software components: a \textbf{\textsf{P}}arser, a \textbf{\textsf{C}}
\textbf{\textsf{p}}re\textbf{\textsf{p}}rocessor, and a \textbf{\textsf{P}}erl action
language.\footnote{Hence its name, \pcppp{}, shortened to \pcp{}.}
These subsystems interact through well-defined interfaces, and together
result in a powerful, expressive, flexible, and accurate framework for
analyzing \C{} code in the context of its \Cpp{} directives.

\subsection{Parser}

The first major component of \pcp{} is an ANSI \C{} compatible parser.
Choosing a parser was difficult as there are many freely available
parsers, often tightly coupled to their back-end, thus complicating
reuse.  Ultimately, the parser from \texttt{CTree}~\cite{CTree}, a
freely available \C{} front end which creates an abstract syntax tree
from a preprocessed input file, was embedded in \pcp{}.  The
\texttt{CTree} parser simply parses and creates an AST.  Its lexical analyzer and
parser both are mechanically generated from
\texttt{flex}~\cite{Flex,Levine92} and
\texttt{bison}~\cite{Bison,Levine92} (freely available implementations
of \texttt{lex} and \texttt{yacc}, respectively) specifications,
respectively.  \texttt{CTree} also implements a simply, but fully scoped
symbol table, relieving \pcp{} of another essential duty.

The implementation of the \texttt{CTree} parser component of \pcp{}
remains at about 5,000 lines of \C{} code and \texttt{Bison} and
\texttt{Flex} specifications.  Most of the changes to the parser were to
eliminate name conflicts\footnote{Ironically the preprocessor came to the
  rescue here; macros provided an easy way to rename \texttt{CTree}'s
  symbols from its automatically-generated parsers and lexical analyzer
  which conflicted with those used when embedding \Perl{}.}  and
introduce calls to the various action routines.

\subsection{Preprocessor}

As a software tool targeting \C{} code, the design of \pcp{} faced the
same difficulties as other tools as outlined in section~\ref{sec:intro}.
Disregarding the preprocessor is clearly not an option since analyzing
the preprocessor is integral to \pcp{}.  But approximating \Cpp{} is not
good enough either, as it is essential that the tool mimic \Cpp{}
exactly. Thus, the \C{} preprocessing library from the GNU \C{}
compiler's (\texttt{gcc}) well-tested (and slightly extended)
\Cpp{}~\cite{GCC} is embedded in \pcp{}.  By using \texttt{gcc}'s
\texttt{cpplib}, \pcp{} is able to ``see'' both the unprocessed and
the preprocessed views of source code.  Though \Cpp{} itself inserts
\ppd{line} directives in its output to provide a rudimentary mapping
between the two views, \pcp{} keeps a much finer mapping, tracking
what tokens came from source code directly, from expansions of macro
bodies, arguments of macros, \etc.\footnote{The tight mapping is similar
  to the approach the intentional programming group at Microsoft
  Research is using trying to recover preprocessor ``intentions'' from
  fully preprocessed code annotated with this mapping
  information~\cite{MSIPPersonal}.}  By connecting the annotated tokens
and exposing the state of the preprocessor to the parser and vice-versa,
the communication between \Cpp{} and the \C{} language parser is widened
from the thin straw of tokens passed via a Unix pipe that plagues other
tools choosing to operate on preprocessed code.

The implementation of \texttt{cpplib} grew from about 7,000 lines of
code as distributed with \texttt{gcc} to almost 8,000 lines.  Most of
the changes involved modifying data structures and function calls to
maintain the extra macro expansion information which maintains the
tighter correspondence between source and output.  Calls to the various
hooks were also added.

\subsection{Perl action language}

Griswold and Atkinson note that using a special-purpose interpreted
action language helped the scalability of various software tools in
extracting a call graph~\cite{Griswold96}.  Interpreted languages can
speed development time, especially when prototyping analyses.  Since the
rest of the framework is written in \C{}, some higher-level language was
needed to provide support for user-programmable custom analyses.
\Perl{}~\cite{Perl,Camel} was chosen because it interfaces easily with
\C{}, it is in widespread use, and is familiar to the author. \Perl{} is
compiled, though the compilation phase is exceedingly fast and
integrated with the execution; thus it has the same advantages in
reducing developer time as interpreted languages, yet its performance is
substantially better.  Additionally, we wrote most of the tools used in
the empirical study of \C{} preprocessor use~\cite{EmpCpp} in
\Perl{}, permitting possible reuse.

The \Perl{} interface of \pcp{} is composed of two parts:

%%FIXGJB: did the numbers of hooks/backcalls change?
\begin{itemize}
\item Action ``hooks'' written in \Perl{} that \C{} code in \pcp{}
      invoke on various events. Each hook is directly passed (via
      normal \Perl{} passing conventions) a set of parameters relevant
      to the current action.  Example actions include the scanning of
      preprocessor directives, the creation of a macro definition, the
      expansion (\ie{} use) of a macro name, and the parsing of a
      variable declaration.  There are forty hooks presently
      specified in \pcp{}. See \appendixref{sec:hooks} for details.
\item Subroutine ``backcalls'' written in \C{} (actually,
      \texttt{PerlXS} the dialect of \C{} used for \Perl{} extensions)
      that the \Perl{} hooks are free to call to access pre-specified
      \C{} data structures or actively interact with the Parser or
      \Cpp{} components of \pcp{}.  Example subroutines include getting
      the name of the currently-processed file, inserting arbitrary code
      for the preprocessor to parse, and instructing the parser to enter
      a new scope.  There are twenty-six backcalls presently permitted
      by \pcp{}.  See \appendixref{sec:backcalls} for details.
\end{itemize}

\noindent The \Perl{} subroutine hooks are written in a user-specifiable script
file which registers a subroutine for each action it wants to process.
That script is free to manage its local data structures arbitrarily, and
may import modules as an ordinary \Perl{} program would.  However, the
\C{} data structures are protected behind the hooks and backcalls
interfaces. All command line options accepted by \Cpp{} are also
accepted by \pcp{}.  Additionally, \pcp{} accepts a \texttt{--noparse}
option which turns off its parser and the calls to related hooks
(parsing-related backcalls are permitted but generally do nothing).  As
a convenience, \pcp{} also provides \Perl{} module containing various
useful (but optional) utility functions (\eg{} the \texttt{AddHook}
subroutine which manipulates the distinguished global hash dictionary
that \pcp{} consults to call the appropriate subroutine hooks for
various events).

The implementation of the main \pcp{} program, the backcalls, and the
``glue'' connecting the components totals about 1,800 lines of \C{}
code.  About 60\% of this code deals directly with passing arguments
between \C{} and \Perl{}.

\subsection{Macro expansion mappings}
To support useful interactions between the parser and preprocessor, it
is essential that \pcp{} maintain an accurate mapping between the
unprocessed source and the preprocessed source.  Macro expansions are
the most complicated aspect of this correspondence.  Macro arguments can
themselves be macros, and macros can expand to other macros in need of
expansion.\footnote{In ANSI \C{}'s preprocessor, recursion is
  prohibited; as a macro name is expanded, that name is disabled from
  future expansions generated by the original expansion.}  To
effectively exploit the macro expansion hooks, the details of the
expanding and substituting process must be understood.  For
\texttt{gcc}'s preprocessor, and consequently for \pcp{}, macro
expansion takes place as follows:\footnote{This description is necessarily
  implementation specific.  See \cite[Ch.~3]{Harbison91} for details of
  what is required by the \C{} language standard.  Also note that
  details of stringization and pasting are omitted as they are
  infrequently used features~\cite{EmpCpp}.}

\begin{enumerate}
\item The macro definition's body is checked to see which arguments it uses.
\item Those actual arguments that are used in the definition body are
      expanded if and only if they contain macros.  They are expanded
      completely (\ie{} macros in their expansions are expanded), and
      the text of the expansion is saved.  Identical arguments are
      expanded independently; for example \texttt{FOO(BAR,BAR)} will
      expand \texttt{BAR} twice if the expansion of \texttt{FOO} uses
      both of its arguments.  They are expanded in the order that they
      are used in the body of the expansion (not from first formal
      argument to last).
\item The body of the top level macro is copied left to right; arguments
      are replaced with the text from their expansions.  Macro names
      previously expanded are ``escaped'' (using a prefix of the
      distinguished symbols ``\texttt{@-}'') in this pseudo input buffer
      to prevent recursion.
\item That entire text is rescanned, and un-escaped macros are expanded
      further.
\end{enumerate}

\noindent The \texttt{EXPAND\_MACRO} hook is called for each macro name as it is
expanded.  The parameters to the hook include the exact location of the
start and end of the macro invocation in the source code.\footnote{Or,
  if the invocation does not directly appear in the source (\ie{} the
  macro appears in the expansion of another macro), the location is an
  offset within the prior macro's expansion.}  Other parameters
describe the ``nesting'' of the expansion, and a backcall
\texttt{Macro\-Expansion\-History()} describes the current history of
expansions.  The nesting of an expansion is the trail through arguments
of other macros that led to this expansion, while the history expansion
is a list of macros that were expanded en route to this macro being
expanded.

\begin{figure}[p]
  \begin{center}
    \leavevmode
    \includegraphics[width=0.45\linewidth]{figs/tree-expn.eps}
    \caption{Post-order traversal of an example macro expansion and the
      hooks that are called. Blue numbers are \texttt{EXPAND\_MACRO}
      actions, green numbers are \texttt{MACRO\_CLEANUP} actions; \eg{}
      the fifth hook \pcp{} invokes is \texttt{MACRO\_CLEANUP} for
      ``\texttt{FOO}''.  The ``cleanup'' means that the macro has been
      fully expanded and is ready to be substituted into the output text
      (or parsed by \pcp{}). Note that between actions 11 and 12, numerous
      \texttt{TOKEN} actions occur.  In general, the tree need not be
      binary, but note that the leaves of a node are the arguments in
      the order of appearance in that node's macro expansion (not their
      order in the list of formal parameters).}
    \label{fig:tree-expn}
  \end{center}
\end{figure}

\begin{figure}[p]
  \begin{center}
    \leavevmode
    \includegraphics[width=0.45\linewidth]{figs/text-expn.eps}
    \caption{Another view of an example macro expansion and the hooks
    that are called.  Red dotted lines are the nestings of macro
    expansions as the algorithm recurses.  Blue lines are
    \texttt{EXPAND\_MACRO} actions on the underlined macro invocation,
    green lines are \texttt{MACRO\_CLEANUP}.}
    \label{fig:text-expn}
  \end{center}
\end{figure}

Consider the example illustrated in figures \ref{fig:tree-expn} and
\ref{fig:text-expn}.  When the \texttt{MACRO\_EXPAND} hook is called for
\texttt{FOO} (marked with a asterisk star in the figures), we have:

\begin{verbatim}
   @nests                   == ( TA3#1; TA4#1 )
   MacroExpansionHistory()  == ( PA2#Body )
\end{verbatim}

\noindent The \texttt{@nests} list tells us we are expanding the first
argument of macro \texttt{TA3}, which itself was the first argument of
macro \texttt{TA4}.  The \texttt{MacroExpansionHistory()} backcall
provides the remaining information about this expansion: that the
expansion came from the body of the earlier expansion of macro
\texttt{PA2}.  From figure~\ref{fig:text-expn}, the \texttt{@nests} list
for a given expansion corresponds to the sequence of red dotted lines
directly above and completely overlapping the blue line representing
that expansion.  Similarly, the \texttt{MacroExpansionHistory()}
backcall's returned list can be visualized as the stack of blue expansion
lines directly above the expansion in question (those that have not
already been paired with a green line representing the completion of
their expansion).

%% Emacs support
\subsection{Program understanding support in Emacs}
%advantage over font-lock mode that you get information from the actual
%parse, not by some lexical rules which may cause a mismatch
Some of the analyses \pcp{} supports generate excessive amounts of
information that is not easy to comprehend in raw form.  For example,
the mapping between unprocessed source code and preprocessed code aids
program understanding little when existing only in a data structure in a
\Perl{} script.  The standard module of hook utilities provides a
mechanism to output character-indexed annotations of the source code.
These annotations are Emacs Lisp source code which manipulate Emacs text
properties of character ranges when evaluated~\cite{GNUELisp}.  This provides a useful
(indeed, essential during debugging) mechanism for visualizing the
results of the analyses within the context of the code analyzed.  As the
cursor is moved over source code that has been annotated, a subsidiary
Emacs frame dynamically displays the annotations applicable to the
user's current location. See figure~\ref{fig:emacsdocprop} for an
example.

\begin{figure}[p]
  \begin{center}
    \leavevmode
    \includegraphics{figs/doc-prop.ps}
    \caption{A view of Emacs using the author's ``doc-property-display''
      feature to dynamically view textual annotations of results of the
      analyses of \pcp{}.  The red outline box in the top frame
      corresponds to the user's cursor, and the lower frame lists the
      various text properties attached to the character under the
      cursor.  The bottom frame potentially changes after each cursor movement.}
    \label{fig:emacsdocprop}
  \end{center}
\end{figure}


Annotating text makes more sense than annotating the abstract syntax
tree for several reasons.  First, \Cpp{} operates at a textual level.
As we have seen, the unprocessed source code cannot necessarily be
viewed as an ordinary abstract syntax tree.  Even if a generalized tree
could be constructed, no available environment provides adequate means
of interacting with the large, complicated trees that inevitably result
from realistic packages.~\footnote{The intentional programming group at
  Microsoft Research is actively investigating this
  area~\cite{MSIPPersonal}.}  Additionally, text permits using Emacs as the
target interaction environment. This allows software engineers to
augment Emacs's other powerful source code understanding tools (\eg{}
\texttt{font-lock-mode}, \texttt{etags}, \texttt{OOBrowser}, \etc{}.)
with their own annotations supplied by \pcp{} analyses.

%% Better than AST for representing the source because cpp is textual
%% Also ASTs not ready for prime-time use

\subsection{Interactions among components}

% PushBuffer

% ParseStateStack
%% lets you know where you're at (in a array size specifier, is a decl
%% allowable here?)
% SetParseStateStack

% YYPopAndRestoreStackState
% YYPushStackState
% YYSwapStackState
% YYPushDupTopStackState
% YYFCompareTopStackState

% Permits using the parser to parse individual pieces of code you're
% interested in

\subsection{Performance}

Although no tuning has been done yet, \pcp{} exhibits excellent
performance comparable to an optimizing compiler.  For \texttt{gcc} to
compile and optimize the 5,000 lines of the \texttt{bc} package (an
arbitrary precision arithmetic language interpreter) required 38 seconds
on the author's Pentium Pro 200 Linux machine. Running \pcp{} with its
test script consisting of 600 lines of hooks for virtually every action
used only 40 seconds of real time.  With the action code turned off, the
running time was reduced by a factor of 4.  Using the \texttt{--noparse}
option hardly reduced the running time without action code at all; when
just executing the infrastructure, a system call trace reveals that \pcp{}
is I/O bound.

\section{Automating replacement of \ppd{define}s with constants}
\label{sec:xform}

%% Transformations really about understanding

%% Def/Use for macros


\section{Related Work}
\label{sec:related}
Numerous tools exist for assisting the software engineer in
understanding and manipulating her source code.  Griswold and Atkinson
review a number of them while motivating their \texttt{TAWK} tool~\cite{Griswold96}.
\texttt{TAWK} uses \C{} as its action language and matches patterns in
the abstract syntax tree.  They take significant pains to handle macros
correctly, since they use a parser which could otherwise be too
fragile.  They try to parse each macro definition as an expression,
allowing macro arguments to be types, as well.  If that succeeds, the
macro is left unexpanded in the code and becomes a node resembling a
function call in their AST.  About 92\% of macro definitions in the two packages
they studied parsed acceptably.  For the remaining 8\%, they expanded
uses before feeding the resulting tokens to their parser.  Griswold and
Atkinson note that counting macro expansions is difficult, yet this is
something \pcp{} does with ease.

LCLint attempts to analyze macro definitions for
dangerous or error-prone constructs.  It allows, for example, to
annotate that a macro argument needs to be side-effect free at an
invocation site, and can generate a warning message if this constraint
is violated.  Evans's focus is on finding errors, and the
analysis on macro expansions is fairly uninvolved
~\cite[Ch.~8]{LCLint}.

Cordy and Carmichael designed \texttt{TXL} as a language to support
arbitrary transformations on abstract syntax trees~\cite{TXL}.  Their
work is not targeted specifically to \C{} code, and ignores issues
relating to the preprocessor.  Their tool uses a grammar to parse the
language and creates the tree, then applies some tree transformations
directly on the tree, and uses an ``unparser'' to write the new source
code back.  Their transformations are similar to the ``syntax macros''
suggested for use with \C{} by Weise and Crew~\cite{Weise93}.  These
constructs resemble the macros provided by Lisp~\cite{Steele90}, and
have the nice feature of syntactic safety, guaranteeing that macros can
not introduce syntax errors. Crew also created \texttt{ASTLOG}, a
\textsf{Prolog}-inspired language for examining abstract syntax trees.
Again, he largely ignores the preprocessor, instead assuming a parse
tree as the initial representation.

Stroustrup explains that making the preprocessor redundant was a design
goal of \CPP{}.\footnote{Stroustrup also discusses differences between
  \C{} and \CPP{} which could cause other problems when re-targeting
  legacy \C{} code to a \CPP{}
  compiler~\cite[p.~816-820]{Stroustrup97}.} Though \CPP{} provides no
feature to replace the obviously still-needed \ppd{include} directive,
Stroustrup discuss the form that construct
might take.   Carroll and Ellis make the strong claim that ``C++ provides
mechanisms that obviate the need for most
macros''~\cite[p.~147]{Carroll95}.  They list numerous uses of the
preprocessor and describe the \CPP{} language features that could
replace them.  Their advice is mostly prescriptive and is directed to
the new \CPP{} programmer who might be inclined to overuse \Cpp{}
constructs as a crutch instead of exploiting \CPP{} language features.
They do not directly address converting existing use of \Cpp{} in legacy
code.  Reiss and Davis casually mention a research project that helps
the user restructure programs by ``translat[ing] an existing \C{}
program into \CPP{}''~\cite[p.~2]{Reiss90}, but nothing further is
mentioned about the project.\footnote{Personal inquiries of that paper's
  authors also led to no pointers.}

Davis provides a thorough review of the issues involved in translating
\CPP{} language constructs to Java.  He ignores the preprocessor and its
contribution to \CPP{} source code artifacts~\cite{Davis97}.  Laffra's
\texttt{C2J} and Tilevich's derived \texttt{C2J++} claim to convert \C{} and
\CPP{} to Java code.  Their approach is a combination of lexical and
parser-based, and is focussed only on reducing the tedious editing when
porting an application to Java.  They use \Cpp{} to preprocess, and
claim to only convert about 85\% of the code correctly.

Krone and Snelting analyze conditional compilation directives in the
context of a lattice-theoretic framework for inferring configuration
(versioning) structures from the source code.  They study how \ppd{if} guards
depend on and relate to each other~\cite{Krone94}.

% Summary & Future work
\section{Summary and Future Work}
\label{sec:summary}
% Framework problems
%** Fairly dependent on internals of the preprocessing and parsing peculiarities of the tools
%** Huge effort in making the hooks interface complete; existing hooks motivated by need not by design
%** Also data structure sharing; duplication of work between perl code and cpp/parser structures
%** AST going to waste for now; ast viewing w/ Tk?

% Possible applications
%** Syntax highlighting
%** Source + output annotations
%** Tags generation
%** Call graph extraction (add summaries to fns)
%** parser debugging, teaching
%** multi-version reasoning

%* Convert to Java
%* C++ parser; C++ code still uses preprocessor too much

\appendix
\newpage

%%FIXGJB: order and organize these
\section{Action hooks called by \pcp}
\begin{footnotesize}
\label{sec:hooks}
\begin{itemize}
\sloppy
\input{pcp3-hooks.tex}
\fussy
\end{itemize}
\end{footnotesize}

\newpage

%%FIXGJB: order and organize these
\section{Subroutines provided by \pcp}
\label{sec:backcalls}
\begin{footnotesize}
\begin{itemize}
\sloppy
\input{backcalls.tex}
\fussy
\end{itemize}
\end{footnotesize}

% References
\newpage

\nocite{ARM}
\nocite{Dragon}
\nocite{Glickstein97} % Writing GNU Emacs Ext.
\nocite{Camel}        % Perl 5
\nocite{Perl}        % Perl 5
\nocite{Levine92}     % Lex & Yacc
\nocite{Harbison91}   % C Ref Man
\nocite{Stroustrup97} % C++, 3rd
\nocite{Stroustrup94} % C++, 2nd
\nocite{Kernighan88}  % C, 2nd
\nocite{Flanagan96}   % Java in a Nutshell
\nocite{EmpCpp}
\nocite{EmpCpp-TR}
\nocite{GCC}
\nocite{CTree}
\nocite{TXL}
\nocite{Cordy92}
\nocite{Bison}
\nocite{Flex}
\nocite{Krone94}
\nocite{Griswold96}
\nocite{Atkinson96}
\nocite{CD2DraftStandard}

\bibliographystyle{alpha}
\bibliography{library,articles,pcp3}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
% LocalWords:  stringization tex fancybox gjb endif cpplib backcall
% LocalWords:  MacroExpansionHistory
