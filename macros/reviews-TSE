X-From-Line: mernst@rhodes  Wed Mar 25 08:40:21 1998
Received: from rhodes.cs.washington.edu (rhodes.cs.washington.edu [128.95.1.134]) by june.cs.washington.edu (8.8.7+CS/7.2ju) with ESMTP id IAA14268; Wed, 25 Mar 1998 08:40:21 -0800
Received: (mernst@localhost) by rhodes.cs.washington.edu (8.8.5+CS/7.2ws+) id IAA06153; Wed, 25 Mar 1998 08:40:20 -0800 (PST)
Date: Wed, 25 Mar 1998 08:40:20 -0800 (PST)
Message-Id: <199803251640.IAA06153@rhodes.cs.washington.edu>
From: Michael Ernst <mernst@cs.washington.edu>
To: Greg Badros <gjb@cs.washington.edu>,
        David Notkin <notkin@cs.washington.edu>
Subject: IEEE TSE reviews for evilmacros paper
Lines: 973
Xref: elwha.cs.washington.edu inbox:2977

------- Start of forwarded message -------
Date: Tue, 24 Mar 1998 23:55:10 -0500 (EST)
From: Jens Palsberg <palsberg@cs.purdue.edu>
To: mernst@cs.washington.edu
Subject: Your paper
Cc: swerner@computer.org

Dear Michael Ernst, Greg Badros, David Notkin,
   Re:  Your submission to IEEE Transactions on Software Engineering
        entitled "An empirical analysis of C preprocessor use."

Thank you for your submission.  Three reviews are appended.
All three reviews are generally positive towards the paper, 
although they also contain substantial criticism.  The reviews
give numerous constructive suggestions for how to improve the
paper.  I think that many of the questions raised by the reviews
are questions that also some IEEE TSE readers would raise.  

My conclusion at this point:   
  - please prepare a major revision of the paper which addresses 
    the questions, suggestions, and issues raised by the reviews; and
  - please prepare an explanation of how you reacted to each of the points.
You may want to copy the reviews and insert explanations of what you did.  
In some cases you may want to explain why you decided not to follow a 
suggestion.  

When I receive the revised version of the paper, I will forward the paper 
to the reviewers and ask them if they think that their points have been 
adequately addressed.  

Sincerely,

Jens Palsberg
Purdue University
Associate Editor of IEEE Transactions on Software Engineering
www.cs.purdue.edu/people/palsberg

============================================================================
Review 1:

There is an ongoing methodological debate in software
engineering. Many people feel that there are too many fancy new
ideas, and to few empirical evaluations of new ideas. Therefore,
empirical studies should be encouraged, and mathematical silver
bullets discouraged.

The article by M. Ernst et al. reflects this debate. Many people
use the C preprocessor, many complain about it, and several
tools have been build in order to improve CPP usage. But nobody
ever seriously investigated CPP usage patterns, in order to
provide an empirical basis for discussions about CPP. M. Ernst
et al are the first ones to present a serious study based on the
analysis of two million lines of UNIX source files.

The article does not contain any "deep results", but is an
in-depth investigation about occurrences of CPP directives, macro
definitions, and macro usage. All CPP constructs are treated;
for many, statistical evaluations are given. The text provides
many interesting insights, in particular the section on CPP
pitfalls is fun reading. The article definitely provides a
service to the community and should therefore be accepted. It
will be widely read not only by scientists, but also by
practitioners.

I have only two more comments. One remark is that
the article is pretty long, but it is hard to see how it could
be condensed. My other remark is about methodology. The reader of
an empirical article is forced to just believe the statistics
which are presented to him. Unlike articles which contain
theorems or conceptual presentations, there is no way how the
reader can validate the material. It is thus for good reasons
that in the natural sciences experiments are duplicated. Maybe
the authors can convince colleagues to perform a similar study
for a different set of source texts, and compare the results.

- ---------------------------------------------------------------------
Review 2:

General comments
- ----------------

This paper addresses a subject - quantification of macro use in C - that
definitely has long needed addressing. Overall, the paper addresses this
subject well.

What is made very clear by this paper is the vast extent to which real C
programs rely on the cpp.

What is the main aim here? Is the big question `how can we develop good new
C programs?' or is it `how can we understand existing C programs, with the
possible intention of transforming them to C++?'

Some of the choices for what is important and the categorizations and
divisions of cpp usage seem arbitrary. Examples are mentioned below.

Section 4
- ---------

second paragraph, last sentence:

"Multiple definitions of a macro are generally innocuous: only 2% of macros
names had definitions with different abstract syntax trees."

What is the evidence for such a statement? Is the claim that a macro like:

#define FOO if (e) f(1) else g(2);

is so different from

#define FOO 3

that someone looking at the code would be confused by an occurrence of FOO
in the program (and that it might be an error), whereas

occurrence likes

#define BAR 0
#define BAR 1

are "innocuous?"

In certain cases this is true, most notably when conditional compilation
flags define BAR's value to be 0 for one architecture (or operating system
or whatever) and 1 for another.  However, in general, it seems that these
two definitions of BAR are _more_ dangerous than the two definitions of
FOO. The definitions of FOO cannot be used interchangeably in the program
because the compiler will catch the error, the same cannot be said for BAR.

This criticism may seem nit picky, but a similar complaint applies to
numerous aspects of this paper: what is considered good or bad seems rather
arbitrary and subjective.

Section 4.1, category "Type":
- -----------

"... These macros may be tricky to understand, and cannot be eliminated via
straightforward translation ..."

What about the use of a typedef?

Maybe replace:

#define __ptr_t void *

with:

typedef void *__ptr_t



Section 4.2
- -----------

Can "extra linguistic" be precisely defined? You cannot compute more using
cpp than without it; however - you can express the same computation more
succinctly. But what makes one use of cpp macros extra-linguistic and
another not? Following this train of thought, why is it important or
interesting to know what percentage of macros use these features?

In particular, why is the `extra-linguistic feature' Assignment (which the
authors admit is "not extra linguistic per se") special? Side-effects can
be obscured almost anywhere in a C program without using macros -
functions that manipulate global variables or pointer values are doing just
that.

Section 4.3
- -----------

Not all of the `lint errors' seem worth looking for.

`side-effected formal'
- ----------------------

The author's claim is that a lint check should warn users of macros of the
form:

#define ASSIGN_0(X) X=0

because the argument might not be an lvalue. But the compiler will catch
such errors and should state the reason (e.g. `invalid lvalue in
assignment')


`inconsistent arity'
- --------------------

It is agreed that this is probably poor form. However, if the user chooses
to undefine a macro, then the override is obviously
intentional. Furthermore, since the arity is different, it should be less
confusing to read than if it was overridden with the same arity.

Functions in C++ can be overridden (without using cpp) with the same or
different arity and similar confusion can arise.


`swallows_else'
- ---------------

"... results in the else clause being executed not if the condition is
false, but if it is true (and tainting is also true) ..."

should read:

"...   but if it is true (and tainting is false) ..."


Section 4.4
- -----------

What does graph in Figure 6 really say? Seems unnecessary.

What is the evidence that redefinition (or multiple definitions) of a
macro is necessarily undesirable? Certainly in some cases it will be
confusing, but is there evidence that it's a problem (regardless of the
frequency with which it occurs)?



Section 4.5
- -----------

The canonicalization process used sounds like type inference. In fact, why
not simply apply type inference on macro bodies to arrive at
canonicalizations?



Section 4.6
- -----------

Finding inconsistent definitions sounds very similar to unification (also
used in type inference).



Section 5.1
- -----------

What does Figure 10 really illustrate? Why is the x-axis scale non-linear in
such an odd fashion?

Section 6
- ---------

What is the relevance of the dependency discussion? In particular, what is
the significance of expansion dependencies? A tool that would allow a user to
select a line of code and see all the macros on which that line "depended"
would be useful. Similarly, selecting a macro and seeing all lines which it
might effect would be useful. (Think of these as cpp-style
`slices'). However, the overall numbers concerning these dependencies don't
seem to communicate much information other than, `yes, macros are used a
lot'.

The graph in Figure 13 appears to be missing (a least in my copy) a line
and a key symbol for "Any".



Section 6.3
- -----------

The cpp partial evaluator seems like an excellent idea. cpp claims to
offer four separate facilities, but there is no obvious way to use only
one or some of those facilities without the others. (For example, only
expand includes.)

It is surprising that your cpp partial evaluator results seem to not
substantially reduce the complexity of the macro definitions and
dependencies. Do nested ifdefs get included/excluded by your partial
evaluator? And does the cpp partial evaluator at least result in
substantial code reduction? (That in itself would seem useful.)

Section 8
- ---------

One problem with replacing macros with code is performance. Yes, very good
compilers might make up for their use, but not in all cases. In this
respect, the use of macros is like specifying partial evaluation directives
to the compiler. In the hands of a competent programmer substantial
efficiencies can be gained.

- ---------------------------------------------------------------------
Review 3:

                      SUMMARY AND RECOMMENDATION

This paper presents an experimental study of the use of the C
preprocessor.  The paper makes two contributions: it presents a
taxonomy of ways the preprocessor may be used, and it reports on
experimental measurements of the frequency with which different taxons
are used.  (It also reports some other properties, like dependencies.)
The presentation of both contributions is flawed.  We are not really
shown the development of this taxonomy, and no effort is made to
convince us that this is a good taxonomy or to show us that this
taxonomy can be put to some practical use.  The paper pounds away at
the reader with experimental measurements, which are not organized in
a way that helps the reader to draw any conclusions.  Too much of the
paper simply counts things, without adding analysis that develops the
reader's understanding.  The presentation of the experimental
measurements suffers from inconsistent presentations (e.g., frequency
distributions in one place vs cumulative counts in another) and from
lack of precision in the explanation of exactly what is being
measured.  Finally, the concluding sections (8 and 9) are not
supported by the empirical work; they seem disconnected from the rest
of the paper.

Despite its flaws, this paper presents work that might well help solve
an important problem: construction of program-analysis and
program-understanding tools that can work on source code containing
preprocessor directives.  I think a revision of this paper has the
potential to be a valuable contribution to the field.  Accordingly, I
recommend that the authors undertake a major revision and resubmit the
paper for further review.  

The rest of this review contains suggestions for revisions and
specific comments about the paper.


            OVERALL COMMENTS AND SUGGESTIONS FOR REVISION

It may be that the most valuable contribution of your work will come
not from your measurements but from your taxonomy.  Certainly the
relevance of the taxonomy is crucial to the significance of the work.
It is not enough to put together a taxonomy based on what you think
you see in the data; your taxonomy must serve a practical purpose or
improve understanding---preferably both.  In other words, to divide
macros into taxonomic categories is not much of a contribution unless
the categories are well chosen.  How did you develop your taxonomy?
Is it useful for solving any particular problems?  Does it communicate
something to tool builders?  To programmers?  To language designers?
How do taxons relate to the analyses that are used in typical tools?
What criteria did you use to prune your taxonomy for this paper?  Was
there anything useful in the elaboration of some taxons?  Did you do
some pruning on the basis of experimental measurement?  If so, it may
be worth saying ``a priori, we though taxon X would be significant,
but measurements showed otherwise, so we consider X only as it forms
part of Y.''

It's hard to tell from your paper if your taxonomy is flat.  For
example, is ``extra-linguistic'' a taxon that is divided into other
taxons?

Don't measure just for the sake of measuring; convince us that what
you are measuring is meaningful.  For example, you might

  1) Argue that it is useful to replace macros with C linguistic
     constructs.  If you can make this case, then show that your
     taxonomy helps identify macros that can be replaced with C.
     Could this be done automatically?

  2) Show which macros can be handled by which different kinds of
     software tools (e.g., test-coverage analysis, program
     understanding, program restructuring, debugging, ``lint'', etc),
     and *why*.

In other words, convince us why these taxons are important and to what
use your taxonomy might be put.  

(Aside: I suspect that a discussion of the taxonomy will show that it
does not make sense to lump Makefiles and other non-C code in with the
C programs in the experimental measurements.)


A serious weakness of your paper is that your taxons are not defined
precisely.  You do not explain exactly what your analyzer computes or
how it works.  The existence of 11,000 lines of perl does not give me
confidence that you are computing something well-defined and
reproducible.  This weakness must be corrected in order to make your
paper acceptable for publication.  A good place to begin would be a
precise characterization of the raw data on which the graphs in the
paper are based.  For example, I guess that your analyzer might produce:
  - List of (macro name, classification, package) triples
  - For each macro name (or definition?), 
       .  a list of the number of lines of C code causing that macro
          to be expanded (per package)
       .  a list of the number of lines of C code on which that macro
          appears
       .  a list of the number of lines of C code that are included in
          or excluded from the preprocessed source based on the value
          or definedness of that macro
Be sure to distinguish carefully between macro names and macro
definitions.

Explain exactly how the classifications are determined.  There are
several hints in your paper that the analysis is not very precise in
spots.  Two of these imprecisions must be corrected before I can
recommend the paper for publication:
  - You must refine your analysis so you can tell the difference
    between macros that refer to global variables and macros that
    refer to local variables or parameters.  
  - You must distinguish ordinary variables and built-in keywords from
    uses of similarly named macros.  A modified cpp might help with
    this analysis.
In general, tell us exactly how well or how badly you are doing in
your analyses.

You might want to consider beginning the paper with the taxonomy,
giving it the full and careful treatment it deserves, before moving on
to the experimental data.  This will help readers manage the sheer
quantity of data, and those readers who fail to do so will at least
have learned your taxonomy.

When you present your taxonomy, give an example for every taxon---the
examples are very helpful.  Try to do something typographically to
make the examples easy for the reader to find.  A table of taxons with
short definitions (and examples if they'll fit) might be helpful.
Give readers every aid in understanding and remembering your taxonomy.

You claim on page 3 that your taxonomy identifies innocuous and
problematic macro usages.  I don't see this claim borne out in the
paper.  This identification should be part of a clearer presentation
of your taxonomy.  Who decides what is innocuous and what is
problematic?  By what criteria?  Will you use your taxonomy to
promulgate usage guidelines?  Are there usages that are innocuous for
some purposes but problematic for others?  How many features of GNU C
or C++ would enable you to eliminate problematic usages that couldn't
be eliminated from ANSI C?  Support your answers with arguments or
evidence.




You've taken a lot of measurements, but I didn't learn much.  What are
readers to *do* differently, or to *understand* better as a result of
your analysis?  What conclusions do you want us to draw from the data?
Are there other conclusions that we might have hoped to draw but that
are not in fact supported by the data?  In other words, is anything in
the paper surprising?


You don't seem to exploit the fact that you have data from many
different packages.  It seems to me you're missing an opportunity to
correlate your measurements and classifications with things people
actually care about.  You have version numbers.  Do you have other
independent information (even guesses) about the packages?  Bug rates?
Number of users?  Can your analysis of usage within a package tell us
anything about that package?





Regarding related work, is it possible that work on software metrics
(complexity metrics) has completely ignored the C preprocessor?  I
don't know this literature at all, but I hope you can find either some
studies or some published evidence that the metrics community has
overlooked Cpp.

I would also like to know how your analysis relates to the analysis
performed by sophisticated tools like Gimpel's FlexeLint (aka
PC-Lint).  FlexeLint undoubtedly has a taxonomy of deprecated macro
constructs, and it would be useful to compare this taxonomy with your
taxonomy.  Such a comparison would help you make the case that your
taxonomy is relevant.  Gimpel supplies this tool in obfuscated source
form, so it might be possible to grep error messages, or they might be
willing to supply code that tickles the error messages, so you could
identify which had to do with macros.





The writing of your paper needs improvement, especially in the
introduction.  By page 6, there is no coherent story about what you
are doing in this paper.  What is the problem you are trying to solve?
The first clear indication of the goals of the work comes on page
26, at the beginning of section 8.  This material should appear on
page 1.

The abstract is frustrating to read.  Instead of distilling the
essential results of the paper, the abstract lists the problems you
worked on.  I recommend consulting the following short paper for
advice on writing a better abstract:

   Landes, Kenneth K. 1966 (September). A scrutiny of the abstract.
   _Bulletin of the American Association of Petroleum Geologists_,
   50(9):1992--1993.


It might help your readers' understanding to pick one or two tools or
applications (e.g., a debugger, a test-coverage analyzer), and as you
go through both the taxonomy and the experimental data, to show how
the taxonomy and the data relate to the construction or use of such
tools.

This paper presents lots of data in the form of graphs and tables, so
you must make extraordinary efforts to help your readers comprehend
it.  Figures with their captions should be meaningful on their own and
should not require extensive explanation in the text.  Moreover,
because you use so many different kinds of figures, you must lead the
reader by the hand, explaining both the significance of the data in
the figures and the conclusions that may or may not be drawn.
Specific suggestions for a few graphs are given below, but almost
every graph in the paper needs work.  Finally, get the figures on the
same page as the text that discusses them---at least in a submission,
large chunks of white space are preferable to forcing the reader to
jump around.




                          DETAILED COMMENTS



Page 1:

The abstract claims an analysis taking cpp into account is
preferable.  Why?  Make your case or withdraw the claim.

What does it mean to say cpp can define new syntax?

What does it mean to say cpp permits system dependencies to be made
explicit and tested?

What is `disciplined' use of the preprocessor?  Who decides?  Which of
the usages in the paper are disciplined?  Connect the usages with
programmer effort and portability.

Page 2:

What is ``attempting to emulate the preprocessor''?

What evidence is there that preprocessing may limit the readability
and reusability of the C++ templates inferred by Siff and Reps?  And
could this problem be solved by limiting macro usage to certain of
your taxons?  If so, explain how (and which taxons).

What is a non-syntactic program?

What is the relationship of your list of pitfalls to your taxonomy?
How do your pitfalls compare with others' pitfalls (e.g., Koenig's C
Traps and Pitfalls)?  I'm not very convinced by your list.  It's all
too vague and general.  Try to improve or prune the list.  Definitely
give a concrete example of each.

Why is high total use problematic?  Computers are good at doing
the same thing over and over.

What's a complicated body?  Does your analysis tell you which bodies
are complicated?  How do you measure complexity?

Explain stringization, pasting, and free variables.

Some of the value of macros comes from multiple definitions (e.g., for
portability or dialects).  Why is this a problem?  Make your case.


Page 3:

Exactly what is ``numerically significant?''  Give me the numbers, and
if it's not immediately obvious why they are significant, argue why
they are---don't just make the claim.  In general, when you make a
claim, make it as precise as possible and provide supporting evidence.

Begin Section 2 by saying how the analysis was done.  At this point
I'm still wondering if it was done by hand.


If it's important that some applications are graphical, interactive,
etc., identify the applications in Figure 1.

Did you remove non-C files by hand?

Disclaimer about library macros should be more prominent.


Page 4:

Comments about your packages:
  1) Very heavy on GNU -- suggest you relate your taxonomy to GNU
     Coding Standards
  2) Where does one get these packages?
  3) There are almost no small programs.
  4) Interesting codes possibly worth adding include:
       - the Linux kernel or parts thereof
       - BSD sources
       - Runtime system from SML/NJ or Objective CAML
       - tcl implementation
       - Hanson's C Interfaces and Implementations
I found I didn't learn anything from the fact that different packages
had different profiles.  I was disappointed.  Did you learn anything?


How much of your 11,000 lines of perl might be useful for other kinds
of analyses?

Explain precisely what it means to attribute a line of code to a
directive.


Page 5:

Label a few of the most obvious outliers, especially remind, which you
discuss later, and the #line outlier.

Show actual directives in caption, e.g., #line, #if/#else/#endif,
#include, #undef, #define.


Page 6:

How many is `most uses' of #undef?  Be precise whenever possible.

HAVE_PROTO is worth discussing.  You may want to compare the
HAVE_PROTO technique to this technique:
  #ifdef __STDC__
  #define ARGS(list) list
  #else
  #define ARGS(list) ()
  #endif
and its use as
  extern void  *allocate ARGS((unsigned long n, unsigned a));
in order to make the point that it is not necessary to have so much
conditional compilation.


When presenting numbers, this paper suffers from what Fowler (A
Dictionary of Modern English Usage) calls `elegant variation'.  Don't
make me compare 98%, three quarters, one sixth, and one quarter---use
a consistent notation throughout.  In this case, percentages are good.

You say 25% of macro definitions contain latent bugs.  Compare this
number to the number found by other tools, e.g., FlexeLint, LClint.


Page 7:

In figure 3 and similar graphs, don't make me read across by
rows.  Fix the graph so the legend is read down by columns.  In this
case, as in Figure 2, you have room for a single column to one side of
the figure.  This is the easiest to read and improves consistency.
Also, there is so much data in this graph that I want you to tell me
explicitly what is interesting and what conclusions can and cannot be
drawn.  If you can't do that, omit the graph.


Page 8:


Exactly what is a partial type?  What makes type macros tricky to
understand?  Why can't they be eliminated with typedefs?

Why the name `syntactic'?

Page 9:

Your tool failed to classify 900 macros.  I hope you studied these
carefully by hand.  Please try to characterize them, and please give
us some samples.

Remind us what stringization and pasting are.  


Page 10:

Figure 4 is a failure.  It is hard to read, and I have no idea what it
is telling me.  Such small percentages are meaningless, because they
don't really tell us the probability of rare events.  Certainly
add absolute numbers, and possibly drop the percentages (except for
the first three).  Slant the headings to make them readable (The
LaTeX Companion shows how if you are using LaTeX).  Even with these
improvements, however, I can make no sense of this table.



Page 11:

Why can't you measure pasting accurately?  Which is more significant,
direct pasting or indirect pasting, or are they both equally
significant (for tool building and understanding)?  Your paper
suggests they are equivalent.  If that's so, how much use is your
measurement really?

Shouldn't the extra-linguistic category include Syntactic macros?


How did you develop your list of macro pitfalls?  Aren't these
well known in the C literature?  How does your work relate?
Your contribution would be more valuable if you were identifying the
incidence of well understood problems instead of problems of your own
invention. 


Page 12:

Add absolute numbers to Figure 5.


Page 13:

How few uses of do { ... } while (0) ?
How many statement macros followed by semicolons?


Page 14:

Be more careful about the whole issue of swallowing semicolons and the
difference between statements and partial statements.  I'm quite
familiar with the issues, but still the thought of wrapping statements
in { ... } made me gasp.  I gasped because, as a matter of policy, I
never, ever define a macro that is equivalent to a statement.  Macros
look like function calls, which are partial statements, so I always
use do { ... } while(0).   Explain this issue thoroughly, and
early in the game.

Explain /**/-style pasting.

Can't you do a simple reaching-definitions analysis on the cpp code to
see which macro definitions could be used at which sites?  Isn't this
straightforward?

You can't postpone discussion of macro-name classification to section
4.6 or appendix A.  Make it clear what is going on: your scheme
classifies definitions, and the problem is to classify names that have
more than one definition.  The whole folding business seems bogus.
Why not just count bodies?  Explain why it might be interesting or
important to classify macro names.

``Failed classification'' seems an artifact of your classification
scheme.  Does this indicate a flaw in your work or a genuine problem
in the data?  

What is `supporting variant declaration styles'?

Page 15:

Again, lead your reader by the hand.  What does it mean for a curve to
be at the top of the graph?  At the bottom?


Page 16:

Figure 7 is another mystery.  On average, you have 1.8 definitions per
what?

I don't follow section 4.5 at all.

The first paragraph of section 4.6 is another mystery.

Give examples of places where you've identified changes that should be
made to package code.  Exactly how did you identify these places?
Would these places have been identified with a commercial lint tool or
other lint-like tool?


Page 17:

What is `participation of statements in incompatibly-defined macros?'

How many classification failures (bad mixes) represent bugs?  If you
can't look at them all, how about a random sample?


In Section 5, more elegant variation.  Give frequencies as macros per
thousand lines.  Give fractions as percentages.

What does it mean for two macros to represent different categories of
information?  How are these categories determined or measured?  How do
they fit into your taxonomy?


Page 18:

Figure 8 is a total loss.  Omit it.

Page 19:

How accurate is ``very accurate in practice''?  How do you know?  What
would constitute perfect accuracy?  How many instances did you spot
check?  How did you select them?


Page 20:

I don't like the games you are playing with X axes.  How about putting
the uses=0 data as separate blots, not connected to the rest of the
graph, and using a true logarithmic scale for the rest of the data?

Again, explain the significance of the graph, e.g., macros that are
frequently expanded are at the bottom of the graph.  Do this for
*every* graph in the paper.


Page 21:

I see no need to abbreviate ``conditional'' in Figure 11.

*What* fraction of uses are accounted for by uses in macro bodies?

Include mean percentages with each of the categories in section 5.3.


Page 22:

In Figure 12, no reading across by rows.  Explain the significance of
the data.  Could the categories be reordered to make the significance
clearer?


Page 23:

The classifications of macros are used to classify conditionals.  Are
these the classifications of the macro names or of the reaching macro
definitions?

Define inclusion dependence precisely and clearly, e.g., a line l is
inclusion-dependent on a macro name M (macro definition?) if and only
if ...

I'm lost in the second paragraph, section 6.

What was the size of the virtual memory that was exceeded trying to
analyze emacs and mosaic?

In Section 6.1, are we talking multiple macros on one line or macros
invoking macros?  Wouldn't it be useful to distinguish those cases?

Explain what LEGITIMIZE_ADDRESS does.  You have discovered an
interesting tidbit---let the rest of us in on it.

``One in twenty'' --> 5%.


Page 24:

Can you justify the violence you do to the X axis in Figure 13?  How
about just presenting 0 separately and using a true logarithmic scale
for the other data?

It's hard to tell what's maximum and minimum in Figure 13.  Again,
help the reader interpret the data.

On a linear-log plot, I would expect exponential decay to be a
straight line.  Figure 14 is not a straight line.  One of us is wrong
about this; how about a more formal statement of what is exponentially
dependent on what?  And tell us *how* closely the figure approximates
an exponential.

I read the third paragraph in section 6.2 as saying, ``Of the ten most
interesting macros our analyzer found, it got four of them wrong.''
My confidence plummets.  You must re-do your analyses to distinguish
ordinary variables and built-in keywords from uses of similarly named
macros.


Page 25:

Figure 14: After a paper full of cumulative data, you suddenly take
the derivative of the curve and present us with a frequency
distribution.  This distribution may actually be easier to understand
than the cumulative data, but consistency is also important.  I
suggest you revise the other graphs to use the frequency-distribution
form and see how that works.

Figure 14 needs work.  The labels are too small.  The numbers on the X
axis should label the boundaries between bars, not the bars
themselves.  This relabeling will eliminate the need for the epsilon
bar (called e in the figure).  It is probably worth putting both Y
axes on the same scale, even if you must increase the amount of
vertical space devoted to the figure.

The preprocessor is commonly used to support multiple dialects of
*which* languages?  How commonly?  On what evidence do you make this
statement?

Exactly how much of a decline is a `not substantial' decline?

The conclusion at the end of section 6.3 is not supported by the
evidence.


If organizations provide hints about using the preprocessor, you
should be able to relate those hints to your taxonomy.  Do so.

How might your empirical data help refine suggestions?  I don't see
it.


Page 26:

Are Spencer and Collyer's recommendations consistent with your
taxonomy?  Or does your taxonomy identify usages that can't be
converted according to their recommendations?  If so, how frequent are
those usages according to your data?  Does the frequency vary by
package?  Can the variation be correlated to maturity?  Bug reports?
Age relative to the ANSI standard?

Please do a better job explaining Krone and Snelting.  It seems
interesting, but I can't follow your explanation.

As noted above, I think much more extensive comparison with lint-like
tools is called for.  You are, after all, claiming to identify
problematic usages.

Please explain the significance of parse errors and internal bugs in
LCLint.  It sounds as if LCLint is nearly unusable for your
application suite.  How many of the packages (or source files) could
it handle without crashing?

You criticize Spuler and Sajeev for not justifying tradeoffs between
techniques that perform parsing and those that do not.  I see no such
justification in your paper.  You barely even explain your techniques
for identifying and classifying potential errors.


Page 27:

Define `problematic' uses.

Your argument about standardization might apply to new code.  It
certainly doesn't apply to legacy code.  Does that matter?

Are function declarations the only declarations at issue?

I'm skeptical about moving directives into the language proper.

I don't buy the argument about replacing common constructs with
special-purpose syntax.  If you have a proposal, make it, and say what
it would gain.  The general argument isn't convincing by itself; there
must be an example.

Use your empirical data to argue the points in section 8---connect
this section to the rest of the paper.

How are the good practices in section 8 different from recommendations
made by others?  The benefits you list are very abstract.  Make a case
for concrete benefits; show the real problems that are created by
``problematic'' macros.

In section 9, I'm not convinced language designers will learn much
from this paper in its present form.  Distill the information from the
rest of the paper that you believe to be relevant to language design.

Elaborate on the third paragraph of section 9.


Page 28:

Your paper doesn't identify ``constructs that cause tools to give
incomplete or incorrect results.''  This information is very important
and should be included in the development of your taxonomy.

You claim to have found interesting and surprising features.  I'm not
convinced---identify the most important such features.

As noted above, the problems identified in the last and third from
last paragraphs must be corrected.  You must not assume that all
lexical occurrences of macro names are macros uses.  You must
distinguish use of global variables from use of local variables and
parameters.


Page 29:

References like [SC92] are unnecessarily cryptic.  Writing out
[Spencer and Collyer 1992] makes it easier for the reader to identify
or remember the work.  LaTeX provides a 'chicago bibliography style',
which puts references in this format.


Page 30:

This algorithm seems simple, and it could probably be clearly and
concisely explained in a figure or table in the main text.  I see no
reason to relegate it to an appendix.

What is a `statement-related' macro?  What is a plural form?  These
questions are not answered on page 8.

- ---------------------------------------------------------------------
------- End of forwarded message -------


