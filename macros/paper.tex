% $Id$
% macros-paper.tex
% FIX: Switch to LaTeX2e, not 209
\documentstyle[11pt,epsf]{article}

\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt

\topmargin   0pt

\textwidth   6.5 in
\textheight  8.5 in

\begin{document}
\bibliographystyle{plain}

\title{An Empirical Analysis of the Use of the C Preprocessor}

\author{Michael Ernst,\thanks{Email 
addresses: {\tt \{mernst, gjb, notkin\}@cs.washington.edu}.  Contact author:
mernst@cs.washington.edu.}
\and Greg J. Badros \and David Notkin}


\date{Department of Computer
Science and Engineering\\
University of Washington\\
Box 352350\\
Seattle, WA  98195-2350\\
\today}  

\maketitle


\begin{abstract}

  The C programming language is intimately connected to its macro
  preprocessor.  This relationship affects, indeed generally hinders,
  both the tools (compilers, debuggers, call graph extractors, etc.)
  built to engineer C programs and also the ease of translating to other
  languages such as C++.  In this paper we analyze over 20 packages
  comprising 1.5 million lines of publicly available C code, determining
  the ways in which the preprocessor is used in practice.  We developed
  a framework for analyzing preprocessor usage, using it to extract
  information about the percentage of preprocessor directives in C
  programs, about the frequency of use of defined macros, about the
  relationship between C functions and their use of macros, and about
  the macros that are difficult to express in terms of other C or C++
  language features.  We report on the analysis, in particular
  illustrating data that are material when considering the definition of
  tools for C or C++.  We present how the results (and the supporting
  framework) lay the foundation for developing a tool to reduce usage of
  the preprocessor by translating from C to C++.

\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is intimately connected to its
macro preprocessor, Cpp~\cite[Ch.3]{Harbison91}.  Nobody writes
programs in ``pure'' C; facilities of the preprocessor such as
including other files, defining constants and macros, conditional
compilation, etc. are present in essentially every program.  This
relationship between the C language and the preprocessor has a number
of consequences, many of which were probably not anticipated by the
original designers.

Stroustrup puts the consequences of this relationship in perspective:
\begin{quote}
Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders~\cite[p.~424]{Stroustrup-DesignEvolution}.
\end{quote}

As one example, source-level debuggers either map information back to
the original source code or else leave this task to the programmer.
As another related example, call graph extractors generally work in
terms of the post-processed code, even when a human programmer---as
opposed to an optimizing compiler, for example---is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Handling the mapping
between the original and the post-processed source is not easy for
many tools, often leaving the programmer responsible for inferring the
mapping, which is an undesirable and error-prone situation.

A related consequence is the difficulty that the preprocessor adds in
translating programs from C to C++.  For example, Siff and Reps have
described a technique that uses type inferencing to produce C++
function templates from C; however, the input is restricted to be ``a
C program component that $\ldots$ has been pre-processed so that all
include files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  O'Callahan and Jackson also use
type inference, although for program understanding rather than
translation; they, too, apply their techniques to post-processed
code~\cite{OCallahan-icse97}.

Having tools use post-processed source code increases the dissonance
between what the programmer writes and what the tools analyze.  There
are several ways to try to reduce this problem; Stroustrup suggests
the following:
\begin{quote}
I'd like to see Cpp abolished.  However, the only realistic and
responsible way of doing that is first to make it redundant, then
encourage people to use the better alternatives, and {\em
then\/}---years later---banish Cpp into the program development
environment with the other extra-linguistic tools where it
belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}

As a step in this direction, we performed an empirical analysis of the
ways in which the preprocessor is used in a large collection of C
programs; we analyzed nearly 30 packages that comprise well over one
million lines of code.  Section~\ref{sec:gathering} lists the packages
and their sizes.  The intent of the analysis is to build a better
understanding of how the preprocessor is used.  In turn, this will
help us to build a C to C++ conversion tool with two attractive
properties: one, it will take as input C programs complete with
preprocessor directives; and two, it will map many---preferably
most---uses of directives into C++ language features.\footnote{It is
not practical to eliminate all uses of Cpp.  For example, C++
currently provides no replacement for the \verb+\#include+ directive.}
The framework we developed for analyzing preprocessor usage provides a
basis for the development of such a conversion tool.

In this paper, we report on several specific analyses that we
performed.  
\begin{itemize}

\item We computed the percentage of original C source code lines that
were preprocessor directives, including a breakdown of the frequency
of specific directives (such as \verb+#define+); the analysis is shown
in Section~\ref{sec:directives}. It was not unusual to find C programs
in which over 10\% of the total lines were preprocessor directives.

\item We computed which macros were defined, how often they were
defined, how often they were undefined, and the number of times they
were expanded both from other preprocessor directives and also from
true C source code; the analysis is shown in Section~\ref{sec:usage}.
[FIX: pithy observation goes here]

\item We categorized definitions of macros according to what they
      expanded to; for example, we determined which macros simply
      defined a preprocessor symbol, which defined constants, etc.;
      the analysis is shown in Section~\ref{sec:categorization}.  We
      were particularly interested in determining which macros are
      defined in a way that is difficult to convert to other language
      features.  [FIX: pithy observation goes here]

\end{itemize}

Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of
C programming support tools.  On the other hand, the analysis also
convinces us that, by extending our analysis framework with some class
type inferencing techniques (similar to those used by Siff and
Reps~\cite{Siff-fse96}, O'Callahan and
Jackson~\cite{OCallahan-icse97}, and others), we can take significant
steps towards a tool that usefully converts a high percentage of Cpp
code into C++ language features.\footnote{Preliminary results using
our tool on C++ code demonstrates that many C++ packages still rely on
Cpp, even when C++ provides mechanisms within the language to support
a nearly identical construct.  This probably holds due to a
combination of trivial translations from C to C++ and of C programmers
becoming C++ and continuing on with particular habits.}  We are
interested in translations that don't merely allow a C program to be
compiled by a C++ compiler (this is not usually too difficult, by
intentional design of C++) but rather---like the conversion to C++
function templates by Siff and Reps---take advantage of the new,
presumably superior, constructs of C++.

%[FIX: Two important points to address: C code almost *is*
%C++ code, so what do we gain from a ``conversion''?  If we can't get all
%the code, can we identify the parts we can't get?
%gjb thinks this is more for the next paper-- we don't need to get into
%it here]

\section{Related Work}\label{sec:related}

We could find no empirical study of the use of the C preprocessor nor
any other macro processor.  However, a number of other efforts provide
guidance on how to use C macros effectively, provide tools for
checking macro usage for given programs, etc.

Carroll and Ellis state that ``almost all uses of macros can be
eliminated from C++ libraries.''~\cite{Carroll95} As evidence of this,
they list eight categories of macro usage, and explain how to convert
these types of constructs into C++ mechanisms.  They do not, however,
discuss automatic conversion, and focus only on instructing the
software engineer on better ways to do Cpp-like things.

Similarly, a number of organizations provide hints about effective
ways to the use the C preprocessor.  One example found on the Web
discusses a set of techniques including simple macros, argument
macros, predefined macros, stringification macros, concatentation
macros, undefining and redefining
macros.\footnote{wwwwbs.cs.tu-berlin.de/\~kraxel/gnuinfo/cpp/Macros.html}
This Web site also identifies a set of ``pitfalls and subtleties of
macros''; these are much like some of the problems our analysis tool
identifies, as discussed below.  Our effort does not only categorize
problems, but it also determines the frequency of appearance of those
problems. 

There are a number of tools to check whether specific C programs
satisfy particular constraints.  The lint program checker is part of
the standard distribution of most Unix systems, checking for uses of C
that are likely to be problematic.  The implementation of lint is
complicated by the fact that it tries to replicate significant
functions of both the C compiler and also the preprocessor.  LCLint, a
recently developed tool at MIT, handles many of the checks that lint
does; furthermore, LCLint allows the programmer to add annotations
that then permit additional checks to be
performed~\cite{Evans-fse94}\cite{Evans-pldi96}.  LCLint is effective
at analyzing macro usage.  Examples of checks that LCLint can
optionally perform on macros that define functions include:
\begin{quote}
$\ldots$ a parameter to a macro may not be used as the left hand side
of an assignment expression $\ldots$, a macro definition must be
syntactically equivalent to a statement when it is invoked followed by
a semicolon $\ldots$, the type of the macro body must match the return
type of the corresponding function $\ldots$\footnote{From Section 8 of
David Evan's LCLint User's Guide, Version 2.2 (August 1996); larch-www.lcs.mit.edu:8001/larch/lclint/guide/guide.html}
\end{quote}

\section{The Data}\label{sec:gathering}

We analyzed 29 freely-available C packages, many taken from the GNU
distribution.  Figure~\ref{fig:packages} shows the packages along with
their sizes.  The size of each package is given in terms of both the
physical line count---the actual numbers of lines in the file (in Unix
terms, this is the number of newline characters plus one)---and also
the non-comment/non-blank (NCNB) line count, which eliminates any
lines that contain only comments or that are blank lines (consisting
only of whitespace).  Throughout the remainder of the analysis, we use
only the NCNB length, since that focuses more clearly on the essential
source text of a package.

\begin{figure}
\begin{tabular}{|l|r|r|}\hline
C Package & Physical Lines & NCNB Lines\\\hline\hline
bc & 7438 & 5177\\\hline
gzip & 9076 & 5787\\\hline\hline

Total & big & num\\\hline
\end{tabular}

\caption{Analyzed Packages and Sizes\label{fig:packages}}
\end{figure}

[FIX: If we could also list the platforms for which each can compile,
that would be great, but I doubt the benefit is worth the effort right
now.]

\section{Occurrence of Preprocessor Directives}\label{sec:directives}

The first question we asked was a simple one: how often do
preprocessor directives appear in C programs?
Figure~\ref{fig:directives} shows the data for the packages we
analyzed.  There are five rows in the figure, each of which has a
separate bar for each of the packages we analyzed.  The topmost row
shows the total percentage of lines in the package that represent
preprocessor directives.  The rows below the topmost one show the
percentage of lines in the package attributed to a subset of
preprocessor directives: (a) \verb+#undef+ is the next row down, (b)
\verb+#include+ is the next row down, (c) \verb+#define+ is the next
row down, (d) the conditional compilation directives (\verb+#ifdef+,
\verb+#ifndef+, \verb+#if+, \verb+#endif+, \verb+#else+, and
\verb+#elif+ are combined into a single group shown in the next row,
and (e) any other directives in the bottom row. 

\begin{figure}
goes here

\caption{Percentages of Directives\label{fig:directives}}
\end{figure}

Perhaps the most surprising information here is the percentage of the
program lines that are preprocessor directives: the number varies by
at least a factor of three across packages [FIX: get this number
right], but in some cases over 11\% [FIX: this number, too] of the
total NCNB lines in a file are preprocessor directives.  Of these,
about half are \verb+#define+ directives; more information about the
breakdown of the \verb+#define+s is given below.

\section{Frequency of Defined Macro Usage}\label{sec:usage}

The second question we asked was: where and how often are macros
defined and used in practice?  Specifically, for each macro we
determined:
\begin{itemize}

\item How many times it was \verb+#define+d.
\item How many times it was \verb+#undef+ined.
\item How many arguments (if any) it defined.
\item How many times (and where) it was mentioned in C source code.
\item How many times (and where) it was mentioned in other
preprocessor directives.

\end{itemize}
[FIX: gjb: This is redundant with the same information in paragraph
form, above]

[FIX: gjb: I'd also like to see info about how often define-d to the same
expansion, or not, etc.]


We display some of these data here, in several different forms.
Figure~\ref{fig:define_count} shows a histogram with cumulative
percentages of the number of times each identifier is \verb+define+d
in each of the packages.  The leftmost column lists the number of
times an identifier is \verb+define+d, with the one through ten counts
listed explicitly and all counts greater than ten collapsed into a
single bin.  For example, looking at the \verb+bc+ package, 98.72\% of
the identifiers were \verb+#define+d between one and six times.

\begin{figure}
goes here

\caption{Frequency of Definition\label{fig:define_count}}
\end{figure}

The \verb+#undef+ counts are generally extremely low; the maximum
number of undefines seen in any package was [FIX: fill in here].

Figure~\ref{fig:use_count} is structured as the last figure, but it
represents the number of times that a defined name is used in either C
code or preprocessor code.

\begin{figure}
goes here

\caption{Frequency of Use\label{fig:use_count}}
\end{figure}

Figure~\ref{fig:define_usage} shows the percentage of defined macros
that are used in C code, in Cpp code, in both, or in neither (i.e. no
uses).

\begin{figure}
goes here

\caption{Use of Defined Macros\label{fig:define_usage}}
\end{figure}


\section{Categorization}\label{sec:categorization}

The third question we asked was: how are the macros used in practice?
In contrast to the earlier analyses, this requires a analysis and
categorization of the bodies of the macros to determine how they can
be used.  This analysis depends in part on some heuristics we defined
to interpret the bodies of the macros; by studying our analyses, we
have refined (and continue to refine) the heuristics to do a better
job of categorization.  (This improvement is important in helping us
produce the C to C++ translation tool we have mentioned.)

[FIX: We should note that we plan to look at uses of macros to help in
this categorizations (though we don't do any of this now)]

The specific categories that our analysis tool identifies are:
\begin{itemize}

\item {\em Constants\/}, where the \verb+#define+ gives only an
identifier name and a value.

\item {\em Null defines\/}, where the \verb+#define+ gives only an
identifier name.

\item {\em Functions\/}, where the \verb+#define+ has one or more
parameters.  The functions are broken down into several sub-categories:
\begin{itemize}

\item {\em Functions/Cpp\/}, where the body of the macro invokes one
or more other macros.  [FIX: This is currently ``function, macro as
function'', I think.]

\item {\em Functions/C-expression\/}, where the body of the macro is
defined as a well-defined C expression.  [FIX: This is currently
``function, some constant'', I think.]

\item {\em Functions/Essential\/}, [FIX: bad name?] where the body of the
macro is not defined as a C expression; specific reasons include
...

These are of special interest with respect to translation.  Some of
them, for instance ones that define C statements rather than
expressions, might yield to translation if further analysis shows that
they are used in particular, disciplined ways.

\end{itemize}

\item {\em Uncategorized\/}, where we could not characterize the macro
as any of the above categories.

\end{itemize}

In anticipation of the translator tool, the analysis tool infers
types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
type information is in the early stages, however, and we do not report
on the preliminary results in this paper.

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.  --not tonight!]

\section{Discussion}\label{sec:discussion}

Even with significant expertise in using C, C++, and the Cpp macro
preprocessor, these data showed us a far broader and diverse use of
preprocessing than we had anticipated.  

We wrote a collection of Perl scripts to produce and analyze these
data.  As we developed them, we slowly shifted from a lexical analysis
of the source to a syntactic analysis of the source.  For example, we
now properly parse virtually all declarations in every package.  We're
pursuing even more analysis, pushing into semantics.  For instance, we
can now determine which identifiers are free variables within a defined
macro.  We are also close to being able to infer the types of the bodies
of defined macros, one of several steps necessary before we can attempt
to build a converter from C to C++.

\section{Conclusion}\label{sec:conclusion}

\small \bibliography{evil}


\end{document}

