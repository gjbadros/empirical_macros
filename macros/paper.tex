\documentstyle[11pt,epsf]{article}

\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt

\topmargin   0pt

\textwidth   6.5 in
\textheight  8.5 in

\begin{document}
\bibliographystyle{plain}

\title{An Empirical Analysis of the Use of the C Preprocessor}

\author{Michael Ernst,\thanks{Email 
addresses: {\tt \{mernst, gjb, notkin\}@cs.washington.edu}.  Contact author:
mernst@cs.washington.edu.}
\and Greg J. Badros \and David Notkin}


\date{Department of Computer
Science and Engineering\\
University of Washington\\
Box 352350\\
Seattle, WA  98195-2350\\
\today}  

\maketitle


\begin{abstract}

The C programming language is intimately connected to its macro
preprocessor.  This relationship affects, indeed generally hinders,
both the tools (compilers, debuggers, call graph extractors, etc.)
built to engineer C programs and also the ease of translating to other
languages such as C++.  In this paper we analyze over 20 packages
comprising 1.5 million lines of publicly available C code, determining
the ways in which the preprocessor is used in practice.  We developed
a framework for analyzing preprocessor usage, using it to extract
information the percentage of preprocessor directives in C programs,
about the frequency of use of defined macros, about the relationship
between C functions and their use of macros, and about the macros that
are difficult to express in terms of other C or C++ language features.
We report on the analysis, in particular illustrating data that are
material when considering the definition of tools for C or translators
to C++.  We present how the results (and the supporting framework) lay
the foundation for developing a tool to translate from C to C++.

\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{kr,ansi} is intimately connected to
its macro preprocessor, Cpp [FIX: any cites?].  Nobody writes
programs in ``pure'' C; facilities of the preprocessor such as
including other files, defining constants and macros, conditional
compilation, etc. are present in essentially every program.  This
relationship between the C language and the preprocessor has a number
of consequences, many of which were probably not anticipated by the
original designers.

Stroustrup puts the consequences of this relationship in perspective:
\begin{quote}
Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders~\cite[p.~424]{Stroustrup-DesignEvolution}.
\end{quote}

As one example, source-level debuggers either map information back to
the original source code or else leave this task to the programmer.
As another related example, call graph extractors generally work in
terms of the post-processed code, even when a human programmer---as
opposed to an optimizing compiler, for example---is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Handling the mapping
between the original and the post-processed source is not easy for many
tools, often leading to situations in which the programmer is
responsible for inferring the mapping, which is an
undesirable and error-prone situation.

A related consequence is the difficulty that the preprocessor adds in
translating programs from C to C++.  For example, Siff and Reps have
described a technique that uses type inferencing to produce C++
function templates from C; however, the input is restricted to be ``a
C program component that $\ldots$ has been pre-processed so that all
include files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  O'Callahan and Jackson also use
type inference, although for program understanding rather than
translation; they, too, apply their techniques to post-processed
code~\cite{OCallahan-icse97}.  

Having tools use post-processed source code increases the dissonance
between what the programmer sees and what the tools analyze.  There
are several ways to try to reduce this problem; Stroustrup suggests
the following:
\begin{quote}
I'd like to see Cpp abolished.  However, the only realistic and
responsible way of doing that is first to make it redundant, then
encourage people to use the better alternatives, and {\em
then\/}---years later---banish Cpp into the program development
environment with the other extra-linguistic tools where it
belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}

As a step in this direction, we performed an empirical analysis of the
ways in which the preprocessor is used in a large collection of C
programs; we analyzed nearly 30 packages that comprise well over one
million lines of code.  The intent of the analysis is to build a
better understanding of how the preprocessor is used.  In turn, this
will help us to build a C to C++ conversion tool with two attractive
properties: one, it will take as input C programs complete with
preprocessor directives; and two, it will map many---preferably
most---uses of directives into C++ language features.\footnote{It is
not practical to eliminate all uses of Cpp since C++ currently
provides no replacement for the \verb+\#include+ directive.}  The
framework we developed for analyzing preprocessor usage provides a
basis for the development of such a conversion tool.

In this paper, we report on several specific analyses that we
performed.  
\begin{itemize}

\item We computed the percentage of original C source code lines that
were preprocessor directives, including a breakdown of the frequency
of specific directives (such as \verb+#define+).  It was not unusual
to find C programs in which over 10\% of the total lines were
preprocessor directives.

\item We computed which macros were defined, how often they were
defined, how often they were undefined, and the number of times they
were expanded both from other preprocessor directives and also from
true C source code.  [FIX: pithy observation goes here]

\item We categorized definitions of macros according to what they
      expanded to; for example, we determined which macros simply
      defined a preprocessor symbol, which defined constants, etc.  We
      were particularly interested in determining which macros are
      defined in a way that is difficult to convert to other language
      features.  [FIX: pithy observation goes here]

\end{itemize}

Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of C
programming support tools.  On the other hand, the analysis also
convinces us that, by extending our analysis framework with some class
type inferencing techniques (similar to those used by Siff and
Reps~\cite{Siff-fse96}, O'Callahan and Jackson~\cite{OCallahan-icse97},
and others), we can take significant steps towards a tool that usefully
converts a high percentage of C code into C++.  [Two important points to
address: C code almost *is* C++ code, so what do we gain from a
``conversion''?  If we can't get all the code, can we identify the parts
we can't get?]

\section{Related Work}\label{sec:related}

\section{The Data}\label{sec:gathering}

We analyzed 29 freely-available C packages, many taken from the GNU
distribution.  Figure~\ref{fig:packages} shows the packages along with
their sizes.  The size of each package is given in terms of both the
physical line count---the actual numbers of lines in the file (in Unix
terms, this is the number of newline characters plus one)---and also the
non-comment/non-blank (NCNB) line count, which eliminates any lines that
contain only comments or that are blank lines (consisting only of
whitespace).  Throughout the remainder of the analysis, we use only the
NCNB length, since that focusses more clearly on the essential source
text of a package.

\begin{figure}
%\epsfxsize 6in
%\centerline{\epsfbox{mc.eps}}
\begin{tabular}{|l|r|r|}\\\hline
C Package & Physical Lines & NCNB Lines\\\hline\hline
bc & 7438 & 5177\\\hline
gzip & 9076 & 5787\\\hline

Total & big & num\\\hline
\end{tabular}

\caption{Analyzed Packages and Sizes\label{fig:packages}}
\end{figure}

[FIX: If we could also list the platforms for which each can compile,
that would be great, but I doubt the benefit is worth the effort right
now.]

\section{Occurrence of Preprocessor Directives}\label{sec:directives}

The first question we asked was a simple one: how often do
preprocessor directives appear in C programs?
Figure~\ref{fig:directives} shows the data for the packages we
analyzed.  There are five rows in the figure, each of which has a
separate bar for each of the packages we analyzed.  The topmost row
shows the total percentage of lines in the package that represent
preprocessor directives.  The rows below the topmost one show the
percentage of lines in the package attributed to a subset of
preprocessor directives: (a) \verb+#undef+ is the next row down, (b)
\verb+#include+ is the next row down, (c) \verb+#define+ is the next
row down, (d) the conditional compilation directives (\verb+#ifdef+,
\verb+#ifndef+, \verb+#if+, \verb+#endif+, \verb+#else+, and
\verb+#elif+ are combined into a single group shown in the next row,
and (e) any other directives in the bottom row. 

\begin{figure}
goes here

\caption{Percentages of Directives\label{fig:directives}}
\end{figure}

\section{Occurrence of Preprocessor Directives}\label{sec:directives}

The first question we asked was simple: how often do preprocessor
directives appear in C programs?  Figure~\ref{fig:gzip_directives} shows
the data for the gzip package.  The top bars shows the percentage of
lines (split into the three categories discussed above) coming from all
preprocessor directives; the remaining bars show the percentages of each
specific directive as labeled. 

Perhaps the most surprising information here is the percentage of the
program lines that are preprocessor directives: the number varies by
at least a factor of three across packages [FIX: get this number
right], but in some cases over 11\% [FIX: this number, too] of the
total NCNB lines in a file are preprocessor directives.  Of these,
about half are \verb+#define+ directives; more information about the
breakdown of the \verb+#define+s is given below.

\section{Frequency of Defined Macro Usage}\label{sec:usage}

The second question we asked was: where and how often are macros
defined and used in practice?  Specifically, for each macro we
determined:
\begin{itemize}

\item How many times it was \verb+#define+d.
\item How many times it was \verb+#undef+ined.
\item How many arguments (if any) it defined.
\item How many times (and where) it was mentioned in C source code.
\item How many times (and where) it was mentioned in other
preprocessor directives.

[FIX: This is redundant with the same information in paragraph form, above]

\end{itemize}

We display some of these data here, in several different forms.
Figure~\ref{fig:define_count} shows a histogram with cumulative
percentages of the number of times each identifier is \verb+define+d
in each of the packages.  The leftmost column lists the number of
times an identifier is \verb+define+d, with the one through ten counts
listed explicitly and all counts greater than ten collapsed into a
single bin.  For example, looking at the \verb+bc+ package, 98.72\% of
the identifiers were \verb+#define+d between one and six times.

\begin{figure}
goes here

\caption{Frequency of Definition\label{fig:define_count}}
\end{figure}

The \verb+#undef+ counts are generally extremely low; the maximum
number of undefines seen in any package was [FIX: fill in here].

Figure~\ref{fig:use_count} is structured as the last figure, but it
represents the number of times that a defined name is used in either C
code or preprocessor code.

\begin{figure}
goes here

\caption{Frequency of Use\label{fig:use_count}}
\end{figure}

Figure~\ref{fig:define_usage} shows the percentage of defined macros
that are used in C code, in Cpp code

\begin{figure}
goes here

\caption{Use of Defined Macros\label{fig:define_usage}}
\end{figure}


\section{Categorization}\label{sec:categorization}

The third question we asked was: how are the macros used in practice?
In contrast to the earlier analyses, this requires a analysis and
categorization of the bodies of the macros to determine how they can
be used.  This analysis depends in part on some heuristics we defined
to interpret the bodies of the macros; by studying our analyses, we
have refined (and continue to refine) the heuristics to do a better
job of categorization.  (This improvement is important in helping us
produce the C to C++ translation tool we have mentioned.)

[FIX: We should note that we plan to look at uses of macros to help in
this categorizations (though we don't do any of this now)]

The specific categories that our analysis tool identifies are:
\begin{itemize}

\item {\em Constants\/}, where the \verb+#define+ gives only an
identifier name and a value.\footnote{In anticipation of the
translator tool, the analysis tool infers types; this is in the early
stages, however, and we do not report on the preliminary results in
this paper.}

\item {\em Null defines\/}, where the \verb+#define+ gives only an
identifier name.

\item {\em Functions\/}, where the \verb+#define+ has one or more
parameters.  The functions are broken down into several sub-categories:
\begin{itemize}

\item {\em Functions/Cpp\/}, where the body of the macro invokes one
or more other macros.  [This is currently ``function, macro as
function'', I think.]

\item {\em Functions/C-expression\/}, where the body of the macro is
defined as a well-defined C expression.  [This is currently
``function, some constant'', I think.]

\item {\em Functions/Essential\/}, [bad name?] where the body of the
macro is not defined as a C expression; specific reasons include
...

These are of special interest with respect to translation.  Some of
them, for instance ones that define C statements rather than
expressions, might yield to translation if further analysis shows that
they are used in particular, disciplined ways.

\end{itemize}

\item {\em Uncategorized\/}, where we could not characterize the macro
as any of the above categories.

\end{itemize}

[FIX: Should this also include Mike's manual breakdown into categories
for gzip.]

\section{Discussion}\label{sec:discussion}

What do these analyses tell us about the use of the C pre-processor?
[FIX: well?] 

We wrote several Perl scripts to produce and analyze these data.  As
we developed them, we slowly shifted from a lexical analysis of the
source to a syntactic analysis of the source.  For example, we now
properly parse all declarations in every package.  We're pursuing even
more analysis, pushing into semantics.  For instance, we can now
determine which identifiers are free variables within a defined macro.
We are also close to being able to infer the types of the bodies of
defined macros, one of several steps necessary before we can attempt
to build a converter from C to C++.

\section{Conclusion}\label{sec:conclusion}

\small \bibliography{evil}


\end{document}

