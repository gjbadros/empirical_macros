% $Id$
\documentclass[10pt]{article}
\usepackage{epsfig}
\usepackage{dcolumn}
\usepackage{alltt}

\def\numpackages{30}
\def\numlines{2 million}
\def\numlinesexact{nearly 2 million}
\def\numlinesncnbexact{about 1.4 million}

\newcommand{\pkg}[1]{\textsf{#1}}

\newcommand{\file}[1]{\texttt{#1}}

% the "fullpage" package does almost the same thing
% as the below lines-- it doesn't make things quite as
% tall or wide, but is generally what I use
% \usepackage{fullpage}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt
\topmargin   0pt
\headsep 0pt
\headheight 0pt
\textwidth   6.5 in
\textheight  9 in

\renewcommand{\floatpagefraction}{.8} %default .5
% Avoid putting all figures at end of text.
\renewcommand{\textfraction}{.1}  % .2 is the default
\renewcommand{\topfraction}{.9}   % .7 is the default

\begin{document}
% \bibliographystyle{plain}
\bibliographystyle{alpha}

\title{An Empirical Analysis of C Preprocessor Use}

\author{Michael D. Ernst%
  \and Greg J. Badros%
  \thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.}
  \and David Notkin}

\date{% Technical Report UW-CSE-97-04-06 \\
Department of Computer Science and Engineering \\
University of Washington \\
Box 352350, Seattle, WA  98195-2350  USA \\
{\small \{{\tt mernst},{\tt gjb},{\tt notkin}\}{\tt @cs.washington.edu}} \\
11 October 1997}  

\maketitle

\begin{abstract}
  The C programming language is intimately connected to its macro
  preprocessor Cpp.  This relationship hinders tools built to engineer C
  programs, such as compilers, debuggers, call graph extractors, and
  translators.  Most tools make no attempt to analyze macro usage, but simply
  preprocess their input, which has a number of negative consequences.  In
  order to determine how the preprocessor is used in practice, and the
  feasibility of automatic analysis of preprocessor use, this paper
  analyzes {\numpackages} packages comprising {\numlines} lines of publicly
  available C code.  We determine the incidence C preprocessor usage which
  is complex, potentially problematic, or inexpressible in in terms of
  other C or C++ language features.
[[We also came up with the list of things to investigate; and along the way
we taxonomized some stuff, which is also a contribution.]]

  We particularly note data that are
  material to the development of tools for C or C++, including translating
  from C to C++ to reduce preprocessor usage.  The results are of interest
  to language designers, tool writers, programmers, and software engineers.
\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is incomplete without its macro
preprocessor, Cpp~\cite[Ch.~3]{Harbison91}, which supplies such facilities
as file inclusion, definition of constants and macros, and conditional
compilation.  While disciplined use of the preprocessor can reduce
programmer effort and improve portability, performance, or readability, Cpp
is widely viewed as a source of difficulty for understanding and
transforming C programs.  Because of Cpp's lack of structure\,---\,its
inputs and outputs are raw token streams\,---\,Cpp is very flexible, but
lends itself to arbitrary source code manipulations that complicate
understanding of the program by both software engineers and tools.  In the
worst case, the preprocessor makes merely determining the program text as
difficult as determining the output of an ordinary program.  The designer
of C++, which shares C's preprocessor, also noted these problems:
``Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders.''~\cite[p.~424]{Stroustrup-DesignEvolution}

While much has been written about Cpp's potential pitfalls, no previous
work has examined actual use of the C preprocessor to determine whether it
presents a practical or merely theoretical obstacle to program
understanding, analysis, and transformation.  This paper fills that gap by
examining CPP use in {\numpackages} programs comprising {\numlines} lines
of source code.

We identified a number of potential pitfalls proceeding from preprocessor
use, including
\begin{description}
\item[high total use]  heavy use of either macro substitution or
  conditional compilation can overwhelm a human or tool; particularly
  problematic are lines that depend on many macros or macros that control
  many lines
\item[complicated bodies]  a macro body need not expand to a complete
  C syntactic entity (like a statement or expression)
\item[extra-linguistic features]  a macro body may exploit features of
  the preprocessor not available in C, such as stringization, token
  pasting, or use of free variables
\item[multiple definitions]  uncertainty about the expansion of a macro
  prevents knowledge of the actual program text; even more problematically,
  two definitions of a macro may be incompatible, for instance if one is a
  statement and the other expands to an expression or type
\item[macro pitfalls]  macros introduce new varieties or programming
  errors, such as function-like macros that fail to swallow a following
  semicolon and macros that fail to parenthesize, or side-effect, uses of
  formal variables
\item[inconsistent usage]  a macro used both for conditional
  compilation and to expand code is harder to understand than one used just
  for one purpose or the other
\item[mixed tests]  a single Cpp conditional may test conceptually
  distinct, unrelated conditions, making it difficult to perceive the
  intention
\item[variation in use]  if there is no clear pattern of use, or
  commonly-repeated paradigms, then no obvious point of attack presents
  itself
  %% No pattern according to package size, relative or absolute Cpp use, etc.
\end{description}
We report in detail on each of these aspects of preprocessor use,
indicating which appear to be innocuous in practice (that is, the
problematic uses appear only infrequently) and which may prove problematic
for software engineers.  We also present new taxonomies of macro body
expansions, macro feature usage, macro pitfalls, and conditional
intentions.  These taxonomies improve on previous work by being more
detailed and more accurately reflecting actual use.

Sections~\ref{sec:first-content-section}--\ref{sec:last-content-section}
present the bulk of these results.  Section~\ref{sec:methodology} describes
our experimental methodology.  Section~\ref{sec:conclusion} discusses the relevance of the research,
suggests techniques for mitigating the negative impact of Cpp on program
understanding, and discusses avenues for future work, while
section~\ref{sec:related} discusses related work.
The remainder of this section [[does some stuff]].

[[Point reader at conclusion/summary of results, at end of paper (and write
that!).]]

% \subsection{Outline}
% 
% The remainder of this paper is organized as follows.
% 
% Section~\ref{sec:directives} reports the percentage of original C source
% code lines that are preprocessor directives, including a breakdown of the
% frequency of specific directives such as {\tt \#define}.  C programs
% commonly have preprocessor directives as over 10\% of their total lines,
% and over 20\% of the lines were directives in 3 of the {\numpackages}
% packages.
% 
% Section~\ref{sec:usage} reports how often each macro is defined and
% expanded.   Identifiers tend to be {\tt \#define}d relatively few times
% (96\% of macro identifiers had three or fewer definitions).  Many packages
% also have a significant number of macros that are never expanded, even
% disregarding system and library header files.
% 
% Section~\ref{sec:categorization} categorizes macro definitions according to
% their expansions; for example, macros may simply define a preprocessor
% symbol, define a literal, expand to a statement, etc.  We were particularly
% interested in determining the frequency of use of macros that are difficult
% to convert to other language features, such as those that string together
% characters as opposed to manipulating lexemes or syntactic units (less than
% one third of one percent of all macro definitions),
% those that expand to partial syntactic units such as unbalanced
% braces or partial declarations (half of one percent), and others not 
% directly expressible in the programming language (about four percent).
% 
% Section~\ref{sec:conclusion} discusses the relevance of the research,
% suggests techniques for mitigating the negative impact of Cpp on program
% understanding, and discusses avenues for future work, while
% section~\ref{sec:related} discusses related work.


[[Does anyone care about this?
Another niche already filled by our tool is that of a ``macro lint''
program which warns of potentially dangerous (or non-standard) uses of Cpp.
And, we wrote CPPP.]]



% In order to assess the practical difficulty of understanding uses of CPP
% (and the potential for replacement by other language constructs), 





Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of C
programming support tools.  On the other hand, the analysis also convinces
us that, by extending our analysis framework with some class type
inferencing techniques (similar to those used for C to C++
translation~\cite{Siff-fse96} and for program
understanding~\cite{OCallahan-icse97}), we can take significant
steps towards a tool that usefully converts a high percentage of Cpp code
into C++ language features.  We are interested not in translations
that merely allow a C program to be compiled by a C++ compiler (which is
usually easy, by intentional design of C++) but those that take advantage
of the added richness and benefits of C++ constructs.

[[Inane, content-free.  Must die.]]
In terms of the complexity of preprocessor usage, the results reported here
contain both good news and bad.  By far
the largest number of macro definitions and uses are relatively simple, of
the variety that a programmer could understand without undue effort (although
perhaps requiring tedious work) or that a relatively unsophisticated tool
could understand (although in practice very few even try).  Despite the
preponderance of innocuous macros, the preprocessor is so heavily used that
the remaining ones are numerically significant.  It is precisely these
macros that are mostly likely to cause difficulties, and there are enough
of them to be problematic in practice and to make the effort of
understanding, annotating, or eliminating them worthwhile.


\subsection{Coping with Cpp}

Tools\,---\,and, to a lesser degree, software engineers\,---\,have three
options for coping with Cpp.    They may ignore preprocessor directives
(including macro definitions) altogether, accept only post-processed code
(usually by running Cpp on their input), or attempt to emulate the
preprocessor.

Ignoring preprocessor directives is an option for approximate tools, such
as those based on lexical or approximate parsing techniques.  Accurate
information about function extents, scopes, declared variables and
functions, and other aspects of a program requires addressing the
preprocessor.

Operating on post-processed code, the most common strategy, is simple to
implement, but then the tool's input differs from what the
programmer sees.  Even when line number mappings are maintained, other
information is lost in the mapping back to the original source code.
For instance, source-level debuggers have no symbolic names or types
for constants and functions introduced via {\tt \#define}, nor can tools
trace or set breakpoints in function macros, as they can for ordinary
functions (even those that have been inlined~\cite{Zellweger83:TR}).
As another example, Siff
and Reps describe a technique that uses type inference to produce
C++ function templates from C; however, the input is ``a C program
component that $\ldots$ has been preprocessed so that all include
files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
the readability and reusability of the resulting C++ templates.  As
yet another related example, call graph extractors generally work in
terms of the post-processed code, even when a human is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
leave the software engineer responsible for inferring the mapping between the
original and the post-processed source, which is an undesirable and
error-prone situation.

A tool that first preprocesses code, or takes already-preprocessed code as
input, cannot be run on a non-syntactic program or one that will not
preprocess on the platform on which the tool is being run.  These
constraints complicate porting and maintenance, two of the situations in
which program understanding and transformation tools are most likely to be
needed.  Additionally, a tool supplied with only one post-processed
instantiation of the source code cannot reason about the program as a
whole, but only about that version that results from one particular set of
preprocessor variables.  For instance, a bug in one configuration may not
be discovered despite exhaustive testing of other configurations that do
not incorporate particular code or do not admit particular execution paths.

The third option, emulating the preprocessor, is fraught with difficulty.
Macro definitions consist of complete tokens but need not be complete
expressions or statements.  Conditional compilation and alternative macro
definitions lead to very different results from a single original program
text.  Preprocessing adds complexity to an implementation, which must trade
off performing preprocessing against maintaining the code in close to its
original form.  Extracting structure from macro-obfuscated source is not a
task for the faint-hearted.  Despite these problems, in many situations
only some sort of preprocessing or Cpp analysis can produce useful answers.

All three approaches would be unnecessary if programs did not use
preprocessor directives.  This is exactly what Stroustrup suggests:
\begin{quote}
  I'd like to see Cpp abolished.  However, the only realistic and
  responsible way of doing that is first to make it redundant, then
  encourage people to use the better alternatives, and {\em then\/}\,---\,years
  later\,---\,banish Cpp into the program development environment with the
  other extra-linguistic tools where it
  belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}
C++ contains features\,---\,such as constant variables, inline functions,
templates, and reference parameters\,---\,that obviate many uses of Cpp.
Thus, translation to C++ is a path for partial elimination of Cpp.
This study indicates the
feasibility\,---\,and our framework for analyzing preprocessor usage
provides a basis for the development\,---\,of an automatic translator with
two attractive properties.  It would take as input C programs complete with
preprocessor directives, and it would map many uses of directives into C++
language features.  (It is not 
practical to eliminate all uses of Cpp.  For example, C++ currently
provides no replacement for the {\tt \#include} directive, or for
stringization or pasting.  Macros that cannot be eliminated might be
annotated with their types or 
effects on parser or program state, so that even tools that do no Cpp
analysis can operate correctly on such programs.)

[[Where do these two points go?]]

Another niche already filled by our tool is that of a ``macro lint''
program which warns of potentially dangerous (or non-standard) uses of Cpp.

And, we wrote CPPP.

%O'Callahan and Jackson also use type
%inference, although for program understanding rather than translation;
%they, too, apply their techniques to post-processed
%code~\cite{OCallahan-icse97}.


\subsection{Cpp: not all bad}

[[This section is out of place and horrible.  Where should the information
go?]]

Despite its evident shortcomings, Cpp is a useful and often necessary
adjunct to C, for it provides capabilities unavailable in the language or
its implementations.  Cpp permits definition of portable language
extensions that can define new syntax, abbreviate repetitive or complicated
constructs, or eliminate reliance on a compiler implementation to
open-code (inline) functions, propagate symbolic constants, eliminate dead
code, and short-circuit constant tests.  The latter guarantees are
especially valuable for compilers that do a poor job optimizing or when the
programmer wishes to override the compiler's heuristics.  Cpp also permits
system dependences to be made explicit and tested, resulting in a clearer
separation of concerns.  Finally, Cpp permits a single source to contain
multiple different dialects of C; a frequent use is to support both
K\&R-style and ANSI-style declarations.

%% NEED A REFERENCE TO DEBUGGER HERE!
%% also mention Emacs hide-ifdef mode

Our long-term goal is not to take these useful features away from
programmers, but to reduce Cpp use, making programs easier for humans to
understand and tools to analyze.






\section{Methodology}
\label{sec:methodology}

We analyzed {\numpackages} publicly-available software packages which
represent a mix of application domains, authors, programming styles, and
sizes.  Some are interactive, while others are not, and some are graphical
while others are text-based, command-line applications.
Figure~\ref{fig:packages} describes the packages and lists their sizes in
terms of physical lines (newline characters) and non-comment, non-blank
(NCNB) lines.  The NCNB figure disregards lines consisting of only comments
or whitespace, as well as lines in a conditional that cannot evaluate to
true (such as {\tt \#if 0}, which is frequently used to comment out code).  The
remainder of our analysis uses the NCNB length, which more accurately
reflects the amount of source code.

\begin{figure}
\centering
{% ``\small'' here does have an effect, despite previous comment to the contrary
  \small
  \setlength{\tabcolsep}{.25em}
  \input{tbl-package-sizes.tex}
}
\caption{Analyzed packages and their sizes.  NCNB lines are non-comment,
  non-blank lines.}
\label{fig:packages}
\end{figure}

Before performing our analysis, we ran {\tt configure} or the equivalent on
each package in order to prepare it for compilation.  This creates various
header files (such as \file{configure.h}).  Our analysis works in the
absence of this step, but warns about uses of never-defined macros and
under-reports some values related to those missing definitions.  We did
not, however, compile the packages; our analysis does not require that the
package be compilable (or even that the preprocessor be able to run on it).

We generated a list of all the C files in the package (both code and header
files).  In general, these have extensions like \file{.c}, \file{.h},
\file{.cc}, \file{.cpp}, \file{.hxx}, etc.  We removed files with such
extensions that don't actually contain valid C, and added others which were
{\tt \#included} by valid C files (some included files had extension
\file{.def}).

We analyzed all the C files in the package, as well as every file {\tt
\#include}d by any of those, primarily library header files.  We
took care to include as many libraries as possible so as not to overlook
those definitions.  However, the figures reported in this paper always omit
all macros defined only in libraries, as well as all macro uses in
libraries.  This prevents libraries from 
swamping the characteristics of the package code, which is our focus in
this study.  The programmer generally has no control over libraries and
their header files, and may not even know whether a library symbol is
defined as a macro or a true function or variable.  Finally, we assume that
library macros are carefully written to behave correctly and robustly
(sadly, we discovered this not to always be true in practice; an
analysis of library macros is outside the scope of this report).

Our analysis is a true whole-program analysis; rather than preprocessing the
code and examining just one configuration, we examine all possible code and
ignore no possible  conditional compilation conditions (though we do skip
over those which can be statically proven to be false).   As a result, this
analysis is more thoroughgoing than traditional ``whole-program'' analyses.

[[We merge different Cpp branches in the code wherever possible.  Discuss
this a bit.]]

We perform approximate parsing because the input is not a valid C program;
as a result, we miss some constructs, but we can cope with uncompilable C
and with partial constructs in conditional compilation branches.  Our tool
includes parsers for expressions, statements, and declarations.

We performed our analysis via a collection of Perk scripts, totaling
approximately 11,000 lines (7,000 NCNB lines).  The raw data, which
includes considerable data not reported here, and the programs used to
generate and manipulate them, are available from the authors.







\section{Occurrence of preprocessor directives}
\label{sec:directives}
\label{sec:first-content-section}

Figure~\ref{fig:directives-breakdown} shows how often preprocessor
directives appear in the programs we analyzed.  Each group of bars in the
figure represents the percentage of non-comment, non-blank (NCNB) lines
attributed to the specified category of directives, with each individual
bar showing the percentage for a specific package.  Conditional compilation
directives ({\tt \#if}, {\tt \#ifdef}, {\tt \#ifndef}, {\tt \#else}, {\tt
\#elif}, {\tt \#endif}) are grouped together.

\begin{figure}
\centerline{\epsfig{file=fig/directives-breakdown.eps,height=7.5in}}
\caption{Preprocessor directives as a fraction of non-comment,
  non-blank (NCNB) lines.}
\label{fig:directives-breakdown}
\end{figure}

The prevalence of preprocessor use makes understanding CPP constructs
crucial in any analysis of a program.  One in ten program lines is a
preprocessor directive rather than C code.  Across
packages, the percentage varies from less than 4\% to more than 22\%.
(These figures do not include the 28\% of lines which expand a macro or the
38\% of lines whose inclusion is controlled by {\tt \#if}; see
section~\ref{sec:dependence}.)

% \#if 46\%, \#define 35\%, \#include 13\%, \#undef 3\%, \#line 2\%

Conditional compilation directives account for just under half (46\%) of
the total directives in all packages, macro definitions comprise another
35\%, and file inclusion makes up most of the rest.  Packages are not very
uniform in their mix of preprocessor directives, however.  (If they were,
each group of bars in figure Figure~\ref{fig:directives-breakdown} would be
a scaled version of the top group.)  In particular, the prevalence of {\tt
\#include} is essentially independent of incidence of other directives.
The percentage of conditional directives varies from 16\% to 74\%, the
percentage of {\tt \#define} varies from 14\% to 52\%, and the percentage
of {\tt \#include}s varies from 4\% to 60\%.  This variation in usage
indicates that a tool for understanding Cpp cannot focus on just a subset
of directives.  


\subsection{{\tt \#line}, {\tt \#undef}, and other directives}

The definedness of a macro is often used as a boolean value.  However, {\tt
\#undef} is rarely used to set such macros to ``false''$\!$.  Most uses of
{\tt \#undef} immediately precede a definition of the just-undefined macro,
to avoid preprocessor warnings about incompatible macro redefinitions.

Every use of {\tt \#line} (in \pkg{bash}, \pkg{cvs}, \pkg{flex}, \pkg{fvwm},
\pkg{gawk}, \pkg{gcc}, \pkg{groff}, and \pkg{perl}) appears in lex or yacc
output that enables packages to build on systems lacking lex, yacc, or
their equivalents.  For instance, \pkg{flex} uses itself to parse its
input, but also includes an already-processed version of its input
specification (that is, C code corresponding to a {\tt .l} file) for
bootstrapping.

% , as are ``other'' directives (such as ).  
% as well as user-defined ones like {\tt \#module}

Rarely-appearing directives such as {\tt \#pragma}, {\tt \#assert}, and
{\tt \#ident}, and unrecognized directives, are omitted from
figure~\ref{fig:directives-breakdown}.  Among the packages we studied,
these directives account for .017\% of directives, or one in six thousand.
Their only significant user is \pkg{g77}, which contains 154 uses of {\tt
\#error} (representing 1.5\% of its preprocessor directives and 0.16\% of
its NCNB lines) to check for incompatible preprocessor flags.  We ignore
the null command (``{\tt \#}'' followed by only whitespace), which produces
no output.


\subsection{Packages with heavy preprocessor use}

The \pkg{gzip}, \pkg{remind}, and \pkg{bash} packages deserve
special attention for their heavy preprocessor usage\,---\,22\%, 21\%, and
16\%, respectively.

\pkg{gzip} {\tt \#define}s disproportionately many macros as literals and
uses them as arguments to system calls, enumerated values, directory
components, and more.  These macros act like {\tt const} variables and are
evidence of good programming style.  \pkg{gzip} also contains many
conditional compilation directives, since low-level file operations (such
as setting creation time and access control bits, accessing directories,
and so forth) are done differently on different systems.

\pkg{remind} supports speakers of multiple natural languages by using {\tt
\#define}d constants for basically all user output.  It also contains
disproportionately many conditional compilation directives; over half of
these test the definedness of \verb|HAVE_PROTO|, in order to provide both
K\&R and ANSI prototypes.

Like \pkg{gzip}, \pkg{bash} is portable across a large variety of
systems, but \pkg{bash} uses even more operating system services.
Ninety-seven percent of \pkg{bash}'s conditional compilation directives
test the definedness of a macro whose presence or absence is a boolean
flag indicating whether the current system supports a specific feature.
The presence or absence of a feature requires different (or sometimes
additional) system calls or other code.


\section{Macro definition bodies}
\label{sec:categorization}

This section examines features of macro definitions that may complicate
understanding the containing program.  We report how many macro definitions
expand to a partial or unidentifiable syntactic entity, take advantage of
special Cpp features that lie outside the programming language, or contain
other error-prone constructs.  We then turn to multiple definitions of a
particular macro name.  Multiple definitions can complicate understanding,
even if they do effectively the same thing.  We report on incidence of
redefinitions and of differing redefinitions, especially redefinitions with
incompatible bodies.

[[Punchline/summary goes here.]]



\subsection{Macro body categorization}

We categorized macro bodies into 28 categories, though for simplicity of
presentation, this paper coalesces these into ten higher-level categories.
We started with a set of categories that we expected to occur frequently
(similar to other macro
taxonomies~\cite{Stroustrup-DesignEvolution,Carroll95}), then iteratively
refined them to break up overbroad categories or add unforeseen ones.

Figure~\ref{fig:categorization} reports, for each package, how many
definitions create an expansion which falls in each category.  Macros which
act like C language constructs\,---\,such as variables or
functions\,---\,are easiest to understand and to translate into the
programming language, so there is reason to be optimistic for the 70\% of
macros whose bodies are expressions and the 6\% that are statements.  Other
macros, especially those which do not expand to a complete syntactic
construct, are more problematic.


The ten categories are as follows.  The examples are chosen for clarity
and brevity from the packages studied.

% Where does this go?
% There's no pattern, again.  (Nor is there a pattern by package size
% or by type of application.)

% The following isn't quite enough to get the columns lined up in this table.
% \newcolumntype{d}{D{.}{.}{2}}
% \begin{tabular}{|l|d|d|d|d|d|d|d|}\hline
\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{categories-tbl.tex}}%
% }
\centerline{\epsfig{file=fig/def-categories.eps,height=6in}}
\caption{Categorization of macro definition bodies.  The legend numerically
  represents the information in the top row[[; the category names in the
  legend should be read across, by rows]].}
\label{fig:categorization}
\end{figure}


{\input{expl-categories.tex}}


%In anticipation of the translator tool, the analysis tool infers
%types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
%and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
%type information is in the early stages, however, and we do not report
%on the preliminary results in this paper.

% [FIX: Benefits even from simple literal constant conversion -- exposes
% symbolic information to the debugger]

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.]



\subsection{Extra-linguistic capabilities}
\label{sec:extra-linguistic}

The C preprocessor has capabilities outside the C programming language;
indeed, this is a primary motivation for using Cpp.  Such constructs can
present special challenges to program understanding, and especially to
elimination of the preprocessor by translation into C or C++.  This section
presents a list of such features and evaluates their frequency of
appearance, both individually and in combination, in our test suite.

We had anticipated problems dealing with macros that use stringization and
pasting, the two explicit extra-linguistic features of Cpp.  These macros
appear in less than one tenth of one percent of all macro definitions.  Far
more prevalent is exploitation of Cpp's lack of structure to effect
mechanisms not available in C\@.  In particular, the fact that Cpp's inputs
and outputs are uninterpreted token streams with no constraints on their
structure permits Cpp to perform arbitrary transformations using
non-first-class and partial syntactic constructs.  Macros that manipulate
types or partial types are moderately common and problematic for program
understanding tools.

\begin{figure}
  {\small\centerline{\input{tbl-subset-properties.tex}}}
  
  \caption{Usage, both singly and in
    combination, of the extra-linguistic capabilities of the C
    preprocessor listed in section~\ref{desc:properties}.  The features are
    listed across the top, along with the percentage of macro definitions
    exploiting each.  Each row of the table reports the percentage of all
    macro definitions that use a particular combination of the
    capabilities, indicated by black squares.  For instance, 0.093\% of all
    macro definitions perform assignment and use the result of a macro
    invocation as a type, but use none of the other extra-linguistic
    features listed.  The last five lines each represent one macro
    definition.}
  \label{fig:subset-properties}
\end{figure}

One in six macros (16.8\%) contains an extra-linguistic construct or a
construct that can be used by Cpp to achieve an extra-linguistic effect.
Figure~\ref{fig:subset-properties} breaks down these macros by the
constructs they contain.  In addition to showing the prevalence of each
construct, the figure shows which ones occur together.  The following list
describes in detail the the constructs appearing in
figure~\ref{fig:subset-properties}.

{\input{expl-properties.tex}}



\subsection{CPP pitfalls:  macro lint}
\label{sec:lint}

Cpp gives rise to syntactically valid but potentially dangerous programming
constructs which might cause a macro not to behave as anticipated (and not
to behave like an ordinary function).  Unlike the extra-linguistic
constructs discussed in section~\ref{sec:extra-linguistic}, these are more
likely to represent bugs than to use Cpp mechanisms to achieve results
outside the abilities of the C language.  However, some of the potential
bugs (such as multiple execution of an actual parameter's [[word choice:
I'm looking for the parallel of ``formal parameter'']] side effects) do
result from differences between C's standard execution model and the
macro expansion performed by Cpp.

Because it flags such errors, our tool can be used as a sort of macro lint.
(A more polished implementation would permit each warning message to be
enabled and disabled individually.)

\begin{figure}
  {\small\centerline{\input{tbl-lint.tex}}}
  
  \caption{Macro lint.  This table indicates the frequency of occurrence of
    error-prone constructs in macro bodies.  Except where specifically
    noted, the percentages refer to the number of macro definitions.  Macro
    definitions falling in the ``not C code'' category (see
    page~\pageref{page:not-c-code}) are omitted.}
  \label{fig:macro-lint}
\end{figure}

One fourth of all macro definitions triggered at least one such warning,
and one fourth of macro names have a definition which triggers a warning.
Figure~\ref{fig:macro-lint} further breaks down the warnings.  Most of
these represent legitimate problems with the way macros are written and
represent bugs waiting to happen.  If they are not currently a problem,
that is because their present uses happen not to trigger the problematic
conditions, as checks of some of the macros indicated.  Future uses,
especially by programmers not familiar with the (generally undocumented)
caveats relating to use of each macro, may well give rise to these dormant
errors.  We were surprised that more macros did not take simple precautions
such as preventive parenthesization or wrapping macro bodies in {\tt do
{\rm \ldots} while (0)}.

The following further describes the errors collected in
figure~\ref{fig:macro-lint}.

{\input{expl-lint.tex}}

In addition to these errors, our tool discovered a number of additional
errors.  There were some illegal constructs, such as {\tt \#module} (which
is not a meaningful Cpp directive) and {\tt \#undef
\verb|GO_IF_INDEXABLE_BASE|(X, ADDR)} ({\tt \#undef} takes a macro name,
not the arguments as they appeared in the {\tt \#define} directive).
  
Use of {\tt /**/}-style pasting is not uncommon, especially in {\tt CONCAT}
macros which provide portability across older and newer versions of the
preprocessor.  Pre-ANSI versions of Cpp merged adjacent symbols, and the
symbols are juxtaposed by placing a comment, which Cpp removes, between
them.  This construct does not perform merging in newer implementations,
so users are warned of its appearance.

A number of files in our test suite begin or end inside a brace scope or an
{\tt \#if} scope.  Some of these were intentional\,---\,as in files meant
to be included around other code.  Others are bugs (such as, in one case, a
failure to close a {\tt /* */} style comment) which were apparently not
discovered because testing did not build the package under all possible
circumstances.

Our tool also warns about a number of stylistic mistakes, such as
unexpected indentation of forms (or lack of indentation where it is
expected).  These warnings are fairly frequent in machine-generated files
(such as lex and yacc output), but often indicate problems (in readability
if not in logic) elsewhere.


\subsection{Multiple definitions}
\label{sec:mult-def}

Because a package may contain multiple definitions of a macro, and a macro
can even be redefined partway through preprocessing, it is difficult to
determine exactly which definition of a macro will be used at a particular
expansion site.

The average number of definitions is less than 2; 79\% of macros are
defined just once, and only 5\% of macros have four or more definitions.
The more complicated macros to handle\,---\,such as types, syntactic
macros, and those that could not be classified\,---\,are most frequently
redefined In all but five packages, at least 94\% of all macros are defined
three or fewer times; a few have have unusual redefinition patterns,
largely because of portability or dependence on operating system
characteristics.


\begin{figure}
\centerline{\epsfig{file=fig/cat-def-frequency.eps,height=7.2in}}
\caption{Number of definitions per Cpp identifier, graphed as
  the percentage of identifiers that are defined a given number of times
  or fewer.  Overall, 95\% of macros were defined three or
  fewer times; the other 5\% of macros had four or more distinct
  definitions ({\tt \#define} directives).}
\label{fig:freq-def-cat}
\end{figure}

Figure~\ref{fig:freq-def-cat} graphs the number of definitions for each
macro name in our test suite, broken down by macro category.  When all
definitions of a name fall into the same category, the name is assigned to
that category; otherwise, the definitions are inconsistent and the name is
assigned to ``failed classification.''  See section~\ref{sec:inconsistent}
for more details of macro name classification.

Our analysis makes no distinction between sequential redefinitions of a
macro and multiple definitions that cannot take effect in a single
configuration.  Independent definitions may result from definitions in in
different branches of a Cpp conditional, from intervening {\#undef}
directives, from compilation conventions as when compiling different
programs in a package or versions of a program.  In general, distinguishing
the cases is undecidable.

The most frequently redefined macros in figure~\ref{fig:freq-def-cat} are
unclassified, non C code, and syntactic.  The more definitions a macro has,
the more likely it is that one of those definitions cannot be classified,
or is misclassified, by our system, resulting in a failure to classify the
macro name.

Macro bodies which are not C code tends to be Makefile compilation
commands, and library filenames.  These definitions differ for each
operating system, and often a package supports many such systems,
necessitating many macro definitions.  

Syntactic macros include those expanding only to punctuation.  These are
frequently used to support variant declaration styles; as such, they
require a definition for each variety, and they are frequently redefined to
ensure that their settings are correct.

The least frequently redefined macros are those classified as unknown
symbol.  If any definition is not so classified, then neither is the
macro name, and we included enough library header files to include some
definition of most common macros.

Half of all packages have no macros defined more than 14 times, and the
overall redefinition behavior of most packages approximates the mean line
of figure~\ref{fig:freq-def-cat}.  Notable exceptions are \pkg{bc},
\pkg{remind}, and \pkg{gcc}.  \pkg{bc} is very sparing with multiple
definitions: every macro is defined either one or two times.  By contrast, 
\pkg{remind} defines 10\% of its macros more than 10 times (but none more
than 15).  It supports ten different human languages (and various character
sets) by using macros for all user output strings.
The tail of \pkg{gcc}'s graph is longest of all:  over 4\% of macros are
defined more than 20 times, and 0\.5% are defined at least 50 times.
Many of these most heavily-defined macros (including the most
frequently-defined, \verb|CPP_PREDEFINES|, with  181 definitions and 198
undefinitions), are not C code.



        

\subsection{Multiple distinct definitions}

Section~\ref{sec:mult-def} counted the number of definitions of a given
macro name, providing an upper bound of sorts on the difficulty of
understanding uses the macro.  Multiple definitions are less worrisome if
their bodies are similar, and present few worries if identical, though it
is possible that a macro, free variable, or other quantity used in the
definition has changed its value between the locations.

\begin{figure}
  \centerline{\epsfig{file=fig/cat-ddf-frequency.eps,height=7.2in}}
  \caption{Number of syntactically distinct definitions per Cpp identifier,
    laid out as figure~\ref{fig:freq-def-cat}.}
  \label{fig:freq-ddf-cat}
\end{figure}
        
Figure~\ref{fig:freq-ddf-cat} repeats figure~\ref{fig:freq-def-cat}, but
reporting the number of syntactically distinct definitions rather than the
total number.  The comparison is less strict than that used by CPP when
determining whether to issue a warning about redefinition.  We eliminate
all comments and whitespace, canonically rename all formal arguments, and
consider all character and string literals identical.  Thus, it is a lower
bound on the number of definitions with distinct abstract syntax trees,
much as the previous chart was an upper bound.

The number of redefinitions reported in this chart is dramatically lower
than that of the previous one.  Syntactic macros are an outlier on the
other side of the mean (most of the multiple definitions are one of just a
few alternatives).  Most macros in \pkg{remind} are identified as
syntactically identical\,---\,usually, only string contents differed.  This
chart revives hope that even multiply-defined macros may be easy to
understand.


\subsection{Inconsistent definitions}
\label{sec:inconsistent}

This section further refines our analysis of multiply-defined macros by
considering, rather than syntactic structure, the categorization of the
macro bodies described in section~\ref{sec:categorization-details}.  This
analysis identifies higher-level commonalities among the macro definitions
that an analysis may be able to take advantage of in different ways than
more exact syntactic similarity.  We focus on definitions of a particular
name which are given different, incompatible categorizations.

In 93\% of cases, multiple definitions of a macro are compatible (more
often than not, identical).  Incompatibilities usually indicate bugs or
inconsistent usage, or failures in our categorization technique.  We
identified a number of places that code should be changed for robustness
and to avoid problems related to potentially incomplete constructs.


The following rules determine how two categories are merged:
\begin{itemize}\itemsep 0pt \parskip 0pt

\item If the categories are the same, use that.

\item If one is an unknown symbol (or the name of an undefined macro), use
  the other on the theory that the unseen definitions are likely to be
  similar to the present one, which is true for well-behaved macros.

\item If one is a null define, use the other.  For instance, a type
  modifier may be present or absent.  Macros not uncommonly expand to
  either a statement or to nothing, in order to either perform an action or
  do nothing.  (Actually, this is the wrong way to accomplish that: the
  no-op definitions should contain null statements in order to avoid
  parsing difficulties with {\tt else} clauses and elsewhere, see
  page~\pageref{item:swallow-semicolon}.  It would be appropriate to issue
  a warning message in this situation.)  Additionally, macros used as
  boolean variables which are checked for definedness may be set via a null
  define or by being assigned a constant (generally 1).  This practice is
  an error if the macro is used outside Cpp's {\tt defined} operator, but
  is also frequent and generally innocuous.

\item If one is a constant and the other is an expression, use the latter.

\item If one is an ambiguous list of space-separated symbols and the other
  is reserved word or type, use the latter.  Sequences of symbols can often
  not be definitely identified in isolation, but the other definitions of
  the same name indicate the intended usage.

\item If one is an expression or constant and the other is a semicolonless
  statement, use the latter, for a semicolon can be added to any expression
  to make a statement.  In particular, function calls are classified as
  expressions but may be intended to be used for side effect rather than
  for value.

\item If one is statement-related, use the other if it is the corresponding
  plural form; if both are partial statements, select the more incomplete
  statement category.  (See page~\pageref{item:statement-category} for
  details.)

\item Otherwise, return failure.
\end{itemize}

A macro name is categorized by merging its definitions pairwise.  The
legend of figure~\ref{fig:freq-def-cat} and several subsequent figures
reports the category breakdown by macro name.  It differs from the
by-definition breakdown of figure~\ref{fig:categorization} in several ways.
The number of null definitions is lower, as null definitions are often
found in conjunction with other types of definition rather than being the
only variety of definition for a particular symbol (which happens for
macros used only in Cpp conditionals, and only check for definedness
there).  The number of statements dropped, largely due to participation of
statements in incompatibly-defined macros which; likewise for non C code,
which turned into failures due to heuristic misclassifications.  The number
of unknown symbols rose because such macros tend to have very few
definitions\,---\,there are almost as many macro names as macro definitions
classified as unknown symbol\,---\,so are more prominent in a breakdown by
name than by definition.  The number of failures increased because it
includes any macro with a failing definition as well as any with
incompatible definitions.


\begin{figure}
  {\small\centerline{\input{tbl-subset-categories.tex}}}
  
  \caption{Macro name categorization breakdown.  For each macro with more
    than one definition, the categorization of its definitions are
    indicated.  For instance, for 23\% of multiply-defined macro names, all
    definitions fall into the expression category, and for 7.1\% of macro
    names, all definitions are either expressions or constants.  Rows less
    than one twentieth of one percent, representing fewer than ten macro
    names, are omitted.  The arrows indicate macros for which the names
    would be classified as ``failure'' (7.1\% of all multiply-defined macro
    names; overall, 2.4\% of all macro names are so classified).}

  \label{fig:subset-categories}
\end{figure}

Figure~\ref{fig:subset-categories} gives more detailed information for the
21\% of macro names which have multiple definitions.  Macros are grouped by
whether all of their definitions fall into the same subset of the
categories, as well as by whether the macro itself is classified as a
failure.

The arrows in the chart indicate which macro names have failing
classifications.  Using 10 rather than 28 categories makes this table
manageable, but does hide some information.  For instance, 
there are two ``expression + statement'' groups each making up
1.1\% of multiply defined macros.  The slightly more prevalent one includes
expressions and semicolonless statements, and those macro names are
classified as semicolonless statements.  The second group has definitions
of both these types, plus some complete sentences.  (There is similar
duplication, along with null define, at 0.26\% and 0.10\%.)

Likewise, the ``statement'' row at 0.35\% is a failure because it includes
both semicolonless and full statements, which are not interchangeable.  An
example is
\begin{verbatim}
    #define TARGET_VERSION fprintf (stderr, " (i860, BSD)")
    #define TARGET_VERSION fprintf (stderr, " (i860 OSF/1AD)");
\end{verbatim}
where the former definition is a bug (most, but not all, other definitions
of the macro are full statements).




\section{Uses}

\subsection{high usage}

        Give average uses per defined macro name (see new summary table).

        In the chart, a higher line indicates less use.

        Half of all macros are used two or fewer times (!).  12\% not used
          at all (!).

        Syntactic macros tend to be used far more often; also those
          expanding to a type.  These make sense, as they are
          frequently-used constructs 
          that permeate code (appear at every definition, for instance).

        Non-C-code, null defines (i.e., every definition of the name is a
          null define), and constants are used least.  The latter is a bit
          surprising, but shows that macros are being used as a
          configuration mechanism rather than a linguistic mechanism.  (Do
          I buy that?  Maybe (probably!) not. ??)

        All packages follow approximately the same curve.  The outlier is
          \pkg{gnuplot}, which uses macros less, on average, than other packages
          do.  Over 40\% of the macros it defines are never used at all --
          say why.  (Largely due to unsupported terminals like tgif; that
          appears elsewhere in this document, so reference it.)

          Another reason for unused macros might be uses in Makefiles and
          other non-C-code files that we don't examine.

% \section{Macro use}

% \begin{figure}
% % {\small
% %   \setlength{\tabcolsep}{.25em}
% %   \centerline{\input{freq_of_use-tbl.tex}}%
% % }
% \centerline{\epsfig{file=fig/use-frequency.eps,height=7.5in}}
% \caption{Number of expansions per Cpp macro.  The numbers in the
%   table represent the percentage of identifiers which are expanded a given
%   number of times or fewer.  For example, \pkg{g77} expands 65\% of its
%   macros two or fewer times.}
% \label{fig:freq-use}
% \end{figure}

\begin{figure}
\centerline{\epsfig{file=fig/cat-use-frequency.eps,height=7.5in}}
\caption{Number of expansions per Cpp macro.  The numbers in the
  table represent the percentage of identifiers which are expanded a given
  number of times or fewer.  For example, 50\% of all
  macros are expanded two or fewer times.}
\label{fig:freq-use-cat}
\end{figure}

Figure~\ref{fig:freq-use-cat} is structured as the previous figure, but it
represents the number of times that a defined name is expanded in 
the package.  About [[82\%]] of all macros were
expanded eight or fewer times.

Most packages contain a significant number of defined
macros that are never expanded\,---\,on average, 12\%.

Macros with 10 or fewer uses
cover approximately [[85\%]] of the cases.

[[Double-check all these numbers!]]
The tail of this distribution is quite long, indicating that some macros
are used very heavily.  Ninety-nine percent of macros are expanded 147 or fewer
times, 99.5\% of macros are expanded 273 or fewer times, 99.9\% are
expanded 882 or fewer times, and \pkg{python} uses {\tt NULL} (which \pkg{python}
itself defines) 4233 times.  Figure~\ref{fig:freq-use-cat} weights each macro
equally rather than weighting each macro use equally, which would weight
\pkg{python}'s {\tt NULL} 4233 times more heavily than a macro used only once
(and infinitely more than a macro never used at all).

\begin{figure}
  {\small\centerline{\input{tbl-summarize-frequency.tex}}}
  
  \caption{Summary of
    figures~\ref{fig:freq-def-cat},~\ref{fig:freq-ddf-cat},
    and~\ref{fig:freq-use-cat}.  The table is by macro name.  Add percentages?
    [[The 1.0 distinct defs for unknown symbol is surprising; it indicates
    that ti was the *same* unknown symbol in each definition (doesn't
    it?).  But a name is only in the category if none of its defs was
    recognized.  Double-check this!}
  \label{fig:freq-sum-cat}
\end{figure}

(Not sure what to say about figure~\ref{fig:freq-sum-cat}.)

\subsection{number of arguments}

\begin{figure}
\centerline{\epsfig{file=fig/cat-numargs.eps,height=7.5in}}
\caption{Is this worth including?}
\label{fig:cat-numargs}
\end{figure}

You might wonder whether macros are used like functions (taking arguments)
or like constants (taking no arguments).  We graphed that in
figure~\ref{fig:cat-numargs}.  This seems irrelevant to me; I don't see
where to fit it in, or what to say about it.

\subsection{inconsistent usage}

[[Does this general discussion belong earlier?]]

Macros have two general purposes: they can control the inclusion of lines
of code (by appearing in a {\tt \#if} condition that controls that line) or
can change the text of a line (by being expanded on that line).  Each of
these uses can often be modeled by C++ language features -- conditionals or
(for certain types of substitution) {\tt const}s and {\tt inline}s.  But when a macro
is used in both ways, then there's no one C++ language feature.
(Additionally, it's harder to understand and to analyze a macro that's used
in both ways than one that is only used in one way or the other.)
[[Or maybe more is going on, or conceptually different uses have been
merged into a single name.]]

    We split macro uses into three categories:  
\begin{itemize}
\item
  uses in C code, where the macro's expansion controls textual
            replacement
\item
  uses in {\tt \#if}, {\tt \#ifdef}, {\tt \#ifndef}, {\tt \#elif} conditions
\item
  uses in a macro body, which eventually bottom out to one of the
            others (unless that macro is never used...)
\end{itemize}

\begin{figure}
{\small
  \setlength{\tabcolsep}{.25em}
  \input{tbl-where-used.tex}%
}
\caption{Where macros are used: in C code, in macro definition bodies, in
  conditional tests, or in some combination thereof.  The numbers don't sum to
  100\% because of rounding.  It might perhaps be interesting to see this
  broken down by category.}
\label{fig:where-used}
\end{figure}

      We discarded uses in CPP conditionals whose only purpose was to
        prevent redefinition.  More specifically, if the condition tested
        only definedness, the next line defined the macro just tested, and
        the line after that ended the CPP conditional.  This is a bit
        overrestrictive, but conservative, and in practice quite accurate.
        
        Uses in a macro body generally are intended to be used however the
        containing macro is.  Since those uses also appear in
        figure~\ref{fig:where-used}, we did not attempt to track each such
        use in a body to some macro's appearance in either code or a
        conditional.  
        A definition isn't a use of the macro being defined, only of those
        in the body.  Since those are uses, 12\% is a lower bound on those
        that never affect the code.
        It would be reasonable to assign the 5.4\% that are macro only, to
        the other categories on a pro rata basis.
        Thus, the interesting categories are "code",
        "conditional", "code and conditional", and "no use".

      Macros are used more frequently to expand code than to control its
        inclusion, by a factor of ten to one.  (However, each inclusion use
        can control many lines of code; see the dependence section, below.)
The dominant usage was in C code only; these uses
do not, therefore, have any affect on conditional compilation (for example).

[[Move this to the previous section, and reference back to it.]]
      A surprising number -- nearly 12\% -- of macros defined in a package
        are never used at all.  Occasionally [[find a concrete example of
        this]] this is a result of shipping a 
        standard set of headers with the package -- it's like a library for
        that development team, but one that can't be counted upon to exist
        everywhere, so it has to be provided.  For gnuplot, over 40\% of
        macros are never used because the package's support for several
        terminal types, such as tgif, is unfinished (and thus unused).
        Even discounting that package, though, the numbers are remarkably
        high.  We would be surprised if one in eight functions and
        variables in a package were never used, not even in testing code.
        [[Do we have any idea what fraction this is in practice?]]
        (The percentage of macros defined in libraries/standard header
        files which are never used in the code is enormous, but that is
        expected.)

      The potentially problematic uses are those for which a macro appears
        both in code and in a conditional; these comprise only 3.4\% of all
        macro names.

      Across packages, there is heavy variation.  Packages which use
        the preprocessor sparingly are as likely to have a high percentage
        of mixed usage as packages which make heavy use of CPP.  (There is
        a very slight tendency for the less aggressive packages (i.e.,
        those lower on the lists in figures~\ref{fig:directives-breakdown}
        and~\ref{fig:categorization}) to have more uses in code, fewer uses
        in conditionals, and fewer macros that are never used.)

In general, packages use macros either to direct conditional compilation or
to produce code, but not for both purposes; this separation of concerns
makes the source code easier to understand.  Only 3.4\% of macros expand in
both code and conditional contexts (plus a similar percentage of those used
in other macros). 
Conditional usage is rare in general; conditional compilation accounts for
half of Cpp directives but only 7.1\% of macro usage (plus the categories just
listed above).  But each use in a conditional can have a large effect on
the system.


\subsection{Mixed usage in conditional control}

\begin{figure}
\centerline{\epsfig{file=fig/ccd-categories.eps,height=7.5in}}
\caption{CCD categories.  [[Reorder according to order in text.  Perhaps
  change legend to 2 or 3 columns, underneath.]]}
\label{fig:ccd-categories}
\end{figure}


    Conditionals are used to check for a bunch of things (examples); but a
      single conditional generally checks just one of these categories.  We
      counted the number in each, and also the number that checked symbols
      falling in multiple categories.

    Few mixed categories.

    Lots of variation overall.

    The macros with mixed usage (especially here, but also in the previous
      section) are akin to Krone \& Snelting's anomalous macros that
      interfere in the lattice.  But we found more than they seemed to!
      [[Did we analyze the package in which they found their problem?]]
      
      Each symbol is assigned to a category; if all symbols in a
      conditional guard are in the same category, then the conditional test
      is assigned to that category; otherwise, the conditional text is
      given category ``mixed.''  The categorization is based on the macro
      {\em name} alone; but this works because of naming conventions for
      various types of macro use.  (We examined a bunch of macro names
      (compiled from a list of all macro names appearing in conditionals)
      to refine our heuristics.)


{\input{expl-ccd-cat.tex}}


\section{Dependences}
\label{sec:dependence}
\label{sec:last-content-section}

 Two types of dependence:  control/inclusion, and substitution/expansion
     This includes macros which control its inclusion via conditional
     compilation, macros that are invoked on the line, and macros that
     control the definitions of, or are called by, directly invoked macros

 Explain this.  Explain how we compute it transitively.
 
 The dependence charts omit Emacs and Mosaic; the full dependence
 information for each overran our computer's virtual memory.  This was in
 part due to dependences in libraries; the Motif library is a particular
 problem.  (We did compute dependence information for Plan, which is the
 third of our 30 packages which uses the Motif library.)  So these numbers
 are smaller than they might be with more complete information.

Note that we are reporting on may dependences, not only must dependences.
Mention some dependence types from notes ``must and may dependences'' section.

\subsection{line dependent on many macros}

\begin{figure}
\centerline{\epsfig{file=fig/dep-byline.eps,height=7.5in}}
\caption{dep-byline for 28 packages.
The log scale is shifted by unity in order to place 0 on the scale; that
is, we added 1 to each of the x axis values, before plotting, then
relabeled ``1'' as ``0''.
[[Make this figure smaller; doesn't deserve a full page.]]}
\label{fig:dep-byline}
\end{figure}


    [Gloss over what kind of line; I think it's a physical line, but each
      physical line is coalesced with other parts of its logical line for
      the purpose of computing dependence info.]

    We computed, for each line, the number of macros it depends on.

There are two types of dependence:  control and expansion.  Make sure we
have good names for these!

    Describe the matrix:  ExE=E, Cx?=C, ?xC=C; plus, we sum across
      conditional compilations and alternate definitions.

    
      58\% of lines have no dependence on macros at all.  (Most lines that
      don't expand any macros and appear in C files fall into this category.)

    Overall, 28\% of lines expand a macro -- that's quite a few.

    5\% of all lines depend on at least 134 macros.

    On average, each line is expansion-dependent on .040 macros,
      inclusion-dependent on .31 macros, and any-dependent on .34 macros.
      (These numbers don't add up because a line may be both expansion and
      inclusion dependent on a macro, but that macro is only counted once
      in the "any dependence" number.)
      
      One line of gcc is expansion-dependent on the expansions of 187
      macros.  (This is an outlier: of the 297,760 [[huh?? this doesn't
      jibe with figure~\ref{fig:packages}!!  double-check]] lines in gcc,
      only 13 are expansion-dependent on more than 100 macros -- all of
      which fall in the range of 159 to 187 such dependences.)

        It's this one, whose mere appearance in the final source is
        dependent on 182 macros (not such an outlier: over 10,000 lines are
        inclusion-dependent on more than that many macros):

        gcc-2.7.2.1/explow.c:430: dependences incl=182; exp=187; either=356
              \verb|LEGITIMIZE_ADDRESS| (x, oldx, mode, win);

        \verb|LEGITIMIZE_ADDRESS| is defined 30 times in gcc, and one randomly-selected
        definition was 37 lines long (and chock full of other macro invocations).
        [[be more specific, or say ``many of them dozens of lines long'']]

\subsection{macro controlling many lines}

    We combined these charts for the different packages, because the shapes
      of each were quite similar for each package.  In order to permit
      combining charts across quite different package sizes, we computed
      values that were percentages of package size.

   \subsubsection{expansion}

\begin{figure}
% This works, but the figures are upside-down.
% \centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=90,height=3.75in}}
% \centerline{\epsfig{file=fig/incl-dep-bymacro.eps,angle=90,height=3.75in}}
% Can't use ``height'' when rotating by -90 or +270; I don't know why.
\centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=270,width=.9\linewidth}}
\bigskip
\centerline{\epsfig{file=fig/incl-dep-bymacro.eps,angle=270,width=.9\linewidth}}
\caption{dep-bymacro for 28 packages.
  Each bar represents all macros that control less than the bar label
  percent of the lines in its package (but more than the previous label).
  For instance, the .17 bar in the red chart indicates that there are 286
  macros that each control between .11\% and .17\% of the entire package
  containing that macro.  The maximum falls in the last bucket specified
  (i.e., the first bucket off the chart is the first empty one).}
\label{fig:dep-bymacro}
\end{figure}

Practically every {\tt \#if} line (which accounts for about half of the
conditional compilation lines) expands a macro.  (Some don't, for instance
when testing which character set is being used on the compiling machine.)

        This exponential decay indicates that, as expected, there are more
          macros which control just a few lines and fewer macros that
          control a lot of lines.

        Outliers (> 6\% of all lines expand):
          int in gcc (14.85\% !)
          NULL, ANY, object in python

        Above 5\%: 
          SvANY in Perl
          const in RCS
          ip in workman -- bogus, as defined just twice, then undefined;
                  most places it is a formal parameter and out of the scope
                  of the macro definition.
          ArgCount, Args in xfig (like argc, argv)
          rtx in gcc (defined to int or int*)

        Overall, for these top 10:  6 types, 1 constant, 3 expressions

   \subsubsection{inclusion}

        For each package, the graph is bimodal (so much so that this even
          shows up a bit on the combined chart; it's more marked in the
          individual packages, I think).  Most macros control inclusion of
          no, or very few, lines; but quite a few control a substantial
          fraction of the package (around 10\%).

        All the heaviest dependences are on header files (for instance,
          \verb|H_PERL| controls inclusion of over 53\% of Perl's lines).

        [It would have been interesting to run these numbers for everything
          but include file multiple inclusion prevention macros.]


\subsection{CPPP experiment}

    [After the above, this experiment doesn't sound like such a smart thing
      to try any more; its results certainly aren't unexpected.]

    We had noticed in our programming that a particularly heavy use of the
      preprocessor is to handle multiple dialects of a language.  These uses
      tend to be less structured:  they don't have a simple pattern.  And
      this work must be done in the preprocessor:  there's no hope of
      integrating it into the language.  So we performed an experiment to
      see whether standardizing on a single language would reduce
      dependences, failed classifications, etc.

    We built a CPPP partial evaluator (called cppp) which, given a set of
      macros known to be defined or undefined (and, optionally, their
      expansions), eliminates all possible CPP conditionals.  We defined
      all the macros that can be depended on if using ANSI standard C or
      C++ (including prototypes and booleans) with POSIX-compliant
      libraries, preprocessed all the source (and 
      all library header files), and reran our experiments.

    The results were disappointing:  while some numeric measures of how
      complicated the resulting program's dependences are decreased, most
      remained at about their previous level.  The number of multiple
      definitions of macros, and the number of failed classifications, did
      not decrease as much as anticipated.

    From this we can conclude that there is no one obvious single point of
      attack:  even eliminating what seems most prevalent to us doesn't
      make a sufficient difference.



\section{Related work}
\label{sec:related}


We could find no other empirical study of the use of the C preprocessor nor
any other macro processor.  However, we did find guidance on using C macros
effectively and tools for checking macro usage.

Carroll and Ellis state that ``almost all uses of macros can be eliminated
from C++ libraries''~[p.~146]{Carroll95}.  They list eight categories
of macro usage and explain how to convert them into C++ mechanisms.  They
do not discuss automatic conversion, but focus on instructing the software
engineer on better ways to do Cpp-like things.

Similarly, a number of organizations provide hints about effective ways to
the use the C preprocessor.  The GNU documentation discusses
a set of techniques including simple macros, argument macros, predefined
macros, stringization macros, concatenation macros, and undefining and
redefining macros.  It also identifies a set of ``pitfalls and subtleties
of macros''; these are much like some of the problems our analysis tool
identifies.

We discovered that these categorizations sometimes focussed on constructs
that don't happen very often or missed ones that are actually frequent.
Our effort not only categorizes problems, but it also determines the
frequency of appearance of those problems and discovers other idiosyncratic
uses.

The Lint program will warn about empty statements, constants in conditional
context, and a few other things.  But it's pretty inadequate.  This mostly
has to do with problems after expansion.

A number of tools check whether specific C programs satisfy particular
constraints.  The lint program checker [[reference]]
checks for potentially problematic uses of C\@.  
[[This is irrelevant, unless you're going to claim that it does a worse job
than one would like.]]
The implementation of lint
is complicated by the fact that it tries to replicate significant functions
of both the C compiler and the preprocessor.


Greg:  describe LCLint methodology and results.  Say exactly what you did
(i.e., how hard you tried), and what the results were.  Also, that you tried
on only 20, not all 30, packages, which doesn't include the biggest ones.


LCLint performs many of lint's checks and also
allows the programmer to add annotations which enable additional
checks~\cite{Evans-pldi96,Evans-fse94}.
LCLint optionally checks function-like
macros\,---\,that is, those which take arguments\,---\,for
macro arguments on the left hand side of assignments, for statements
playing the role of expressions, and for consistent return types.
LCLint's approach is prescriptive: programmers are encouraged not to use
constructs that might be dangerous, or to change code that contains such
constructs.  We are more interested in analyzing, describing, and
automatically removing such uses so that tools can better process existing
code without requiring human interaction or producing misleading results.

LCLint
considers assignment to a macro argument dangerous but does not appear
to check for assignments to local variables.~\cite[\S 8]{Evans:LCLint}
[[Should we mention these things?  I don't want to seem nitpicky or petty, so
if we mention this, mention them as differences, not as things LCLint does
wrong.
\begin{quote}
$\ldots$ a parameter to a macro may not be used as the left hand side
of an assignment expression $\ldots$, a macro definition must be
syntactically equivalent to a statement when it is invoked followed by
%a semicolon $\ldots$, the type of the macro body must match the return
type of the corresponding function $\ldots$~\cite[\S 8]{Evans:LCLint}
\end{quote}
That quotation is really easy to nitpick:
\begin{enumerate}
 \item assignment parameters is fine (just turn into a reference argument) but
    assignment of non-parameters that aren't at global scope is quite bad.
 \item in x=foo(); we do NOT want foo() to be a statement
 \item a macro doesn't have a single type, but may have many polymorphic types
\end{enumerate}
]]


%% FIX: If we could also list the platforms for which each can compile,
%% that would be great, but I doubt the benefit is worth the effort for now

Krone and Snelting use mathematical concept analysis to determine the
conditional compilation structure of code~\cite{Krone94}.  They determine,
for each line, which preprocessor macros it depends upon, and display that
information in a lattice.  They do not determine how macros depend upon one
another directly, only by their nesting in {\tt \#if}, and the information
conveyed is about the program as a whole.


[[Expand; relocate in this section?]]
A limited number of tools do exist to assist software engineers to
understand code with containing Cpp directives, such as debuggers that can
call {\tt \#define}d functions and editors that support viewing one
particular configuration of the code.




\section{Conclusions}
\label{sec:conclusion}

[[Grist for conclusion: These data demonstrate that multiple definitions of
symbols is not numerically frequent; even more importantly, the definitions
of a symbol tend to be compatible, as shown in
section~\ref{sec:inconsistent}.]]


% \subsection{Who cares?}
\subsection{Relevance of the results}

[[This is the place to recap the various sections, giving the highlights of
each.]]

The results of this research are of interest to language designers, tool
writers, programmers, and software engineers.

Language designers can examine uses of the macro system's extra-linguistic
capabilities to determine what programmers consider missing from the
language.  Future language specifications can support (or prevent!)\ such
practices in a more disciplined, structured way.

% [[[Think more about this:  Also, how do language choices lead to more/less
% tightly integrated (as opposed to open, component-based) environments?
% E.G., no need for \verb|__LINE__| in Java?]]]

Programming tool writers, too, need to understand how Cpp is used, for that
sheds insight on the sorts of inputs that will be provided to the tool.  By
coping with the most common constructs, the tool can provide relatively
good coverage for low effort.  By identifying problematic uses, much better
feedback can be given to the programmer, who can be more effective as a
result.  The analysis results also indicate the difficulty of processing [wc]
preprocessor directives; before these analyses, we did not know whether the
task was so trivial as to be uninteresting, so difficult as to be not worth
attempting, or somewhere in between.

The analyses are of interest to programmers who wish to make their code
cleaner and more portable, and can help them to avoid constructs that cause
tools (such as test frameworks and program understanding tools)
to give incomplete or incorrect results.

% Also, learn weird new Cpp tricks!

Finally, our results are of interest to software engineers for all of the
above reasons and more.  Since this is the first Cpp usage study of which
we are aware, it is worth performing simply to determine whether the
results were predictable a priori; we did in fact discover a number of
interesting features of our suite of programs.


\subsection{Making C programs easier to understand}

The combination of C and Cpp makes a source text unnecessarily difficult to
understand.  A good first step is to eliminate Cpp uses where an equivalent
C or C++ construct exists, and to apply tools to explicate the remaining
uses.  Here we discuss a few approaches to solving this problem by
eliminating the source of confusion rather than applying tools.  We do not
seriously consider simply eliminating the preprocessor, for it provides
conveniences and functionality not present in the base language.

Since many of the most problematic uses of Cpp provide portability across
different language dialects or different operating environments,
standardization can obviate many such uses.  Canonicalizing library
function names and calling conventions makes conditional compilation less
necessary and incidentally makes all programs more portable, even those
which have not gone to special effort to achieve portability.  This
proposal moves the responsibility for portability (really, conformance to a
specification) from the application program into the library or operating
system, which is a reasonable design choice since many application programs
rely on a much smaller number of libraries and run on relatively few
operating systems.

Likewise, the most common single cause [[Really?  I'm not sure I believe
that without qualification]] for Cpp directives would be eliminated if the
C language and its dialects had only a single declaration syntax.  Because
most C compilers, and all C++ compilers, accept ANSI-style declarations,
much support for multiple declaration style may have outlived its
usefulness.  [[On the other hand, K\&R support was just *added* to some
mature, popular package (zsh?) recently.]]  We are investigating the effect
on our statistics (and program understandability) of ``partially
evaluating'' a program source by specifying the definedness and values of
some Cpp identifiers.

Some Cpp directives, such as {\tt \#include}, can be moved into the
language proper; this would also eliminate the need for Cpp constructs that
prevent multiple inclusion of header files.  Likewise, compilers that do a
good job of constant-folding and dead code elimination can encourage
programmers to use language constructs rather than relying on the
guarantees of an extra-linguistic tool like Cpp.\footnote{Interestingly,
  the issue seems to not be whether compilers do the appropriate
  optimizations, but whether programmers have confidence that the
  optimizations will be performed; if unsure, programmers will continue to
  resort to Cpp, since certainly a compiler cannot generate code for source
  that it does not ever even see (because Cpp has already stripped it
  away).}

Common Cpp constructs could be replaced by a special-purpose syntax.  For
instance, declarations or partial declarations could be made explicit
(perhaps first-class) objects; similar support could be provided for
repetitive constructs and dynamic scoping.  Manipulations of these objects
would then be performed through a clearly-specified interface rather than
via string and token concatenation, easing the understanding burden on the
programmer.  Such uses would also be visible to the compiler and could be
checked and reasonable error messages provided.  The downside of this
approach is the introduction of a new syntax or new library functions which
may not simplify the program text and which cannot cover all cases, only a
few specified ones.

[[Add a hygienic macros reference somewhere.]]

An alternative approach which avoids the clumsiness of a separate language
of limited expressiveness is to make the macro language more
powerful\,---\,perhaps even using the language itself via constructs
evaluated at compile time rather than run time.  (The macro systems of
Common Lisp and Scheme, and their descendants~\cite{WeiseC93}, take this
approach.)  An extreme example would be to provide a full-fledged
reflection capability.  Such an approach is highly general, powerful, and
theoretically clean; it circumvents many of the limitations of Cpp,
though it does not necessarily support all of Cpp's low-level features.
However, this approach may degrade rather than improve programs
understanding.  As difficult as it may be to determine what output a
macroless program produces, it can be just as difficult simply to determine
the text of a program which uses such macros.  (This is also a problem with
meta-object protocols, aspect-oriented programming, and intentional
programming, all of which permit the programmer to specify transformations
on other parts of the source code.  In practice, such systems are used in
fairly restricted ways, perhaps because other uses would be too
complicated.)  A dialog among users, compiler writers, tool writers, and
language theorists is necessary when introducing a feature in order to
prevent unforeseen consequences from turning it into a burden.


%% See slide 15 and its notes from Mike's quals talk.
\subsection{Future work}
 
These results suggest a wide variety of future avenues for research, both
in terms of expanding our understanding of uses of the preprocessor in
practice and in addressing the issues identified by this study.

A straightforward refinement that we are pursuing examines macro uses to
aid this categorization.  (For example, a macro used where a type should
appear can be inferred to expand to a type; a macro used before a function
body is probably expanding to a declarator.)

Further analysis of the conditional
compilation structure (in the style of Krone and Snelting~\cite{Krone94})
and of the macros with free variables (essentially achieving dynamic
scoping) is needed to see which of the roughly 29\% of expression macros
should be easy to convert to C++ language features such as constants or
enumerated values.

  We are concerned with how it {\em could} be used, not just how it {\em
  happens} to be used.  That's why we can't take the easy out.  But it
  would be interesting to look at uses, which would let us infer the {\em
  intended} use (if not all {\em possible} uses).  We are actively pursing
  this line of research.


No libraries (e.g., \pkg{glibc}), as they're too different than applications.
No C++.\footnote{Preliminary results indicate that many
  C++ packages rely heavily on Cpp, even when C++ supports a nearly
  identical language construct, probably due to a combination of trivial
  translations from C to C++ and of C programmers becoming C++ programmers
  without changing their habits.}

``As with all benchmarks, there is the question of how representative this is.''

Variations among packages:
 * The C language has changed.  How do newer packages compare with older ones?
 * How do PC packages compare with Unix packages?  Less pressure to make it
   work on many platforms, because there aren't as many?
 * Correlate human perceptions of good or bad preprocessor use with what my
   tool says.
 * What's special about GNU programs?  Compare GNU to non-GNU?
 * Applications vs. libraries?  Get more libraries, or eliminate the current
   ones.


  [[We didn't analyze whether the free variables are globals or locals,
          which could substantially impact understanding/translation.]]

        We didn't examine every use.  Besides, the point isn't just the
          current uses, but also future uses:  we want these to be easy for
          programmers to use.
          
Reaching definitions for macros (``flow-sensitive'' due to
dependences on other macros).

% Not really right:  Don't want the ``References'' section head to be small.
{\small \bibliography{evil}}

\end{document}
