% $Id$
\documentclass[10pt]{article}
\usepackage{epsfig}
\usepackage{dcolumn}
\usepackage{alltt}

\def\numpackages{30}
\def\numpackageslesstwo{28}
\def\numlines{2 million}
\def\numlinesexact{nearly 2 million}
\def\numlinesncnbexact{about 1.4 million}

\newcommand{\pkg}[1]{\textsf{#1}}

\newcommand{\file}[1]{\texttt{#1}}
\newcommand{\vs}{vs}

% the "fullpage" package does almost the same thing
% as the below lines-- it doesn't make things quite as
% tall or wide, but is generally what I use
% \usepackage{fullpage}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt
\topmargin   0pt
\headsep 0pt
\headheight 0pt
\textwidth   6.5 in
\textheight  9 in

%% Figures
\newcommand{\captionsmall}[1]{\caption[]{\small #1}}
% Changing this to .9 shoves lots of figures to the end of the document; why?
\renewcommand{\floatpagefraction}{.8} %default .5
% \renewcommand{\floatpagefraction}{.9} %default .5
% Avoid putting all figures at end of text.
\renewcommand{\textfraction}{.1}  % .2 is the default
\renewcommand{\topfraction}{.9}   % .7 is the default
%% Get a line between figures at the top of the page and the text
 \makeatletter
 \def\topfigrule{\kern3\p@ \hrule \kern -3.4\p@} % the \hrule is .4pt high
 \def\botfigrule{\kern-3\p@ \hrule \kern 2.6\p@} % the \hrule is .4pt high
 \makeatother


%%% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \bibliographystyle{plain}
\bibliographystyle{alpha}

\title{An Empirical Analysis of C Preprocessor Use}

\author{Michael D. Ernst \and Greg J. Badros \and David Notkin}

\date{% Technical Report UW-CSE-97-04-06 \\
Department of Computer Science and Engineering \\
University of Washington \\
Box 352350, Seattle, WA  98195-2350  USA \\
{\small \{{\tt mernst},{\tt gjb},{\tt notkin}\}{\tt @cs.washington.edu}} \\
20 November 1997}  

\maketitle

\begin{abstract}
  The C programming language is intimately connected to its macro
  preprocessor Cpp.  This relationship hinders tools built to engineer C
  programs, such as compilers, debuggers, call graph extractors, and
  translators.  Most tools make no attempt to analyze macro usage, but simply
  preprocess their input, which has a number of negative consequences.  In
  order to determine how the preprocessor is used in practice, and the
  feasibility of automatic analysis of preprocessor use, this paper
  analyzes {\numpackages} packages comprising {\numlines} lines of publicly
  available C code.  We determine the incidence of C preprocessor usage which
  is complex, potentially problematic, or inexpressible in in terms of
  other C or C++ language features.
[[We also came up with the list of things to investigate; and along the way
we taxonomized some stuff, which is also a contribution.]]

  We particularly note data that are
  material to the development of tools for C or C++, including translating
  from C to C++ to reduce preprocessor usage.  The results are of interest
  to language designers, tool writers, programmers, and software
  engineers.
[[ FIX: rewrite ]]
\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is incomplete without its macro
preprocessor, Cpp~\cite[Ch.~3]{Harbison91}. By supplying such
facilities as file inclusion, definition of constants and macros, and
conditional compilation, Cpp can be used to define new syntax,
abbreviate repetitive or complicated constructs, or eliminate reliance
on a compiler implementation to inline functions, propagate symbolic
constants, eliminate dead code, and short-circuit constant tests.  Cpp
also permits system dependences to be made explicit and tested,
resulting in a clearer separation of concerns.  In addition, Cpp
permits a single source to contain multiple different dialects of C; a
frequent use is to support both K\&R-style and ANSI-style
declarations.  Indeed, one cannot write practical C programs without
these facilities.

While disciplined use of the preprocessor can reduce programmer effort
and improve portability, performance, or readability, Cpp is widely
viewed as a source of difficulty for understanding and transforming C
programs.  Cpp's lack of structure\,---\,its inputs and
outputs are raw token streams\,---\,engenders flexibility and allows
arbitrary source code manipulations that complicate 
understanding of the program by both software engineers and tools.  In
the worst case, the preprocessor makes merely determining the program
text as difficult as determining the output of an ordinary program.
The designer of C++, which shares C's preprocessor, also noted these
problems: ``Occasionally, even the most extreme uses of Cpp are
useful, but its facilities are so unstructured and intrusive that they
are a constant problem to programmers, maintainers, people porting
code, and tool builders.''~\cite[p.~424]{Stroustrup-DesignEvolution}

Tools\,---\,and, to a lesser degree, software engineers\,---\,have
three options for coping with Cpp.  They may ignore preprocessor
directives altogether, accept only post-processed code (usually by
running Cpp on their input), or attempt to emulate the preprocessor.
Each approach has different strengths and weaknesses.

\begin{itemize}

\item Ignoring preprocessor directives is an option for tools that produce
approximate information, such as those based on lexical or approximate parsing
techniques.  However, if accurate information about function extents,
scope nesting, declared variables and functions, and other aspects of
a program are required, the preprocessor cannot be ignored.

\item Operating on post-processed code, the most common strategy, is
simple to implement, but then the tool's input differs from what the
programmer sees.  Even when line number mappings are maintained, other
information is lost in the mapping back to the original source code.
For instance, source-level debuggers have no symbolic names or types
for constants and functions introduced via {\tt \#define}, nor can
tools trace or set breakpoints in function macros, as they can for
ordinary functions (even those that have been
inlined~\cite{Zellweger83:TR}).  As another example, Siff and Reps
describe a technique that uses type inferencing to produce C++
function templates from C; however, the input is ``a C program
component that $\ldots$ has been preprocessed so that all include
files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
the readability and reusability of the resulting C++ templates.  As
yet another related example, call graph extractors generally work in
terms of the post-processed code, even when a human is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
leave the software engineer responsible for inferring the mapping
between the original and the post-processed source, which is an
undesirable and error-prone situation.

In addition, a tool that manipulates post-processed code cannot be run
on a non-syntactic program or one that will not preprocess on the
platform on which the tool is being run.  These constraints complicate
porting and maintenance, two of the situations in which program
understanding and transformation tools are most likely to be needed.
Additionally, a tool supplied with only one post-processed
instantiation of the source code cannot reason about the program as a
whole, only about that version that results from one particular set of
preprocessor variables.  For instance, a bug in one configuration may
not be discovered despite exhaustive testing of other configurations
that do not incorporate particular code or do not admit particular
execution paths.

\item The final option, emulating the preprocessor, is fraught with
difficulty.  Macro definitions consist of complete tokens but need not
be complete expressions or statements.  Conditional compilation and
alternative macro definitions lead to very different results from a
single original program text.  Preprocessing adds complexity to an
implementation, which must trade off performing preprocessing against
maintaining the code in close to its original form.  Extracting
structure from macro-obfuscated source is not a task for the
faint-hearted.  Despite these problems, in many situations only some
sort of preprocessing or Cpp analysis can produce useful answers.

\end{itemize}

While much has been written about Cpp's potential pitfalls, no
previous work has examined actual use of the C preprocessor to
determine whether it presents a practical or merely theoretical
obstacle to program understanding, analysis, and transformation.  This
paper fills that gap by examining Cpp use in {\numpackages} programs
comprising {\numlines} lines of source code.

The analysis focuses on potential pitfalls that complicate the work of
software engineers and tool builders:
\begin{description}
\item[high total use]  heavy use of either macro substitution or
  conditional compilation can overwhelm a human or tool; particularly
  problematic are lines that depend on many macros or macros that control
  many lines
\item[complicated bodies]  a macro body need not expand to a complete
  C syntactic entity (like a statement or expression)
\item[extra-linguistic features]  a macro body may exploit features of
  the preprocessor not available in C, such as stringization, token
  pasting, or use of free variables
\item[multiple definitions]  uncertainty about the expansion of a macro
  prevents knowledge of the actual program text; even more problematically,
  two definitions of a macro may be incompatible, for instance if one is a
  statement and the other expands to an expression or type
\item[macro pitfalls]  macros introduce new varieties or programming
  errors, such as function-like macros that fail to swallow a following
  semicolon and macros that fail to parenthesize, or side-effect, uses of
  formal variables
\item[inconsistent usage]  a macro used both for conditional
  compilation and to expand code is harder to understand than one used just
  for one purpose or the other
\item[mixed tests]  a single Cpp conditional may test conceptually
  distinct, unrelated conditions, making it difficult to perceive the
  intention
\item[variation in use]  if there is no clear pattern of use, or
  commonly-repeated paradigms, then no obvious point of attack presents
  itself
  %% No pattern according to package size, relative or absolute Cpp use, etc.
\end{description}
We report in detail on each of these aspects of preprocessor use,
indicating which appear to be innocuous in practice (that is, the
problematic uses appear only infrequently) and which may prove problematic
for software engineers.  We also present new taxonomies of macro body
expansions, macro feature usage, macro pitfalls, and conditional
intentions.  These taxonomies improve on previous work by being more
detailed and more accurately reflecting actual use.

Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of
C programming support tools.  In terms of the complexity of
preprocessor usage, the results reported here contain both good news
and bad.  By far the largest number of macro definitions and uses are
relatively simple, of the variety that a programmer could understand
without undue effort (although perhaps requiring tedious work) or that
a relatively unsophisticated tool could understand (although in
practice very few even try).  Despite the preponderance of innocuous
macros, the preprocessor is so heavily used that the remaining ones
are numerically significant.  It is precisely these macros that are
mostly likely to cause difficulties, and there are enough of them to
be problematic in practice and to make the effort of understanding,
annotating, or eliminating them worthwhile.

Sections~\ref{sec:first-content-section}--\ref{sec:last-content-section}
present the bulk of these results.  Section~\ref{sec:methodology}
describes our experimental methodology.  Section~\ref{sec:conclusion}
discusses the relevance of the research, suggests techniques for
mitigating the negative impact of Cpp on program understanding, and
discusses avenues for future work, while Section~\ref{sec:related}
discusses related work.
%  The remainder of this section [[does some stuff]].

%[[Point reader at conclusion/summary of results, at end of paper (and write
%that!).]]

% \subsection{Outline}
% 
% The remainder of this paper is organized as follows.
% 
% Section~\ref{sec:directives} reports the percentage of original C source
% code lines that are preprocessor directives, including a breakdown of the
% frequency of specific directives such as {\tt \#define}.  C programs
% commonly have preprocessor directives as over 10\% of their total lines,
% and over 20\% of the lines were directives in 3 of the {\numpackages}
% packages.
% 
% Section~\ref{sec:usage} reports how often each macro is defined and
% expanded.   Identifiers tend to be {\tt \#define}d relatively few times
% (96\% of macro identifiers had three or fewer definitions).  Many packages
% also have a significant number of macros that are never expanded, even
% disregarding system and library header files.
% 
% Section~\ref{sec:categorization} categorizes macro definitions according to
% their expansions; for example, macros may simply define a preprocessor
% symbol, define a literal, expand to a statement, etc.  We were particularly
% interested in determining the frequency of use of macros that are difficult
% to convert to other language features, such as those that string together
% characters as opposed to manipulating lexemes or syntactic units (less than
% one third of one percent of all macro definitions),
% those that expand to partial syntactic units such as unbalanced
% braces or partial declarations (half of one percent), and others not 
% directly expressible in the programming language (about four percent).
% 
% Section~\ref{sec:conclusion} discusses the relevance of the research,
% suggests techniques for mitigating the negative impact of Cpp on program
% understanding, and discusses avenues for future work, while
% Section~\ref{sec:related} discusses related work.


%[[Does anyone care about this?
%Another niche already filled by our tool is that of a ``macro lint''
%program which warns of potentially dangerous (or non-standard) uses of Cpp.
%And, we wrote Cppp.]]


% In order to assess the practical difficulty of understanding uses of Cpp
% (and the potential for replacement by other language constructs), 





%Overall, our analysis confirms that the C preprocessor is used in
%exceptionally broad and diverse ways, complicating the development of C
%programming support tools.  On the other hand, the analysis also convinces
%us that, by extending our analysis framework with some class type
%inferencing techniques (similar to those used for C to C++
%translation~\cite{Siff-fse96} and for program
%understanding~\cite{OCallahan-icse97}), we can take significant
%steps towards a tool that usefully converts a high percentage of Cpp code
%into C++ language features.  We are interested not in translations
%that merely allow a C program to be compiled by a C++ compiler (which is
%usually easy, by intentional design of C++) but those that take advantage
%of the added richness and benefits of C++ constructs.
%
%[[Inane, content-free.  Must die.]]
%In terms of the complexity of preprocessor usage, the results reported here
%contain both good news and bad.  By far
%the largest number of macro definitions and uses are relatively simple, of
%the variety that a programmer could understand without undue effort (although
%perhaps requiring tedious work) or that a relatively unsophisticated tool
%could understand (although in practice very few even try).  Despite the
%preponderance of innocuous macros, the preprocessor is so heavily used that
%the remaining ones are numerically significant.  It is precisely these
%macros that are mostly likely to cause difficulties, and there are enough
%of them to be problematic in practice and to make the effort of
%understanding, annotating, or eliminating them worthwhile.
%
%
%\subsection{Coping with Cpp}
%
%Tools\,---\,and, to a lesser degree, software engineers\,---\,have three
%options for coping with Cpp.    They may ignore preprocessor directives
%(including macro definitions) altogether, accept only post-processed code
%(usually by running Cpp on their input), or attempt to emulate the
%preprocessor.
%
%Ignoring preprocessor directives is an option for approximate tools, such
%as those based on lexical or approximate parsing techniques.  Accurate
%information about function extents, scopes, declared variables and
%functions, and other aspects of a program requires addressing the
%preprocessor.
%
%Operating on post-processed code, the most common strategy, is simple to
%implement, but then the tool's input differs from what the
%programmer sees.  Even when line number mappings are maintained, other
%information is lost in the mapping back to the original source code.
%For instance, source-level debuggers have no symbolic names or types
%for constants and functions introduced via {\tt \#define}, nor can tools
%trace or set breakpoints in function macros, as they can for ordinary
%functions (even those that have been inlined~\cite{Zellweger83:TR}).
%As another example, Siff
%and Reps describe a technique that uses type inference to produce
%C++ function templates from C; however, the input is ``a C program
%component that $\ldots$ has been preprocessed so that all include
%files are incorporated and all macros
%expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
%the readability and reusability of the resulting C++ templates.  As
%yet another related example, call graph extractors generally work in
%terms of the post-processed code, even when a human is the intended
%consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
%leave the software engineer responsible for inferring the mapping between the
%original and the post-processed source, which is an undesirable and
%error-prone situation.
%
%A tool that first preprocesses code, or takes already-preprocessed code as
%input, cannot be run on a non-syntactic program or one that will not
%preprocess on the platform on which the tool is being run.  These
%constraints complicate porting and maintenance, two of the situations in
%which program understanding and transformation tools are most likely to be
%needed.  Additionally, a tool supplied with only one post-processed
%instantiation of the source code cannot reason about the program as a
%whole, but only about that version that results from one particular set of
%preprocessor variables.  For instance, a bug in one configuration may not
%be discovered despite exhaustive testing of other configurations that do
%not incorporate particular code or do not admit particular execution paths.
%
%The third option, emulating the preprocessor, is fraught with difficulty.
%Macro definitions consist of complete tokens but need not be complete
%expressions or statements.  Conditional compilation and alternative macro
%definitions lead to very different results from a single original program
%text.  Preprocessing adds complexity to an implementation, which must trade
%off performing preprocessing against maintaining the code in close to its
%original form.  Extracting structure from macro-obfuscated source is not a
%task for the faint-hearted.  Despite these problems, in many situations
%only some sort of preprocessing or Cpp analysis can produce useful answers.
%
%All three approaches would be unnecessary if programs did not use
%preprocessor directives.  This is exactly what Stroustrup suggests:
%\begin{quote}
%  I'd like to see Cpp abolished.  However, the only realistic and
%  responsible way of doing that is first to make it redundant, then
%  encourage people to use the better alternatives, and {\em then\/}\,---\,years
%  later\,---\,banish Cpp into the program development environment with the
%  other extra-linguistic tools where it
%  belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
%\end{quote}
%C++ contains features\,---\,such as constant variables, inline functions,
%templates, and reference parameters\,---\,that obviate many uses of Cpp.
%Thus, translation to C++ is a path for partial elimination of Cpp.
%This study indicates the
%feasibility\,---\,and our framework for analyzing preprocessor usage
%provides a basis for the development\,---\,of an automatic translator with
%two attractive properties.  It would take as input C programs complete with
%preprocessor directives, and it would map many uses of directives into C++
%language features.  (It is not 
%practical to eliminate all uses of Cpp.  For example, C++ currently
%provides no replacement for the {\tt \#include} directive, or for
%stringization or pasting.  Macros that cannot be eliminated might be
%annotated with their types or 
%effects on parser or program state, so that even tools that do no Cpp
%analysis can operate correctly on such programs.)
%
%[[Where do these two points go?]]
%
%Another niche already filled by our tool is that of a ``macro lint''
%program which warns of potentially dangerous (or non-standard) uses of Cpp.
%
%And, we wrote Cppp.

%O'Callahan and Jackson also use type
%inference, although for program understanding rather than translation;
%they, too, apply their techniques to post-processed
%code~\cite{OCallahan-icse97}.


%\subsection{Cpp: not all bad}
%
%[[This section is out of place and horrible.  Where should the information
%go?]]
%
%Despite its evident shortcomings, Cpp is a useful and often necessary
%adjunct to C, for it provides capabilities unavailable in the language or
%its implementations.  Cpp permits definition of portable language
%extensions that can define new syntax, abbreviate repetitive or complicated
%constructs, or eliminate reliance on a compiler implementation to
%open-code (inline) functions, propagate symbolic constants, eliminate dead
%code, and short-circuit constant tests.  The latter guarantees are
%especially valuable for compilers that do a poor job optimizing or when the
%programmer wishes to override the compiler's heuristics.  Cpp also permits
%system dependences to be made explicit and tested, resulting in a clearer
%separation of concerns.  Finally, Cpp permits a single source to contain
%multiple different dialects of C; a frequent use is to support both
%K\&R-style and ANSI-style declarations.
%
%%% NEED A REFERENCE TO DEBUGGER HERE!
%%% also mention Emacs hide-ifdef mode
%
%Our long-term goal is not to take these useful features away from
%programmers, but to reduce Cpp use, making programs easier for humans to
%understand and tools to analyze.






\section{Methodology}
\label{sec:methodology}

We analyzed {\numpackages} publicly-available software packages which
represent a mix of application domains, authors, programming styles, and
sizes.  Some are interactive, while others are not; some are graphical,
while others are text-based, command-line applications.
Figure~\ref{fig:packages} describes the packages and lists their sizes in
terms of physical lines (newline characters) and non-comment, non-blank
(NCNB) lines.  The NCNB figure disregards lines consisting of only comments
or whitespace, as well as lines in a conditional that cannot evaluate to
true (such as {\tt \#if 0}, which is frequently used to comment out code).  The
remainder of our analysis uses the NCNB length, which more accurately
reflects the amount of source code.

\begin{figure}
\centering
{% ``\small'' here does have an effect, despite previous comment to the contrary
  \small
  \setlength{\tabcolsep}{.25em}
  \input{tbl-package-sizes.tex}
}
\captionsmall{Analyzed packages and their sizes.  NCNB lines are non-comment,
  non-blank lines.}
\label{fig:packages}
\end{figure}

Before performing our analysis, we ran {\tt configure} or the equivalent on
each package in order to prepare it for compilation.  This creates various
header files (such as \file{configure.h}).  Our analysis works in the
absence of this step, but warns about uses of never-defined macros and
under-reports some values related to those missing definitions.  We did
not, however, compile the packages; our analysis does not require that the
package be compilable (or even that the preprocessor be able to run on it).

We generated a list of all the C files in the package (both code and header
files).  In general, these have extensions like \file{.c}, \file{.h},
\file{.cc}, \file{.cpp}, \file{.hxx}, etc.  We removed files with such
extensions that don't actually contain valid C, and added others which were
{\tt \#included} by valid C files (some included files had extension
\file{.def}).

We analyzed all the C files in the package, as well as every file {\tt
\#include}d by any of those, primarily library header files.  We
took care to include as many libraries as possible so as not to overlook
those definitions.  However, the figures reported in this paper always omit
all macros defined only in libraries, as well as all macro uses in
libraries.  This prevents libraries from 
swamping the characteristics of the package code, which is our focus in
this study.  The programmer generally has no control over libraries and
their header files, and may not even know whether a library symbol is
defined as a macro or a true function or variable.  Finally, we assume that
library macros are carefully written to behave correctly and robustly
(sadly, we discovered this not to always be true in practice; an
analysis of library macros is outside the scope of this report).

Our analysis is a true whole-program analysis; rather than preprocessing the
code and examining just one configuration, we examine all possible code and
ignore no possible  conditional compilation conditions (though we do skip
over those which can be statically proven to be false).   As a result, this
analysis is more thorough than traditional ``whole-program'' analyses.

% [[We merge different Cpp branches in the code wherever possible.  Discuss
% this a bit.]]

We performed our analysis via a collection of Perl scripts, totaling
approximately 11,000 lines (7,000 NCNB lines).  We perform approximate
parsing because the input may not be a valid C program; as a result, we
miss some constructs, but we can cope with uncompilable C and with partial
constructs in conditional compilation branches.  Our tool includes parsers
for expressions, statements, and declarations.  The raw data, which
includes considerable data not reported here, and the programs used to
generate and manipulate them, are available from the authors.


\section{Occurrence of preprocessor directives}
\label{sec:directives}
\label{sec:first-content-section}

Figure~\ref{fig:directives-breakdown} shows how often preprocessor
directives appear in the programs we analyzed.  Each group of bars in the
figure represents the percentage of NCNB lines
attributed to the specified category of directives, with each individual
bar showing the percentage for a specific package.  Conditional compilation
directives ({\tt \#if}, {\tt \#ifdef}, {\tt \#ifndef}, {\tt \#else}, {\tt
\#elif}, {\tt \#endif}) are grouped together.

\begin{figure}
\centerline{\epsfig{file=fig/directives-breakdown.eps,height=7.5in}}
\captionsmall{Preprocessor directives as a fraction of non-comment,
  non-blank (NCNB) lines.  For
  example, 2.0\% of gzip's NCNB lines are {\tt \#include}s, and 4.4\% of all
  lines across the packages are conditional compilation directives.}
\label{fig:directives-breakdown}
\end{figure}

The prevalence of preprocessor use makes understanding Cpp constructs
crucial in any analysis of a program.  One in ten program lines is a
preprocessor directive rather than C code.  Across
packages, the percentage varies from less than 4\% to more than 22\%.
(These figures do not include the 28\% of lines which expand a macro or the
38\% of lines whose inclusion is controlled by {\tt \#if}; see
Section~\ref{sec:dependence}.)

% \#if 46\%, \#define 35\%, \#include 13\%, \#undef 3\%, \#line 2\%

Conditional compilation directives account for just under half (46\%) of
the total directives in all packages, macro definitions comprise another
35\%, and file inclusion makes up most of the rest.  Packages are not very
uniform in their mix of preprocessor directives, however.  (If they were,
each group of bars in figure Figure~\ref{fig:directives-breakdown} would be
a scaled version of the top group.)  In particular, the prevalence of {\tt
\#include} is essentially independent of incidence of other directives.
The percentage of conditional directives varies from 16\% to 74\%, the
percentage of {\tt \#define}s varies from 14\% to 52\%, and the percentage
of {\tt \#include}s varies from 4\% to 60\%.  This variation in usage
indicates that a tool for understanding Cpp cannot focus on just a subset
of directives.  


\subsection{{\tt \#line}, {\tt \#undef}, and other directives}

The definedness of a macro is often used as a boolean value.  However, {\tt
\#undef} is rarely used to set such macros to ``false''$\!$.  Most uses of
{\tt \#undef} immediately precede a definition of the just-undefined macro,
to avoid preprocessor warnings about incompatible macro redefinitions.

Every use of {\tt \#line} (in \pkg{bash}, \pkg{cvs}, \pkg{flex}, \pkg{fvwm},
\pkg{gawk}, \pkg{gcc}, \pkg{groff}, and \pkg{perl}) appears in lex or yacc
output that enables packages to build on systems lacking lex, yacc, or
their equivalents.  For instance, \pkg{flex} uses itself to parse its
input, but also includes an already-processed version of its input
specification (that is, C code corresponding to a {\tt .l} file) for
bootstrapping.

% , as are ``other'' directives (such as ).  
% as well as user-defined ones like {\tt \#module}

Rarely-appearing directives such as {\tt \#pragma}, {\tt \#assert}, and
{\tt \#ident}, and unrecognized directives, are omitted from
Figure~\ref{fig:directives-breakdown}.  Among the packages we studied,
these directives account for .017\% of directives, or one in six thousand.
Their only significant user is \pkg{g77}, which contains 154 uses of {\tt
\#error} (representing 1.5\% of its preprocessor directives and 0.16\% of
its NCNB lines) to check for incompatible preprocessor flags.  We ignore
the null command (``{\tt \#}'' followed by only whitespace), which produces
no output.


\subsection{Packages with heavy preprocessor use}

The \pkg{gzip}, \pkg{remind}, and \pkg{bash} packages deserve
special attention for their heavy preprocessor usage\,---\,22\%, 21\%, and
16\%, respectively.

\pkg{gzip} {\tt \#define}s disproportionately many macros as literals and
uses them as arguments to system calls, enumerated values, directory
components, and more.  These macros act like {\tt const} variables and are
evidence of good programming style.  \pkg{gzip} also contains many
conditional compilation directives, since low-level file operations (such
as setting creation time and access control bits, accessing directories,
and so forth) are done differently on different systems.

\pkg{remind} supports speakers of multiple natural languages by using {\tt
\#define}d constants for basically all user output.  It also contains
disproportionately many conditional compilation directives; over half of
these test the definedness of \verb|HAVE_PROTO|, in order to provide both
K\&R and ANSI prototypes.

Like \pkg{gzip}, \pkg{bash} is portable across a large variety of
systems, but \pkg{bash} uses even more operating system services.
Ninety-seven percent of \pkg{bash}'s conditional compilation directives
test the definedness of a macro whose presence or absence is a boolean
flag indicating whether the current system supports a specific feature.
The presence or absence of a feature requires different (or sometimes
additional) system calls or other code.


\section{Macro definition bodies}
\label{sec:categorization}

This section examines features of macro definitions that may complicate
understanding the containing program.  We report how many macro definitions
expand to a partial or unidentifiable syntactic entity, take advantage of
special Cpp features that lie outside the programming language, or contain
other error-prone constructs.  We then turn to multiple definitions of a
particular macro name.  Multiple definitions can complicate understanding,
even if they do effectively the same thing.  We report on incidence of
redefinitions and of differing redefinitions, especially redefinitions with
incompatible bodies.

[[Punchline/summary goes here.]]



\subsection{Macro body categorization}

We categorized macro bodies into 28 categories, though for simplicity of
presentation, this paper coalesces these into ten higher-level categories.
We started with a set of categories that we expected to occur frequently
(similar to other macro
taxonomies~\cite{Stroustrup-DesignEvolution,Carroll95}), then iteratively
refined them to break up overbroad categories or add unforeseen ones.

Figure~\ref{fig:categorization} reports, for each package, how many
definitions create an expansion which falls in each category.  Macros which
act like C language constructs\,---\,such as variables or
functions\,---\,are easiest to analyze, understand, and perhaps even
translate into other language constructs.  Thus, there is reason to believe
that the 70\% of macros whose bodies are expressions and the 6\% that are
statements can be handled relatively easily by people and tools.  Other
macros, especially those which do not expand to a complete syntactic
construct, are more problematic.


The ten categories are as follows.  The examples are chosen for clarity
and brevity from the packages studied.

% Where does this go?
% There's no pattern, again.  (Nor is there a pattern by package size
% or by type of application.)

% The following isn't quite enough to get the columns lined up in this table.
% \newcolumntype{d}{D{.}{.}{2}}
% \begin{tabular}{|l|d|d|d|d|d|d|d|}\hline
\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{categories-tbl.tex}}%
% }
\centerline{\epsfig{file=fig/def-categories.eps,height=6in}}
\captionsmall{Categorization of macro definition bodies.  The legend numerically
  represents the information in the top row[[; the category names in the
  legend should be read across, by rows]].}
\label{fig:categorization}
\end{figure}



\label{sec:categorization-details}

\begin{description}
  \sloppy
  \emergencystretch=2em

%%  mcat_NULL: 5391
%%    100%  null_define (5391)
% @mcat_NULL = qw( catNULL_DEFINE );
\item[Null define]  The {\tt \#define} directive gives only an
  identifier name but no macro body, as in {\tt \#define
  \verb|HAVE_PROTO|}\@.  Such macros appear frequently in Cpp
directives (such as {\tt \#ifdef}), where they are used as boolean
variables by the preprocessor.  In code, they often represent optional
syntax.  For instance,
macro {\tt private} may expand either to {\tt static} or to nothing,
depending on whether a debugging mode is set.

%%  mcat_CONSTANT: 18971
%%    0.00%  constant (0)
%%    97%  literal (18426)
%%    2.9%  some_constant (545)
% @mcat_CONSTANT = qw( catCONSTANT catLITERAL catSOME_CONSTANT );
\item[Constant] The macro body is either a literal (97\% of this category)
  or an operator applied to constant values (3\% of this category).
  These macros act like {\tt const} variables. 
  For instance, {\tt \#define NULL 0}, {\tt \#define \verb|ARG_MAX|
  131072}, and {\tt \#define ETCHOSTS "/etc/hosts"} define literals, while
{\tt \#define \verb|RE_DUP_MAX| ((1<<15)-1)} and {\tt
\#define \verb|RED_COLS| (1 << \verb|RED_BITS|)} (where \verb|RED_BITS| is
a constant, possibly a literal) define constants.  This category includes
both macros whose value is invariant across all configurations of the
package and those which depend on other compile-time values.  

%%  mcat_NONCONSTANT_EXPRESSION: 13529
%%    100%  expression (13529)
% @mcat_NONCONSTANT_EXPRESSION = qw( catEXP );
\item[Expression]  The macro body is an expression, as in {\tt \#define
  sigmask(x) (1 << ((x)-1))} or {\tt \#define mtime mailfiles[i]->\verb|mod_time|}.
Such a macro acts like a function which returns a value (though the
macro need not take any arguments, so its uses may look syntactically
unlike function calls). 
The expression might have a single constant value everywhere (the usual
case for expression macros without arguments, most of which are classified
as constants, above) or might have a different value on each use (the usual
case for expression macros with arguments).

%%  mcat_STATEMENT: 2656
%%    47%  statement (1242)
%%    45%  semicolonless_statement (1197)
%%    1.5%  partial_statement (40)
%%    3.0%  statements (79)
%%    3.5%  semicolonless_statements (94)
%%    0.15%  partial_statements (4)
% @mcat_STATEMENT = qw( catSTATEMENT catSTATEMENT_SANS_SEMI catPARTIAL_STATEMENT
%                        catSTATEMENTS catSTATEMENTS_SANS_SEMI catPARTIAL_STATEMENTS );
% These aren't great examples (not from actual code); but so be it, as the
% actual examples are *very* long.
\item[Statement]\label{item:statement-category}
  The macro body is a complete statement such as
\begin{verbatim}
    #define L_ORDINAL_OVERRIDE plu = ".";
    #define FREE(x) if (x) {free(x); x=NULL;}
    #define SWALLOW_LINE(fp) { int c; while ((c = getc(fp)) != '\n' && c != EOF); }
\end{verbatim}
  Such a macro is like a function returning {\tt void}, except that uses
  should not be followed by a semicolon (see Section~\ref{sec:lint}).
    
  To reduce the number of categories in this presentation, the statement
  category aggregates single statements (comprising 47\% of the category),
  statements missing their final semicolon (as in {\tt \#define QUIT if
  (\verb|interrupt_state|) \verb|throw_to_top_level|()}; these account
  for 45\%), multiple statements (3.0\%), multiple statements where the
  last one is missing its final semicolon (3.5\%), and partial statements
  (as in {\tt \#define ASSERT(p) if (!(p)) botch(\verb|__STRING|(p));
  else}; these are the final less than 2\% of the statement category).

%%  mcat_TYPE: 697
%%    82%  type (569)
%%    0.00%  partial_type (0)
%%    3.3%  declaration (23)
%%    15%  semicolonless_declaration (105)
% @mcat_TYPE = qw( catTYPE catPARTIAL_TYPE catDECLARATION catDECLARATION_SANS_SEMI);
% #my @mcat_DECLARATION = qw( catDECLARATION catDECLARATION_SANS_SEMI );
% #folded DECLARATION into the above, TYPE
% The DECLARATION ones are quite rare.
\item[Type] 
  These macros expand to a type or partial type (such as a storage class),
  or expand to a declaration (possibly missing its terminating semicolon).
  Examples include {\tt \#define \verb|__ptr_t|
  void~*}, {\tt \#define \verb|__INLINE| extern inline}, {\tt \#define
private static}, {\tt \#define \verb|FLOAT_ARG_TYPE| union
\verb|flt_or_int|}, and {\tt \#define CMPtype SItype}.  As a result, these
macros may be tricky to understand, and cannot be eliminated via
straightforward translation (though C++ templates may provide some hope).



%%  mcat_SYNTAX: 189
%%    18%  mismatched_entities (34)
%%    82%  punctuation (155)
% This includes ( catUNBALANCED catPUNCTUATION )
\item[Syntactic]  The macro body is either punctuation (82\% of this
  category; for example, {\tt \#define AND ;}) or contains unbalanced
  parentheses, braces, or brackets.  The latter are often used to create a
  block and perform actions that must occur at its beginning and end, as
  for \verb|BEGIN_GC_PROTECT| and \verb|END_GC_PROTECT|.
  Macros in this category are inexpressible in the underlying programming
  language, but depend on the preprocessor's manipulation of uninterpreted
  token streams; see also Section~\ref{sec:extra-linguistic}.

%%  mcat_SYMBOL: 870
%%    1.7%  reserved_word (15)
%%    94%  function_name (820)
%%    4.0%  symbols (35)
%%  fudged to:
%%    1.8%  reserved_word (15)
%%    98%  function_name (820)

% @mcat_SYMBOL = qw( catRESERVED_WORD catFUNCTION_NAME catSYMBOLS);
\item[Symbol]
  The macro body is a single identifier that is either a function name
  (98\% of this category) or a reserved word (2\%, much of it uses of
  variable names such as {\tt new} which are reserved words in another
  C dialect).  A macro body which is a macro name inherits that macro's
  classification rather than appearing here.


%%  mcat_SYMBOL_UNKNOWN: 2765
%%    100%  unknown_symbol (2765)
% @mcat_SYMBOL_UNKNOWN = qw( catSYMBOL_UNKNOWN );
\item[Unknown symbol]
  The macro expands to a single symbol which is not defined in the package
  or in any library header files included by the package.  The symbol may
  be defined by compiler command arguments or may be used only inside an
  appropriate conditional compilation guard because it is only meaningful
  with a particular architecture, system, or library (for which we did not
  have header files available).
  
  Unknown symbols can also be variables or functions that we failed to
  parse.  Our approximate parser can succeed where an exact parse would not
  (as for unsyntactic code or entities interrupted by preprocessor
  directives), but sometimes fails to recognize declarations or
  definitions.


%%  mcat_NON_C_CODE: 639
%%    90%  command_line_arguments (574)
%%    10%  assembly_code (65)
% @mcat_NON_C_CODE = qw( catCOMMAND_LINE catASSEMBLY_CODE );
\item[Not C code]\label{page:not-c-code}
  The predominant use of such macros is for filenames and operating system
  command lines (together, 90\% of this category) and assembly code (the
  remaining 10\%).  The former usually appear in a file which is used by
  the preprocessor when run over both code and Makefiles.\footnote{Cpp's
    specification states that its input should be syntactic C code, so it
    can avoid performing replacements in comments and string constants.
    Cpp may not behave as expected when its input is not C, so it is good
    style to avoid such uses.  In practice, such uses have forced many C
    preprocessors to make fewer assumptions about their input than they
    otherwise might be able to.}  Our heuristics misclassify some such
  macros\,---\,after all, {\tt \#define \verb|SYSDEP_CFLAGS| -43 -w}
  creates a perfectly valid C expression.  (See the ``expression plus not C
  code'' line at 0.14\% in Figure~\ref{fig:subset-categories}.)  The
  assembly code component includes only macros whose expansion is assembly
  code, not all expressions and statements that contain snippets of
  assembly code.

%%  mcat_FAILURE: 802
%%    0.00%  uncategorized (0)
%%    7.9%  being_categorized (63)
%%    10%  never_defined (82)
%%    78%  failed_categorization (628)
%%    3.6%  multiply_categorized (29)
% @mcat_FAILURE = qw( catNOT_YET catIN_PROCESS catNO_DEF catFAILURE catMULTIPLE );
\item[Failed classification]
  Our tool failed to categorize less than 2\% of the 46462 definitions, and
  8 of the {\numpackages} packages had no macro classification failures.
  No one variety of macro stands out among the failures, so a more complete
  categorization is unlikely to affect our conclusions.
  
  Some failures resulted from limitations of our parser, which does not
  handle pasting (defined in Section~\ref{def:pasting}), C++, or some
  non-portable C extensions.  Handling of partial entities is incomplete,
  so case labels, parts of structure declarations, and partial expressions
  are left unclassified.  Heuristics for recognizing operating system
  command lines or flags occasionally fail.  Additional failures result
  from macro bodies which cannot be unambiguously assigned to another
  category because their syntactic class depends on their
  arguments\,---\,such as {\tt \#define EXFUN(name, proto) name proto} and
  {\tt \#define
\verb|DO_OP|(OP,a,b) (a OP b)}.  Finally, macros in bodies were not
expanded (though their classifications were examined), causing a small
amount of cascading of failures when complicated or unclassified macros
appeared in other macro definitions.

\end{description}


%In anticipation of the translator tool, the analysis tool infers
%types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
%and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
%type information is in the early stages, however, and we do not report
%on the preliminary results in this paper.

% [FIX: Benefits even from simple literal constant conversion -- exposes
% symbolic information to the debugger]

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.]



\subsection{Extra-linguistic capabilities}
\label{sec:extra-linguistic}

The C preprocessor has capabilities outside the C programming language;
indeed, this is a primary motivation for using Cpp.  Such constructs can
present special challenges to program understanding, and especially to
reducing the use of the preprocessor by translation into C or C++.  This
section
presents a list of such features and evaluates their frequency of
appearance, both individually and in combination, in our test suite.

We had anticipated problems dealing with macros that use stringization and
pasting, the two explicit extra-linguistic features of Cpp.  These macros
appear in less than one tenth of one percent of all macro definitions.  Far
more prevalent is exploitation of Cpp's lack of structure to effect
mechanisms not available in C\@.  In particular, the fact that Cpp's inputs
and outputs are uninterpreted token streams with no constraints on their
structure permits Cpp to perform arbitrary transformations using
non-first-class and partial syntactic constructs.  Macros that manipulate
types or partial types are moderately common and problematic for program
understanding tools.

\begin{figure}
  {\small\centerline{\input{tbl-subset-properties.tex}}}
  
  \captionsmall{Usage, both singly and in
    combination, of the extra-linguistic capabilities of the C
    preprocessor listed in Section~\ref{desc:properties}.  The features are
    listed across the top, along with the percentage of macro definitions
    exploiting each.  Each row of the table reports the percentage of all
    macro definitions that use a particular combination of the
    capabilities, indicated by black squares.  For instance, 0.093\% of all
    macro definitions perform assignment and use the result of a macro
    invocation as a type, but use none of the other extra-linguistic
    features listed.  The last five lines each represent one macro
    definition.}
  \label{fig:subset-properties}
\end{figure}

One in six macros (16.8\%) contains an extra-linguistic construct or a
construct that can be used by Cpp to achieve an extra-linguistic effect.
Figure~\ref{fig:subset-properties} breaks down these macros by the
constructs they contain.  In addition to showing the prevalence of each
construct, the figure shows which ones occur together.  The following list
describes in detail the the constructs appearing in
Figure~\ref{fig:subset-properties}.


\label{desc:properties}

\begin{description}
\item[Free variables]\label{page:freevar}
  The macro body uses as a subexpression (that is, applies an operator or
  function to) a symbol which is not a formal argument, a variable defined
  in the macro body, or a function, macro, typedef, or reserved word.  Such
  symbols are typically local or global variables.  Uses of global
  variables are generally innocuous; uses of local variables (in which the
  local definition in scope at the point of use captures the free variable
  in the macro body) can be used to achieve dynamic scoping, which C does
  not directly support.  We did not separately analyze global and local
  free variables.

\item[Assignment]
  The macro body side-effects state via assignment (of the form {\tt =},
  {\tt {\em op}=}, {\tt -{}-}, or {\tt ++}).  We did not look for calls to
  functions or macros that might have side effect, nor did we discount side
  effects to variables local to the macro body.  
  
  Assignment is not extra-linguistic per se, but macros containing
  assignment operators have potentially unexpected results (doubly so for
  those with no arguments, whose invocations look like variable uses, not
  function calls), some of which lie outside the scope of C\@.  A macro
  argument that is assigned to is similar to a pass-by-reference function
  argument and need only be noted in the macro's documentation.  A macro
  that assigns a global variable also presents no difficulties in
  understanding or translation into a C++ inline function.  Assignment to
  other variables free in the macro body demands that such a variable exist
  wherever the macro is invoked, and assigns to different variables at
  different invocations.  Such a macro implements a restricted form of
  dynamic scoping by capturing the instance of a variable visible at the
  point of macro invocation.

\item[Use macro as type]
  In this macro's body, the result of another macro invocation is used as a
  type\,---\,for instance, in a declaration or a type cast.  C cannot
  simulate this behavior, because its types are not first class; they may
  not be passed to functions, returned as results, or otherwise
  manipulated.

\item[Pass type as argument]
  In this macro's body, a literal type is passed to another macro, as in
  {\tt \#define PTRBITS \verb|__BITS|(char*)}.  Like using a macro result
  as a type, this is impossible in C\@.

\item[Pasting]\label{def:pasting}
  The body uses symbol pasting ({\tt \#\#}), which treats its arguments not
  as tokens but as strings, constructing a new token out of their
  concatenation, as in {\tt \#define \verb|_SIZEOF|(x) \verb|sz_|\#\#x},
  after which the macro invocation {\tt \verb|_SIZEOF|(int)} expands to the
  symbol {\tt \verb|sz_int|}.  The resulting symbol might appear literally, or
  only as a pasted symbol, at its other uses.  Since pasting is often
  abstracted out into a separate macro\,---\,such as {\tt \#define
  \verb|__CONCAT|(x,y) x \#\# y}\,---\,the incidence of pasting is higher
  than the direct uses reflected by this statistic.

\item[Use argument as type]
  This macro uses one of its arguments as a type.  Not all uses can be
  unambiguously identified lexically.  For instance, the macro 
  {\tt \#define \verb|MAKE_DECL|(type, name) type name;}
  is not identified as necessarily using its first argument as a type, for
  it might be invoked as {\tt \verb|MAKE_DECL|(printf, ("hello
  world\verb|\|n"))} or as {\tt \verb|MAKE_DECL|(x =, y+z)}.
  Like using a macro result
  as a type, this is impossible in C\@.

\item[Self-referential]
  The body refers to its own name, as in {\tt \#define LBIT vcat(LBIT)}.
  This feature is used when building a wrapper around an existing function
  or variable.  Since the ISO C preprocessor performs only one level of
  expansion on such recursively defined macros,\footnote{Pre-ANSI
    implementations were permitted to loop forever when expanding
    self-referential macros.} the expanded macro contains a reference to
  the original name.

\item[Stringization]
  The body uses argument stringization ({\tt \#}), which replaces its
  argument (a preprocessor symbol) by its contents as a C string.  After
  {\tt \#define FOO BAR BAZ}, the expression {\tt \#FOO} expands to {\tt
  "BAR~BAZ"}.  Examples using stringization include
\begin{verbatim}
    #define spam1(OP,DOC) {#OP, OP, 1, DOC},
    #define REG(xx) register long int xx asm (#xx)
\end{verbatim}
  No C or C++ language mechanism can replace such macros.  This feature is
  particularly useful in debugging, in order to print or record the exact
  operations being performed.

\end{description}



\subsection{Cpp pitfalls:  macro lint}
\label{sec:lint}

Cpp gives rise to syntactically valid but potentially dangerous programming
constructs which might cause a macro not to behave as anticipated (and not
to behave like an ordinary function).  Unlike the extra-linguistic
constructs discussed in Section~\ref{sec:extra-linguistic}, these are more
likely to represent bugs than to use Cpp mechanisms to achieve results
outside the abilities of the C language.  However, some of the potential
bugs (such as multiple executions of side effects on a macro argument
which appears more than once in the expansion) do
result from differences between C's standard execution model and the
macro expansion performed by Cpp.

Because it flags such errors, our tool can be used as a sort of macro lint.
(A more polished implementation would permit each warning message to be
enabled and disabled individually.)

\begin{figure}
  {\small\centerline{\input{tbl-lint.tex}}}
  
  \captionsmall{Macro lint.  This table indicates the frequency of occurrence of
    error-prone constructs in macro bodies.  Except where specifically
    noted, the percentages refer to the number of macro definitions.  Macro
    definitions falling in the ``not C code'' category (see
    page~\pageref{page:not-c-code}) are omitted.}
  \label{fig:macro-lint}
\end{figure}

One fourth of all macro definitions triggered at least one such warning,
and one fourth of macro names have a definition which triggers a warning.
Figure~\ref{fig:macro-lint} further breaks down the warnings.  Most of
these represent legitimate problems with the way macros are written and
represent bugs waiting to happen.  If they are not currently a problem,
that is because their present uses happen not to trigger the problematic
conditions, as checks of some of the macros indicated.  Future uses,
especially by programmers not familiar with the (generally undocumented)
caveats relating to use of each macro, may well give rise to these dormant
errors.  

% We were surprised that more macros did not take simple precautions
% such as preventive parenthesization or wrapping macro bodies in {\tt do
% {\rm \ldots} while (0)}.

The following further describes the errors collected in
Figure~\ref{fig:macro-lint}.


\begin{description}
\item[free variables]
  See page~\pageref{page:freevar} for a discussion of issues related to
  free variables.  We specifically check for side-effected formal arguments
  as well; see below.

\item[multiple formal uses]
        Some argument is used as an expression multiple times, so any side
        effects in the actual argument expression will occur multiple
        times.  
        Given a macro defined as
\begin{verbatim}
    #define EXP_CHAR(s) (s == '$' || s == '`' || s == CTLESC)
\end{verbatim}
        an invocation such as {\tt \verb|EXP_CHAR|(*p++)} increments the
        pointer by three locations rather than just one as intended (and as
        would occur were \verb|EXP_CHAR| a function).  Even if the argument
        has no side effects, as in \verb|EXP_CHAR|(peekc(stdin)), repeated
        evaluation may be unnecessarily expensive.
        
        Some C dialects provide an extension for declaring a local variable
        within an expression.  In GNU C~\cite{GCC}, this is achieved in the
        following manner:
\begin{verbatim}
    #define EXP_CHAR(s) ({ int _s = (s); (_s == '$' || _s == '`' || _s == CTLESC) })
\end{verbatim}

\item[unparenthesized formal uses]
        Some argument is used as a subexpression (i.e., is adjacent to an
        operator) without being enclosed in parentheses, so that precedence
        rules could result in an unanticipated computation being performed.
        For instance, in
\begin{alltt}
    #define DOUBLE(i) (2*i)
    \ldots\ DOUBLE(3+4) \ldots
\end{alltt}
        the macro invocation computes the value 10, not 14.
        This warning is suppressed when the argument is the entire body
        or is the last element of a comma-delimited list (which has 
        low precedence).

\item[unparenthesized body]
        The macro body is an expression which ought to be parenthesized to
        avoid precedence problems at the point of use.  For instance, in
\begin{alltt}
    #define DOUBLE(i) i+i
    \ldots\ 3*DOUBLE(4) \ldots
\end{alltt}
        the expression's value is 16 rather than 24.
        
        This warning is applicable only to macros that expand to an
        expression and is suppressed if the body is a single token or a
        function call (which has high precedence).

\item[doesn't swallow semicolon]\label{item:swallow-semicolon}
        The macro body takes arguments and expands into a statement or
        multiple statements.  Thus, its invocations look like function
        calls, but it cannot be legally used like a function call, as in
\begin{alltt}
    #define ABORT() kill(getpid(),SIGABRT);
    \ldots
    if (*p == 0)
      ABORT();
    else \ldots
\end{alltt}
        because {\tt ABORT();} expands to two statements (the second a null
        statement), which is unsyntactic between the {\tt if} condition and
        {\tt else}.

        Macros without arguments, such as {\tt \#define \verb|FORCE_TEXT|
        \verb|text_section|();}, suppress this warning on the theory that their
        odd syntax will remind the programmer not to add the usual semicolon.

        The solution to this problem is to wrap the macro body in
\begin{alltt}
             do \verb|{| \ldots\ \verb|}| while (0)
\end{alltt}
        which is a partial statement that requires a final semicolon.  To
        our surprise, we found few uses of this standard, widely-recommended
        construct but many error-prone uses of statement macros followed by
        semicolons.

\item[null body with arguments]
        The macro is a null define of the form {\tt \#define name(e)} 
        which might have been intended to be {\tt \#define name (e)}.
        An empty comment is the idiomatic technique for indicating that the
        null definition is not a programming error, so a comment where the macro
        body would be suppresses this error, as in
\begin{verbatim}
    #define __attribute__(Spec) /* empty */
    #define ReleaseProc16(cbp) /* */
    #define __inline /* No inline functions.  */
    #define inline /**/
\end{verbatim}

\item[side-effected formal]
        A formal argument is side-effected.  This is erroneous if the
        argument is not an lvalue.  A similar constraint applies to
        reference parameters in C++, which can model such macro arguments.

\item[inconsistent arity]
        The macro name is defined multiple times with different arity; for example,
\begin{verbatim}
    #define ISFUNC 0
    #define ISFUNC(s, o) ((s[o + 1] == '(')  && (s[o + 2] == ')'))
\end{verbatim}
        This may indicate either a genuine bug or a macro name used for
        different purposes in different parts of a package, in which case
        the programmer must take care that the two are never simultaneously
        active (lest one override the other).  The latter situation may be
        caught by Cpp's redefinition warnings, if the macro name is not
        subjected to {\tt \#undef} before the second definition.

\item[swallows else]
        The macro, which ends with an {\tt else}-less {\tt if} statement,
        swallows any {\tt else} clause that follows it.  For instance, after
\begin{verbatim}
    #define TAINT_ENV() if (tainting) taint_env()
    #define merge_(a,b) if (TM_DEFINED (b)) (a) = (b);
\end{verbatim}
        a use like
\begin{alltt}
    if ({\rm\em{}condition})
      TAINT_ENV();
    else \ldots
\end{alltt}
        results in the {\tt else} clause being executed not if  the
        condition is false, but if it is true (and {\tt tainting} is also
        true).
        
        This problem results from a potentially incomplete statement (though
        an {\tt if} statement doesn't require an {\tt else} clause) which
        may be attached to some following information.  It is the mirror of
        the ``doesn't swallow semicolon'' problem listed above which
        resulted from a too-complete statement which failed to be
        associated with a textually subsequent token.  The solution is
        similar: either add an else clause with an empty statement, as in
\begin{verbatim}
    #define ASSERT(p) if (!(p)) botch(__STRING(p)); else
\end{verbatim}
        or wrap statements in {\tt \verb|{| \ldots\ \verb|}|} and wrap
        partial statements in {\tt do \verb|{| {\rm \ldots}\ \verb|}| while
        (0)}.

\item[bad formal name]
        The formal name is not a valid identifier or is a reserved word
        (possibly in another dialect of C), as in
\begin{verbatim}
    #define CR_FASTER(new, cur) (((new) + 1) < ((cur) - (new)))
\end{verbatim}
        This presents no difficulty to Cpp, but a programmer reading the
        body (especially a more complicated one) may become confused.

\end{description}


In addition to these errors, our tool discovered a number of additional
errors.  There were some illegal constructs, such as {\tt \#module} (which
is not a meaningful Cpp directive) and {\tt \#undef
\verb|GO_IF_INDEXABLE_BASE|(X, ADDR)} ({\tt \#undef} takes a macro name,
not the arguments as they appeared in the {\tt \#define} directive).
  
Use of {\tt /**/}-style pasting is not uncommon, especially in {\tt CONCAT}
macros which provide portability across older and newer versions of the
preprocessor.  Pre-ANSI versions of Cpp merged adjacent symbols, and the
symbols are juxtaposed by placing a comment, which Cpp removes, between
them.  This construct does not perform merging in newer implementations,
so users are warned of its appearance.

A number of files in our test suite begin or end inside a brace scope or an
{\tt \#if} scope.  Some of these were intentional\,---\,as in files meant
to be included around other code.  Others are bugs (such as, in one case, a
failure to close a {\tt /* */} style comment) which were apparently not
discovered because testing did not build the package under all possible
configurations.

Our tool also warns about a number of stylistic mistakes, such as
unexpected indentation of forms (or lack of indentation where it is
expected).  These warnings are fairly frequent in machine-generated files
(such as lex and yacc output), but often indicate problems (in readability
if not in logic) elsewhere.


\subsection{Multiple definitions}
\label{sec:mult-def}

Because a package may contain multiple definitions of a macro, and a macro
can even be redefined partway through preprocessing, it is difficult to
determine exactly which definition of a macro will be used at a particular
expansion site.

The average number of definitions is less than 2; 79\% of macros are
defined just once, and only 5\% of macros have four or more definitions.
The more complicated macros to handle\,---\,such as types, syntactic
macros, and those that could not be classified\,---\,are most frequently
redefined.  In all but five packages, at least 94\% of all macros are defined
three or fewer times; a few have have unusual redefinition patterns,
largely because of portability or dependence on operating system
characteristics.


\begin{figure}
\centerline{\epsfig{file=fig/cat-def-frequency.eps,height=7.2in}}
\captionsmall{Number of definitions ({\tt \#define} directives) per Cpp
  identifier, graphed as the percentage of identifiers that are defined a
  given number of times or fewer.  Overall, 95\% of macros were defined
  three or fewer times; the other 5\% of macros had four or more distinct
  definitions.}
\label{fig:freq-def-cat}
\end{figure}

Figure~\ref{fig:freq-def-cat} graphs the number of definitions for each
macro name in our test suite, broken down by macro category.  When all
definitions of a name fall into the same category, the name is assigned to
that category; otherwise, the definitions are inconsistent and the name is
assigned to ``failed classification.''  See Section~\ref{sec:inconsistent}
for more details of macro name classification.

Our analysis makes no distinction between sequential redefinitions of a
macro and multiple definitions that cannot take effect in a single
configuration.  Independent definitions may result from definitions in
different branches of a Cpp conditional, from intervening {\tt \#undef}
directives, from compilation conventions as when compiling different
programs in a package or versions of a program.  In general, distinguishing
the cases is undecidable.

The most frequently redefined macros in Figure~\ref{fig:freq-def-cat} are
unclassified, non C code, and syntactic.  The more definitions a macro has,
the more likely it is that one of those definitions cannot be classified,
or is misclassified, by our system, resulting in a failure to classify the
macro name.

Macro bodies which are not C code tends to be Makefile compilation
commands, and library filenames.  These definitions differ for each
operating system, and often a package supports many such systems,
necessitating many macro definitions.  

Syntactic macros include those expanding only to punctuation.  These are
frequently used to support variant declaration styles; as such, they
require a definition for each variety, and they are frequently redefined to
ensure that their settings are correct.

The least frequently redefined macros are those classified as unknown
symbol.  If any definition is not so classified, then neither is the
macro name, and we included enough library header files to include some
definition of most common macros.

Half of all packages have no macros defined more than 14 times, and the
overall redefinition behavior of most packages approximates the mean line
of Figure~\ref{fig:freq-def-cat}.  Notable exceptions are \pkg{bc},
\pkg{remind}, and \pkg{gcc}.  \pkg{bc} is very sparing with multiple
definitions: every macro is defined either one or two times.  By contrast, 
\pkg{remind} defines 10\% of its macros more than 10 times (but none more
than 15).  It supports ten different human languages (and various character
sets) by using macros for all user output strings.
The tail of \pkg{gcc}'s graph is longest of all:  over 4\% of macros are
defined more than 20 times, and 0.5\% are defined at least 50 times.
Many of these most heavily-defined macros (including the most
frequently-defined, \verb|CPP_PREDEFINES|, with  181 definitions and 198
undefinitions), are not C code.



        

\subsection{Multiple distinct definitions}

Section~\ref{sec:mult-def} counted the number of definitions of a given
macro name, providing an upper bound of sorts on the difficulty of
understanding uses the macro.  Multiple definitions are less worrisome if
their bodies are similar, and present few worries if identical, though it
is possible that a macro, free variable, or other quantity used in the
definition has changed its value between the locations.

\begin{figure}
  \centerline{\epsfig{file=fig/cat-ddf-frequency.eps,height=7.2in}}
  \captionsmall{Number of syntactically distinct definitions per Cpp identifier,
    laid out as Figure~\ref{fig:freq-def-cat}. [[ I think this figure
    should get skipped; just use figure 11, instead, and drop the uses
    from that figure --gjb ]]}
  \label{fig:freq-ddf-cat}
\end{figure}
        
Figure~\ref{fig:freq-ddf-cat} repeats Figure~\ref{fig:freq-def-cat}, but
reporting the number of syntactically distinct definitions rather than the
total number.  The comparison is less strict than that used by CPP when
determining whether to issue a warning about redefinition.  We eliminate
all comments and whitespace, canonically rename all formal arguments, and
consider all character and string literals identical.  Thus, it is a lower
bound on the number of definitions with distinct abstract syntax trees,
much as the previous chart was an upper bound.

The number of redefinitions reported in this chart is dramatically lower
than that of the previous one.  Syntactic macros are an outlier on the
other side of the mean (most of the multiple definitions are one of just a
few alternatives).  Most macros in \pkg{remind} are identified as
syntactically identical\,---\,usually, only string contents differed.  This
chart revives hope that even multiply-defined macros may be easy to
understand.


\subsection{Inconsistent definitions}
\label{sec:inconsistent}

This section further refines our analysis of multiply-defined macros by
considering, rather than syntactic structure, the categorization of the
macro bodies described in Section~\ref{sec:categorization-details}.  This
analysis identifies higher-level commonalities among the macro definitions
that an analysis may be able to take advantage of in different ways than
more exact syntactic similarity.  We focus on definitions of a particular
name which are given different, incompatible categorizations.

In 93\% of cases, multiple definitions of a macro are compatible (more
often than not, identical).  Incompatibilities usually indicate bugs or
inconsistent usage, or failures in our categorization technique.  We
identified a number of places that code should be changed for robustness
and to avoid problems related to potentially incomplete constructs.



A macro name is categorized by merging its definitions pairwise.  (The
rules for merging categories appear in Appendix~\ref{app:category-lub}.)
The legend of Figure~\ref{fig:freq-def-cat} and several subsequent figures
reports the category breakdown by macro name.  It differs from the
by-definition breakdown of Figure~\ref{fig:categorization} in several ways.
The number of null definitions is lower, as null definitions are often
found in conjunction with other types of definition rather than being the
only variety of definition for a particular symbol (which happens for
macros used only in Cpp conditionals, and only check for definedness
there).  The number of statements dropped, largely due to participation of
statements in incompatibly-defined macros which; likewise for non C code,
which turned into failures due to heuristic misclassifications.  The number
of unknown symbols rose because such macros tend to have very few
definitions\,---\,there are almost as many macro names as macro definitions
classified as unknown symbol\,---\,so are more prominent in a breakdown by
name than by definition.  The number of failures increased because it
includes any macro with a failing definition as well as any with
incompatible definitions.


\begin{figure}
  {\small\centerline{\input{tbl-subset-categories.tex}}}
  
  \captionsmall{Macro name categorization breakdown.  For each macro with more
    than one definition, the categorization of its definitions are
    indicated.  For instance, for 23\% of multiply-defined macro names, all
    definitions fall into the expression category, and for 7.1\% of macro
    names, all definitions are either expressions or constants.  Rows less
    than one twentieth of one percent, representing fewer than ten macro
    names, are omitted.  The arrows indicate macros for which the names
    would be classified as ``failure'' (7.1\% of all multiply-defined macro
    names; overall, 2.4\% of all macro names are so classified).}

  \label{fig:subset-categories}
\end{figure}

Figure~\ref{fig:subset-categories} gives more detailed information for the
21\% of macro names which have multiple definitions.  Macros are grouped by
whether all of their definitions fall into the same subset of the
categories, as well as by whether the macro itself is classified as a
failure.

The arrows in the chart indicate which macro names have failing
classifications.  Using 10 rather than 28 categories makes this table
manageable, but does hide some information.  For instance, 
there are two ``expression + statement'' groups each making up
1.1\% of multiply defined macros.  The slightly more prevalent one includes
expressions and semicolonless statements, and those macro names are
classified as semicolonless statements.  The second group has definitions
of both these types, plus some complete statements.  (There is similar
duplication, along with null define, at 0.26\% and 0.10\%.)

Likewise, the ``statement'' row at 0.35\% is a failure because it includes
both semicolonless and full statements, which are not interchangeable.  An
example is
\begin{verbatim}
    #define TARGET_VERSION fprintf (stderr, " (i860, BSD)")
    #define TARGET_VERSION fprintf (stderr, " (i860 OSF/1AD)");
\end{verbatim}
where the former definition is a bug (most, but not all, other definitions
of the macro are full statements).




\section{Uses}

This section discusses uses of macro names within the source code.  A
use of a macro name is one of three things: an expansion in the source,
an expansion in another macro definition, or a test for definedness in a
conditional compilation directive.  We first consider how frequently
macros are used by various packages.  We then study in what contexts
macro names are used, and whether they are used consistently for solely
their expansion or definedness, or if they are used in both ways.
Finally, we consider how the number of uses of a macro name varies with
respect to the categories for definitions described in the preceding
section.

The packages we analyzed had widely varied macro usage---the most
aggressive package, \pkg{perl}, used macros almost ten times more often
than the least aggressive, \pkg{ghostview}.  Overall, half of the macro
names defined by the package are used two or fewer times.  However, many
macros are used very frequently.  Macros categorized as syntactic and
type-related tend to be expanded nearly ten times as often as the
simpler macros defining constants or expressions.  Most macros are used
in the same context consistently---less than 4\% of macro names are both
tested for definedness and expanded in the source code.

\subsection{Macro usage varies widely}

Figure~\ref{fig:use-per-line} illustrates how frequently each package
uses macros.  For packages such as \pkg{perl} which use macros most
aggressively, there are about two uses every three lines;  but for
packages with the least use, 14 of 15 lines use no macro names.
The four packages that are use macros the most
(\pkg{perl}, \pkg{gcc}, \pkg{gs}, and \pkg{python}) are the
only four language implementations of the packages we study.

\begin{figure}
\centerline{\epsfig{file=fig/uses-per-line.eps,height=5in}}
\captionsmall{Per package, the number of macro uses divided by the number of
  NCNB lines.  The packages are ordered from most preprocessor
  directives per line to fewest (as in
  Figure~\ref{fig:directives-breakdown}).}
\label{fig:use-per-line}
\end{figure}

Macro usage also varies widely relative to the kind of macro definition,
as categorized in the preceding section.  Figure~\ref{fig:freq-use-cat}
reports on the frequency of use of the various kinds of macros.  It is
structured like Figures~\ref{fig:freq-def-cat}
and~\ref{fig:freq-ddf-cat}. A higher line indicates less frequent use.
Notice that half of the macros that are defined inside a package are
used only twice or fewer.  In fact, 12\% are never used at all!  One
significant reason for unused macros is code that is either no longer
supported or incomplete but still included with the package;
\pkg{gnuplot}, which does not use 24\% of the macros it defines,
includes some partially supported terminal types which account for the
large number of unused macros.

One of the reasons to be concerned about excessive use of macros is that
a macro expansion can confuse a tool analyzing the unprocessed source
code.  Each expansion can hide essential constructs from such a tool.
Not all macro expansions necessarily cause substantial difficulty: a
macro defined as a constant or an expression would likely not hinder the
extraction capabilities of a tool at all, while a syntactic or
type-related macro may cause a mis-parse at each use.  Unfortunately for
tools operating on unprocessed source code, macros in these categories which are more
difficult to handle are use much more frequently
than simpler macros.  While 58\% of macros defining constants are
used 2 or fewer times, the same fraction of syntactic macros are used up
to 40 times!  (One notable outlier is \texttt{NULL} which, though usually
defined simply, is sometimes used very heavily---4233 times in the
62,137 lines of \pkg{python} source). Type-related macros are also used
very often---over 10\% are used more than 80 times.  Often, such macros
are used idiomatically, and pervasively (e.g., at every variable
declaration), thus accounting for their substantial use, and the long
tail of the distribution.

%        All packages follow approximately the same curve.  The outlier is
%          \pkg{gnuplot}, which uses macros less, on average, than other packages
%          do.  Over 40\% of the macros it defines are never used at all --
%          say why.  (Largely due to unsupported terminals like tgif; that
%          appears elsewhere in this document, so reference it.)

%          Another reason for unused macros might be uses in Makefiles and
%          other non-C-code files that we don't examine.

% \section{Macro use}

% \begin{figure}
% % {\small
% %   \setlength{\tabcolsep}{.25em}
% %   \centerline{\input{freq_of_use-tbl.tex}}%
% % }
% \centerline{\epsfig{file=fig/use-frequency.eps,height=7.5in}}
% \captionsmall{Number of expansions per Cpp macro.  The numbers in the
%   table represent the percentage of identifiers which are expanded a given
%   number of times or fewer.  For example, \pkg{g77} expands 65\% of its
%   macros two or fewer times.}
% \label{fig:freq-use}
% \end{figure}

\begin{figure}
\centerline{\epsfig{file=fig/cat-use-frequency.eps,height=7.5in}}
\captionsmall{Number of expansions per Cpp macro.  The numbers in the
  table represent the percentage of identifiers which are expanded a given
  number of times or fewer.  For example, 50\% of all
  macros are expanded two or fewer times. Higher lines indicate less usage.}
\label{fig:freq-use-cat}
\end{figure}

%[[Double-check all these numbers!]]
%The tail of this distribution is quite long, indicating that some macros
%are used very heavily.  Ninety-nine percent of macros are expanded 147 or fewer
%times, 99.5\% of macros are expanded 273 or fewer times, 99.9\% are
%expanded 882 or fewer times, and \pkg{python} uses {\tt NULL} (which \pkg{python}
%itself defines) 4233 times.  Figure~\ref{fig:freq-use-cat} weights each macro
%equally rather than weighting each macro use equally, which would weight
%\pkg{python}'s {\tt NULL} 4233 times more heavily than a macro used only once
%and infinitely more than a macro never used at all).

\begin{figure}
  {\small\centerline{\input{tbl-summarize-frequency.tex}}}
  
  \captionsmall{Summary of
    Figures~\ref{fig:freq-def-cat},~\ref{fig:freq-ddf-cat},
    and~\ref{fig:freq-use-cat}.  The table is by macro name.  Add percentages?
    [[The 1.0 distinct defs for unknown symbol is surprising; it indicates
    that it was the *same* unknown symbol in each definition (doesn't
    it?).  But a name is only in the category if none of its defs was
    recognized.  Double-check this!]]}
  \label{fig:freq-sum-cat}
\end{figure}

[[ Not sure what to say about Figure~\ref{fig:freq-sum-cat}. --mernst]]
[[ I'd remove this figure, or use it without the uses column instead of
the distinct definitions figure. --gjb]]

\subsection{Macro usage in conditional control}

\begin{figure}
\centerline{\epsfig{file=fig/ccd-categories.eps,height=7.5in}}
\captionsmall{Categories for conditional compilation directives.}
\label{fig:ccd-categories}
\end{figure}

Conditionals are used to control inclusion of code for many different
reasons.  We categorized the uses of conditionals by the name and
context, and looked at how frequently conditionals are used for various
purposes.


The following categories are based on macro names:

\begin{description}
%% See ccd_lexical_category in em_analyze for the routines
%% which implements these heuristics

\item[Package-specific] 
  These symbols are specific to the given package.  They do not fit any of
  the other categories.

\item[Portability, machine]
  These symbols name the operating system or machine
  hardware (e.g., \texttt{sun386} or \texttt{MACINTOSH}).
      
\item[Portability, feature] These symbols describe specific parameters
      or capabilities of the target machine or operating system (e.g.,
      \texttt{BYTEORDER}, \verb|BROKEN_TIOCGWINSZ|).  
      
%      These symbols are different from ``portability, machine'' because
%      they may correspond to multiple machines or architectures.

\item[Portability, system macro]
  These symbols are commonly defined constants or
  pseudo-inline functions in system or language libraries (e.g.,
  \verb|O_CREATE|, \texttt{isalnum}, or \verb|S_IRWXUSR|).

\item[Portability, language or library]
  These symbols are predefined by a compiler, defined by a standard
  library, or defined by the package as part of the build
  process to indicate existence of compiler, language, or library features
  (e.g., \texttt{GNUC}, \texttt{STDC}, or \verb|HAS_BOOL|).

\item[Miscellaneous system]
  These symbols are reserved (i.e., they begin with two underscores) and do
  not fit any other category.
      
\item[Debugging]
  These symbols control inclusion of debugging or tracing code.  The macro
  names include \texttt{DEBUG} or \texttt{TRACE} (or both).
      
\item[Multiple inclusion prevention]
  These guards encompass an entire file to ensure that the enclosed code is
  seen only once per translation unit by the compiler.  Such guards are
  indicated by convention with a trailing \verb|_H| or \verb|_INCLUDED| in the macro name
  they check.
\end{description}


The below three categories consider the entire guard or the context of
the directive.  These categories have precedence over the above name-based
heuristics:

\begin{description}
\item[Commenting] These guards either definitely succeed and
  have no effect as written (e.g., \texttt{\#ifdef 1}), or definitely fail
  and unconditionally skip a block (e.g., {\tt \#ifdef (0 \&\&
    \verb|OTHER_TEST|)}).  These guards are used to comment out code or to
  override other conditions (e.g., to unconditionally enable a previously
  experimental feature).
      
\item[Redefinition suppression] These guards test non-definedness of
  symbol, and control only  a definition of the same symbol, thus avoid preprocessor
      warnings about a redefinition of a name (e.g., \texttt{\#ifndef
      FOO} followed by \texttt{\#define FOO ...} and \texttt{\#endif}).
    
    The purpose is to provide a default value used unless another part of
    the system, or the compilation command, specifies another value.

\item[Mixed categories] These guards test multiple symbols
      which independently fall into different categories (e.g.,
      {\tt \#if defined(\verb|STDIO_H|) || \verb|SYSV_SIGNALS|}).

\end{description}



Figure~\ref{fig:ccd-categories} shows how each package uses macros in
conditional compilation directives.  There is significant variation
among packages, but overall portability is a major use of conditional
compilation directives.  Redefinition warning suppression, at 16.5\%, is
surprisingly high, and is essentially a macro definition mechanism, not
a conditional inclusion technique.  Encouragingly, mixed categories were
relatively rare; these correspond to conditional inclusions whose
meaning and intent is especially unclear.  Additionally, the low mixed
categories suggests that the conventions for macro names are fairly
standardized.

[[ The macros with mixed usage (especially here, but also in the
previous section) are akin to Krone \& Snelting's anomalous macros that
interfere in the lattice.  But we found more than they seemed to!  Did
we analyze the package in which they found their problem?  I'd like to
somehow be able to compare. --mernst]]
      

\subsection{Inconsistent usage}

Macros have two general purposes: they can control the inclusion of
lines of code (by appearing in a \texttt{\#if} condition that controls
that line) or can change the text of a line (by being expanded on that
line).  Each of these uses can correspond to language features---normal
\texttt{if} clauses or (for certain types of substitution) {\tt const}s
and {\tt inline}s.  But when a macro is used in both ways, something
more subtle is occurring, and there is
no easy mapping to a language feature.  It is harder to understand
and analyze a macro that is used in both ways.

We split macro uses into three categories:
\begin{itemize}
\item uses in C code, where the macro's expansion controls textual
      replacement; 
\item uses in \texttt{\#if}, \texttt{\#ifdef}, \texttt{\#ifndef},
      \texttt{\#elif} conditions;\footnote{We discarded uses in Cpp
        conditionals whose only purpose was to prevent redefinition.
        More specifically, if the condition tested only definedness, the
        next line defined the macro just tested, and the line after that
        ended the CPP conditional.  This is a bit overrestrictive, but
        conservative, and in practice quite accurate.} and
\item uses in the body of a macro definition.\footnote{These eventually
        bottoms out to one of the other contexts if that definition is
        ever used.  Uses in a macro body generally are intended to be
        used however the containing macro is.  Since those uses also
        appear in the figure, we did not attempt to track each such use
        in a body to some macro's appearance in either code or a
        conditional.}
\end{itemize}

%        A definition isn't a use of the macro being defined, only of those
%        in the body.  Since those are uses, 12\% is a lower bound on those
%        that never affect the code. It would be reasonable to assign the
%        5.4\% that are macro only, to the other categories on a pro rata basis.
%
%        Thus, the interesting categories are "code",
%        "conditional", "code and conditional", and "no use".

\noindent Figure~\ref{fig:where-used} reports on the ways that macro
names are used.  It shows that macros are expanded ten times more often
than they are used to control source code inclusion.  (However, each
inclusion use can control many lines of code; see
Section~\ref{sec:dependence}.)  The potentially subtle uses are those
for which a macro appears both in code and in a conditional.

Our data shows that packages use macros either to direct conditional
compilation or to produce code, but not for both purposes. This
separation of concerns makes the source code easier to understand.  Only
3.4\% of macros both expand in code and are used in conditional
contexts.\footnote{Plus a similar percentage of those used in other
  macro definition bodies, probably bringing the total up only another
  fraction of a percent to around 4\%.}  

%Conditional usage is rare in
%general; conditional compilation accounts for half of Cpp directives but
%only 7.1\% of macro usage (plus the categories just listed above).  But
%each use in a conditional can have a large effect on the system.


\begin{figure}
\centerline{\small
  \setlength{\tabcolsep}{.25em}
  \input{tbl-where-used.tex}%
}
\captionsmall{Where macros are used: in C code, in macro definition bodies, in
  conditional tests, or in some combination thereof.  The numbers don't sum to
  100\% because of rounding.}
\label{fig:where-used}
\end{figure}



%[[Move this to the previous section, and reference back to it.]]
%      A surprising number -- nearly 12\% -- of macros defined in a package
%        are never used at all.  Occasionally [[find a concrete example of
%        this]] this is a result of shipping a 
%        standard set of headers with the package -- it's like a library for
%        that development team, but one that can't be counted upon to exist
%        everywhere, so it has to be provided.  For gnuplot, over 40\% of
%        macros are never used because the package's support for several
%        terminal types, such as tgif, is unfinished (and thus unused).
%        Even discounting that package, though, the numbers are remarkably
%        high.  We would be surprised if one in eight functions and
%        variables in a package were never used, not even in testing code.
%        [[Do we have any idea what fraction this is in practice?
%        Ask Dave Grove; he can compute this relatively easily.]]
%        (The percentage of macros defined in libraries/standard header
%        files which are never used in the code is enormous, but that is
%        expected.)

%      Across packages, there is heavy variation.  Packages which use
%        the preprocessor sparingly are as likely to have a high percentage
%        of mixed usage as packages which make heavy use of CPP.  (There is
%        a slight tendency for the less aggressive packages (i.e.,
%        those lower on the lists in Figures~\ref{fig:directives-breakdown}
%        and~\ref{fig:categorization}) to have more uses in code, fewer uses
%        in conditionals, and fewer macros that are never used.)


\subsection{Number of arguments}

\begin{figure}
\centerline{\epsfig{file=fig/cat-numargs.eps,height=4in}}
\captionsmall{Is this worth including?
  [[PROBLEM:  the numbers in this legend don't accord with those in
  previous legends by name.  This is because the numargs information was
  gleaned from the .catg file which contains only the macro bodies and
  the filename they were found in, thus there is no way to include macro
  bodies defined outside the package of macro names that are defined
  somewhere inside the package as is the case almost everywhere else;
  I'm now thinking that this information could've/should've been used
  for aiding the categorizations: i.e., a null define taking no
  arguments is a lot different than one taking arguments;  since that
  wasn't done, I'm inclined to drop this since to avoid the
  inconsistency in the numbers (and because it is of questionable
  utility as presented now --gjb]]}
\label{fig:cat-numargs}
\end{figure}


You might wonder whether macros are used like functions (taking arguments)
or like constants (taking no arguments).  
[[However, a fair number of statement macros also take no arguments.]]
We graphed that in
Figure~\ref{fig:cat-numargs}.  This seems irrelevant to me; I don't see
where to fit it in, or what to say about it.



\section{Dependences}
\label{sec:dependence}
\label{sec:last-content-section}

Macros control the program which results from running Cpp in two distinct
ways.  {\em Inclusion} dependence results from Cpp conditionals testing
macros (for definedness, or by examining expansions) to determine which
which lines of the Cpp input appear in the output.  {\em Expansion}
dependence results from replacement of macros outside Cpp conditionals by
their definition bodies, which controls the content of the lines on which
the macros appear.  This section reports the incidence of these
dependences, both by macro and by line.

We report both direct and indirect dependences.  A line directly depends
upon macros that appear in the line or in a {\tt \#if} condition whose
scope contains the line.  It also has indirect dependences on macros that
control the definitions of directly controlling macros.  For instance,
after {\tt \#define \verb|S_ISBLK|(m) ((m)~\&~\verb|S_IFBLK|)}, the final
text of a line that uses \verb|S_ISBLK| depends not just on its definition
but also on that of \verb|S_IFBLK|.  An indirect dependence is an expansion
dependence if every dependence in the chain is an expansion dependence;
otherwise, the indirect dependence is an inclusion dependence.

We did not distinguish must from may dependences.  When a macro was defined
on both branches of a {\tt \#if} conditional, the macro's definedness was
not deemed to depend on the values tested in the conditional, though its
value was.  We tracked dependences across file boundaries: if a macro
controls whether a file is {\tt \#include}d, then the macro also controls
every line of that file.

%% This makes absolutely no sense.  What is going on here?
%% I think there's something meaningful about lopping off prefixes of must
%% dependences, but can't puzzle it out now.  -MDE 11/1/97
% When different conditions control different definitions of a macro, uses
% are dependent on the independent parts of those conditions.  For instance,
% after
% \begin{verbatim}
%   #if A
%     #if B1
%       #define M ...
%     #elsif B2
%       #define M ...
%     #else
%       #define M ...
%     #endif
%   #endif
% \end{verbatim}
% a use of macro {\tt M} is expansion-dependent on {\tt B1} and {\tt B2}, but
% not on {\tt A}, which must have been set to true in order for {\tt M} to be
% defined at all.  That is, no setting of {\tt A} can affect {\tt M}'s

The statistics reported in this section are underestimates because they
omit two packages which aggressively use of macros.  The full dependence
information for \pkg{emacs} and \pkg{mosaic} exceeded our computer's
virtual memory.  These packages' excessive dependences result in part from
use of Motif, a complex external library.  We did generate dependence
information for \pkg{plan}, which is the smallest of the three packages
which use the Motif library.


\subsection{Dependences by line}

\begin{figure}
\centerline{\epsfig{file=fig/dep-byline.eps,height=4in}}

\captionsmall{Percentage of lines dependent on a particular number of macros (or
  fewer).  For instance, 93\% of all lines are expansion-dependent on two
  or fewer macros, and 90\% of all lines are inclusion-dependent on 27 or
  fewer macros.  The log scale is shifted by unity in order to place 0
  on the scale:  before plotting, we added 1 to each of the x axis values,
  then relabeled ``1'' as ``0''.}
\label{fig:dep-byline}
\end{figure}

Figure~\ref{fig:dep-byline} graphs the percentage of lines dependent on a
given number of macros.  Over two in five lines (42\%) are controlled by
macros.  Most of these (38\% of all lines) are inclusion-controlled by at
least one macro; some of these lines, such as those in header files, appear
unconditionally but are inside a guard to avoid multiple inclusion.  Over
one in four lines (28\%) expands a macro, a higher value than we anticipated.

Expansion dependence on multiple macros is not prevalent\,---\,only 5\% of
lines are expansion-dependent on more than 3 macros, and only 1\% are
expansion-dependent on more than 7 macros.  However, one line of
\pkg{gcc}\,---\,{\tt \verb|LEGITIMIZE_ADDRESS| (x, oldx, mode,
win);}\,---\,is expansion-dependent on 187 different macros.  Macro
\verb|LEGITIMIZE_ADDRESS| is defined 30 times in \pkg{gcc}, many of the
definitions dozens of lines long and themselves studded with macro
invocations.  Overall, only 13 out of 325,000 lines in \pkg{gcc} are
expansion-dependent on more than 100 macros.

Inclusion dependences have a much wider distribution.  One in twenty lines
is inclusion-dependent on at least 133 macros, and 1\% of lines are
dependent on over 300 macros.  As an example, the line of \pkg{gcc}
mentioned above has 182 inclusion dependences (with only 13 macros in
common with its set of expansion dependences), but over over 10,000 lines
of \pkg{gcc} have even heavier inclusion dependences than that.

On average, each line in the {\numpackageslesstwo} packages tested is
expansion-dependent on .04 macros, inclusion-dependent on .31 macros, and
has both varieties of dependence on .01 macros, and has some dependence on
.34 macros.
      

\subsection{Dependences by macro}

\begin{figure}
% This works, but the figures are upside-down.
% \centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=90,height=3.75in}}
% \centerline{\epsfig{file=fig/incl-dep-bymacro.eps,angle=90,height=3.75in}}
% Can't use ``height'' when rotating by -90 or +270; I don't know why.
% \centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=270,width=.5\linewidth}}
% \bigskip
% \centerline{\epsfig{file=fig/incl-dep-bymacro.eps,angle=270,width=.5\linewidth}}
\centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=270,width=.49\linewidth}%
~%
\epsfig{file=fig/incl-dep-bymacro.eps,angle=270,width=.49\linewidth}}
\captionsmall{Dependences by macro name for 22648 macro names in 28 packages.
  Each bar represents all macros that control at least as many as the
  labeled percent of the lines in its package (but fewer than the next
  bar).  For instance, the .17 bar in the expansion dependence chart
  indicates that 286 macros each control between .17\% and .26\% of the
  entire package which contains that macro.  The maximum falls in the last
  bucket specified (i.e., the first bucket off the chart is the first empty
  one).  The ``Epsilon'' bar represents a small non-zero value, so that
  macros not controlling any lines are not conflated with macros
  controlling very few lines.  A log scale is used for the x axis labels.}

%% expansion is red; inclusion is blue
\label{fig:dep-bymacro}
\end{figure}

Figure~\ref{fig:dep-bymacro} graphs how many lines are dependent on each
macro (Figure~\ref{fig:dep-byline} gave the same information by line rather
than by macro).  Since the {\numpackageslesstwo} packages vary in size, the
graphs of Figure~\ref{fig:dep-bymacro} aggregate them by reporting
percentages of a package rather than absolute numbers.


The expansion dependence chart closely approximates an exponential decay.
Most macros control few lines, a few macros control many lines, and the
transition between the two varieties is gradual.  Most {\tt \#if}
directives (which account for about 2.2\% of all lines) expand at least one
macro; the rare exceptions include such tests as which character set is
being used on the compiling machine.

Of the ten macros that are expanded by more than 5\% of lines in a package,
six are types ({\tt int} and {\tt rtx} in \pkg{gcc}, {\tt ANY} and {\tt
object} in \pkg{python}, {\tt SvANY} in \pkg{perl}, and {\tt const} in
RCS), one is a constant ({\tt NULL} in python), and three are expressions
({\tt ip} in \pkg{workman} and {\tt ArgCount} and {\tt Args} in
\pkg{xfig}).  Three of these ({\tt int}, {\tt const}, and {\tt NULL})
redefine built-in C keywords or values, and those macros are not
necessarily active on every line containing the symbol.  Likewise, {\tt ip}
is an ordinary variable in most of its uses, though our anlysis did not
discover that fact.

%         Outliers (> 6\% of all lines expand):
%           int in gcc (14.85\% !)
%           NULL, ANY, object in python
% 
%         Above 5\%: 
%           SvANY in Perl
%           const in RCS
%           ip in workman -- bogus, as defined just twice, then undefined;
%                   most places it is a formal parameter and out of the scope
%                   of the macro definition.
%           ArgCount, Args in xfig (like argc, argv)
%           rtx in gcc (defined to int or int*)
% 
%         Overall, for these top 10:  6 types, 1 constant, 3 expressions



The inclusion dependence graph is bimodal.  While most macros control
inclusion of zero or few lines, quite a few control very substantial
fractions (10\% or so) of the package, and there are not a lot of macros in
between.  The graphs for the individual packages exhibit far higher peaks
than the aggregate inclusion dependence graph of
Figure~\ref{fig:dep-bymacro}; summing the graphs tended to average them.
The heaviest dependences are on header files (for instance, \verb|H_PERL|
controls inclusion of over 53\% of \pkg{perl}'s lines).

%        [[It would have been interesting to run these numbers for everything
%          but exclude file multiple inclusion prevention macros.]]


\subsection{Cppp}

Support for multiple dialects of a language is a particularly common use of
the preprocessor, one which leads to unstructured macros (partial
declarations and other difficult-to-handle constructs) and which can be
performed only by the preprocessor, not in the language.  We performed an
experiment to determine whether eliminating these macros would lead to
substantially simpler uses of macros with fewer dependences, failed
classifications of macro bodies, and so forth.

We built a Cpp partial evaluator called Cppp.  Given Cpp-style command-line
arguments specifying which macros are known to be defined or undefined
(and, optionally, their expansions), it discharges Cpp conditionals that
depend on those macros.  It does not expand macros inline or use
definitions found in its input files.

% (It does not eliminate or expand other macros with only one
% remaining definition, because other definitions may appear in libraries or
% on the command line when the package is compiled.)

We defined all the macros that can be depended on if using ANSI standard C
or C++ (including prototypes and booleans) with POSIX-compliant libraries,
preprocessed all the source (and all library header files), and reran all
of our other experiments.  The results were little changed from the full
versions of the packages (which generally supported both K\&R and ANSI C,
and sometimes other dialects as well): the number of multiple definitions
of macros, of failed classifications, and of dependences on macros did not
decline substantially.  We conclude that macro usage in our test programs
presents no obvious single point of attack:  even eliminating one prevalent
use did not eliminate the complexity introduced by preprocessor.


\section{Related work}
\label{sec:related}

%Split this into:
% * taxonomies
% * checking tools
% * understanding tools such as Emacs hideif mode
% * other?

We could find no other empirical study of the use of the C preprocessor
nor any other macro processor.  However, we did find some guidance on
using C macros effectively, tools for checking macro usage, and
techniques for understanding and exploring C source code which uses the
preprocessor.

A number of organizations provide hints about effective ways to the use
the C preprocessor.  The GNU C preprocessor manual~\cite{cpp-manual}
discusses a set of techniques including simple macros, argument macros,
predefined macros, stringization macros, concatenation macros, and
undefining and redefining macros.  It also identifies a set of
``pitfalls and subtleties of macros''; these are much like some of the
problems our analysis tool identifies.  Several coding style guides
discuss the preprocessor and make recommendations on its acceptable uses
and on ways to reduce unexpected behavior resulting from poorly
designed constructs~\cite{Stallman97,ellemtel92,Cannon95,Dolenc90}.

Carroll and Ellis state that ``almost all uses of macros can be
eliminated from C++ libraries''~[p.~146]{Carroll95}.  They list eight
categories of macro usage and explain how the software engineer can use
C++ language features instead of using the preprocessor.  Our categories
differ from theirs---we focus on actual use in C programs, whereas they
concentrate on potential uses in C++ programs.

Spencer and Collyer recommend against using most varieties of
\texttt{\#ifdef}~\cite{SpencerC92}.  As we do, they accept the special
cases of providing default values for macros and preventing
multiple-inclusion of header files as essential uses of the
preprocessor.  Spencer and Collyer also suggest testing for specific
features instead of machines in conditional compilation directives; we
use this preference in our categorization of those directives.

Krone and Snelting use mathematical concept analysis to determine the
conditional compilation structure of code~\cite{Krone94}.  They determine,
for each line, which preprocessor macros it depends upon, and display that
information in a lattice.  They do not determine how macros depend upon one
another directly, only by their nesting in {\tt \#if}, and the information
conveyed is about the program as a whole.  

%[[Compare to our CCD work, which
%also tries to find such mismatches; and talk a bit more about what they
%discovered and how it compares to what we discovered.]]

%Greg:  describe LCLint methodology and results.  Say exactly what you did
%(i.e., how hard you tried), and what the results were.  Also, that you tried
%on only 20, not all 30, packages, which doesn't include the biggest ones.

A number of tools check whether specific C programs satisfy particular
constraints.  Various ``lint'' source-code analyzers checks for
potentially problematic uses of C, often including the C preprocessor.
LCLint also allows the programmer to add annotations which enable more
sophisticated checks than many other lint programs~\cite{Evans-fse94}.
LCLint optionally checks function-like macros\,---\,that is, those which
take arguments\,---\,for macro arguments on the left hand side of
assignments, for statements playing the role of expressions, and for
consistent return types.  LCLint's approach is prescriptive: programmers
are encouraged not to use constructs that might be dangerous, or to
change code that contains such constructs.  Our macro lint work notices
a similar but broader set of subtle or potentially dangerous constructs.

%LCLint considers assignment to a macro argument dangerous but does not
%appear to check for assignments to local variables.~\cite[\S
%8]{Evans:LCLint} [[Should we mention these things?  I don't want to seem
%nitpicky or petty, so if we mention this, mention them as differences,
%not as things LCLint does wrong.
%\begin{quote}
%$\ldots$ a parameter to a macro may not be used as the left hand side
%of an assignment expression $\ldots$, a macro definition must be
%syntactically equivalent to a statement $\ldots$ when it is invoked followed by
%a semicolon $\ldots$, the type of the macro body must match the return
%type of the corresponding function $\ldots$~\cite[\S 8]{Evans:LCLint}
%\end{quote}
%That quotation is really easy to nitpick:
%\begin{enumerate}
% \item assignment parameters is fine (just turn into a reference argument) but
%    assignment of non-parameters that aren't at global scope is quite bad.
% \item in x=foo(); we do NOT want foo() to be a statement
% \item a macro doesn't have a single type, but may have many polymorphic types
%\end{enumerate}
%]]


%% FIX: If we could also list the platforms for which each can compile,
%% that would be great, but I doubt the benefit is worth the effort for now

A limited number of tools do exist to assist software engineers to
understand code with containing Cpp directives.  Emacs provides
\texttt{hide-ifdef-mode} which enables the programmer to specify
preprocessor variables as explicitly defined or not defined; the mode
then presents a view of the source code corresponding to that
configuration, hiding code that is conditionally unincluded.  Various
language construct ``tagging'' mechanisms (e.g., \texttt{etags},
\texttt{ctags}) recognize macro definitions and permit tag-aware editors
to move easily from a macro expansion to the various definitions of
that macro name.

%[[ What debugger does this? --11/21/97 gjb]]
% [[ I don't remember, and couldn't find a reference.  -MDE ]]
% Debuggers exist that can call {\tt \#define}d functions.

\section{Conclusions}
\label{sec:conclusion}

[[Grist for conclusion: These data demonstrate that multiple definitions of
symbols is not numerically frequent; even more importantly, the definitions
of a symbol tend to be compatible, as shown in
Section~\ref{sec:inconsistent}.]]


\subsection{Relevance of the results}

The results of this research are of interest to language designers, tool
writers, programmers and software engineers.

Language designers can examine how programmers use the macro system's
extra-linguistic capabilities.  Future language specifications can
directly support (or prevent!)\ such practices thus imposing greater
discipline and structure.

% [[[Think more about this:  Also, how do language choices lead to more/less
% tightly integrated (as opposed to open, component-based) environments?
% E.G., no need for \verb|__LINE__| in Java?]]]

Programming tool writers can choose to cope only with common uses of the
preprocessor.  By partial preprocessing (or embedded understanding of
some Cpp constructs), a parser can maintain the
programmer-oriented abstractions provided by preprocessor directives and
macro names while not getting confused by non-syntactic programs.

Our analyses are of interest to programmers who wish to make their code
cleaner and more portable.  By recognizing the widely-used Cpp idioms,
programmers can choose to limit their use of the preprocessor to
constructs that other tools are most likely to cope with easily.  They
can choose to avoid constructs that cause tools (such as test frameworks
and program understanding tools) to give incomplete or incorrect
results.

% Also, learn weird new Cpp tricks!

Finally, our results are of interest to software engineers for all of the
above reasons and more.  Since this is the first study of Cpp usage of which
we are aware, it is worth performing simply to determine whether the
results were predictable a priori; we did in fact discover a number of
interesting features of our suite of programs.


\subsection{Making C programs easier to understand}

The combination of C and Cpp makes a source text unnecessarily difficult
to understand.  A good first step is to eliminate Cpp uses where an
equivalent C or C++ construct exists, and to apply tools to explicate
the remaining uses.  Here we discuss a few approaches to reducing the
need for the preprocessor by better changing the state of the art in C
programming, rather than applying tools to a specific source code
artifact.  We do not seriously consider simply eliminating the
preprocessor, for it provides conveniences and functionality not present
in the base language.

Since many of the most problematic uses of Cpp provide portability across
different language dialects or different operating environments,
standardization can obviate many such uses.  Canonicalizing library
function names and calling conventions makes conditional compilation less
necessary and incidentally makes all programs more portable, even those
which have not gone to special effort to achieve portability.  This
proposal moves the responsibility for portability (really, conformance to a
specification) from the application program into the library or operating
system.  

Likewise, one of the most common uses of Cpp macros could be eliminated
if the C language and its dialects had only a single declaration syntax.
Because most C compilers, and all C++ compilers, accept ANSI-style
declarations, much support for multiple declaration style may have
outlived its usefulness.  The ``ansi2knr'' tool~\cite{Deutsch90}
translates a C program using ANSI style function declarations into one
using classical function declarations.  By ubiquitous use of this
transformation tool, we can free the primary source code artifact from
the need to maintain two commonly required configurations.  This would
dramatically reduce the use of a large class of preprocessor constructs
in packages which support both declaration styles.

Some Cpp directives, such as {\tt \#include}, can be moved into the
language proper; this would also eliminate the need for Cpp constructs that
prevent multiple inclusion of header files.  Likewise, compilers that do a
good job of constant-folding and dead code elimination can encourage
programmers to use language constructs rather than relying on the
guarantees of an extra-linguistic tool like Cpp.\footnote{Interestingly,
  the issue seems to not be whether compilers do the appropriate
  optimizations, but whether programmers have confidence that the
  optimizations will be performed; if unsure, programmers will continue to
  resort to Cpp, since certainly a compiler cannot generate code for source
  that it does not ever even see (because Cpp has already stripped it
  away).}

Common Cpp constructs could be replaced by a special-purpose syntax.  For
instance, declarations or partial declarations could be made explicit
(perhaps first-class) objects; similar support could be provided for
repetitive constructs and dynamic scoping.  Manipulations of these objects
would then be performed through a clearly-specified interface rather than
via string and token concatenation, easing the understanding burden on the
programmer.  Such uses would also be visible to the compiler and could be
checked and reasonable error messages provided.  The downside of this
approach is the introduction of a new syntax or new library functions which
may not simplify the program text and which cannot cover all cases, only a
few specified ones.

[[Add a hygienic macros reference somewhere.]]  
[[What is this? --gjb]]  [[Scheme's macro system never captures variables;
see the end of the R^n Report, for instance.  -MDE]]

An alternative approach which avoids the clumsiness of a separate
language of limited expressiveness is to make the macro language more
powerful\,---\,perhaps even using the language itself via constructs
evaluated at compile time rather than run time.  (The macro systems of
Common Lisp and Scheme, and their descendants~\cite{WeiseC93}, take this
approach.)  An extreme example is to provide a full-fledged reflection
capability.  Such an approach is highly general, powerful, and
theoretically clean; it circumvents many of the limitations of Cpp,
though it does not necessarily support all of Cpp's low-level features.
Such a syntactic approach does eliminate the problem of mis-parsing due
to macro expansion, but the added generality seems likely to degrade a
tool's ability to reason about the source code.  In practice, such
systems are used in fairly restricted ways, perhaps because other uses
would be too complicated.  A dialog among users, compiler writers, tool
writers, and language theorists is necessary when introducing a feature
in order to prevent unforeseen consequences from turning it into a
burden.


%% See slide 15 and its notes from Mike's quals talk.
\subsection{Future work}
 
These results suggest a wide variety of future avenues for research, both
in terms of expanding our understanding of uses of the preprocessor in
practice and in addressing the issues identified by this study.

We did not analyze any reusable libraries (e.g., \pkg{glibc}) for their
macro use patterns.  Comparing how Cpp is used in libraries in contrast
to application code may yield insights into the different needs of
library authors.  Other dimensions to the packages may be beneficial to
analyze independently and contrast.  [[Not a sentence:]]  For example, cross-platform
packages \vs{} Unix-only \vs{} Microsoft Windows\footnote{Windows is a
  trademark of Microsoft Corporation.} packages, or GNU-project packages
\vs{} non-GNU packages.

Similarly, we did not formally analyze any C++ source code.  Preliminary
results indicate that many C++ packages rely heavily on Cpp, even when
C++ supports a nearly identical language construct.  This unfortunate
situation probably stems from a combination of trivial translations from
C to C++ and of C programmers becoming C++ programmers without changing
their habits.  A useful analysis of C++ packages would consider the code
in the context of both the history of the package and the background of
its authors.

Further analysis of the macros with free variables is needed to see
which of the roughly 32\% of expression macros should be easy to convert
to inline functions.  In particular, we did not investigate whether the
free variables in macro definitions referred to local or global
variables.  The former achieve dynamic scoping which is impossible using
the C language proper, while the latter could likely be converted to an
inline function.

Our framework currently does not benefit from analyzing the context of
macro expansions in determining a macro's category.  For example, a
macro used where a type should appear can be inferred to expand to a
type; a macro used before a function body is probably expanding to a
declarator.  We are actively pursuing this extension.

Additionally, the framework is overly conservative with respect to
conflicting definitions.  We assume that every occurrence (except in
strings and comments) of any name that is \texttt{\#define}d is a macro
use and could correspond to any of the definitions of that same name.
This pessimism provides an upper bound on the preprocessor complexity of
the source code, but is too inaccurate when considering code
transformations.  We currently are experimenting with a different
framework that accurately tracks macro definitions and uses to provide
support for more sophisticated reasoning and code transformations.

\section*{Acknowledgment}

This work was supported by a National Science Foundation Graduate
Fellowship and an IBM Cooperative Fellowship.
    
% Any opinions, findings, conclusions, or recommendations expressed in
% this publication are those of the author, and do not necessarily
% reflect the views of the National Science Foundation.


% Not really right:  Don't want the ``References'' section head to be small.
{\small \bibliography{evil}}


\appendix

\section{Merging categories}
\label{app:category-lub}

The following rules determine how two categories are merged:
\begin{itemize}\itemsep 0pt \parskip 0pt

\item If the categories are the same, use that.

\item If one is an unknown symbol (or the name of an undefined macro), use
  the other on the theory that the unseen definitions are likely to be
  similar to the present one, which is true for well-behaved macros.

\item If one is a null define, use the other.  For instance, a type
  modifier may be present or absent.  In order to either perform an action
  or do nothing, macros not uncommonly expand to either a statement or to
  nothing (though it would be more robust to expand to a null statement in
  the latter case).  Additionally, macros used as boolean variables which
  are checked for definedness may be set via a null define or by being
  assigned a constant (generally 1).  This practice is an error if the
  macro is used outside Cpp's {\tt defined} operator, but is also frequent
  and generally innocuous.

\item If one is a constant and the other is an expression, use the latter.

\item If one is an ambiguous list of space-separated symbols and the other
  is reserved word or type, use the latter.  Sequences of symbols can often
  not be definitely identified in isolation, but the other definitions of
  the same name indicate the intended usage.

\item If one is an expression or constant and the other is a semicolonless
  statement, use the latter, for a semicolon can be added to any expression
  to make a statement.  In particular, function calls are classified as
  expressions but may be intended to be used for side effect rather than
  for value.

\item If one is statement-related, use the other if it is the corresponding
  plural form; if both are partial statements, select the more incomplete
  statement category.  (See page~\pageref{item:statement-category} for
  details.)

\item Otherwise, return failure.
\end{itemize}


\end{document}
