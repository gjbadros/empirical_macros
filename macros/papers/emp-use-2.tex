% $Id$
\documentclass[10pt]{article}
\usepackage{epsfig}
\usepackage{dcolumn}

\def\numpackages{30}
\def\numlines{2 million}
\def\numlinesexact{nearly 2 million}
\def\numlinesncnbexact{about 1.4 million}

\newcommand{\pkg}[1]{\textsf{#1}}

\newcommand{\file}[1]{\texttt{#1}}

% the "fullpage" package does almost the same thing
% as the below lines-- it doesn't make things quite as
% tall or wide, but is generally what I use
% \usepackage{fullpage}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt
\topmargin   0pt
\textwidth   6.5 in
\textheight  8.5 in

\renewcommand{\floatpagefraction}{.8} %default .5
% Avoid putting all figures at end of text.
\renewcommand{\textfraction}{.1}  % .2 is the default
\renewcommand{\topfraction}{.9}   % .7 is the default

\begin{document}
% \bibliographystyle{plain}
\bibliographystyle{alpha}

\title{An Empirical Analysis of C Preprocessor Use}

\author{Michael D. Ernst%
  \and Greg J. Badros%
  \thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.}
  \and David Notkin}

\date{% Technical Report UW-CSE-97-04-06 \\
Department of Computer Science and Engineering \\
University of Washington \\
Box 352350, Seattle, WA  98195-2350  USA \\
{\small \{{\tt mernst},{\tt gjb},{\tt notkin}\}{\tt @cs.washington.edu}} \\
5 October 1997}  

\maketitle

\begin{abstract}
  The C programming language is intimately connected to its macro
  preprocessor Cpp, which generally hinders tools built to engineer C
  programs (compilers, debuggers, call graph extractors, translators,
  etc.).  Most tools make no attempt to analyze macro usage, but simply
  preprocess their input, which has a number of negative consequences.  In
  order to determine how the preprocessor is used in practice, and the
  feasibility of automatic analysis of preprocessor use, this paper
  analyzes {\numpackages} packages comprising {\numlines} lines of publicly
  available C code.  We determine the incidence C preprocessor usage which
  is complex, potentially problematic, or inexpressible in in terms of
  other C or C++ language features.
[[We also came up with the list of things to investigate; and along the way
we taxonomized some stuff, which is also a contribution.]]

  We particularly note data that are
  material to the development of tools for C or C++, including translating
  from C to C++ to reduce preprocessor usage.  The results are of interest
  to language designers, tool writers, programmers, and software engineers.
\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is incomplete without its macro
preprocessor, Cpp~\cite[Ch.~3]{Harbison91}, which supplies such facilities
as file inclusion, definition of constants and macros, and conditional
compilation.  While disciplined use of the preprocessor can reduce
programmer effort and improve portability, performance, or readability, Cpp
is widely viewed as a source of difficulty for understanding and
transforming C programs.  Because of Cpp's lack of structure\,---\,its
inputs and outputs as raw token streams\,---\,Cpp is very flexible, but
lends itself to arbitrary source code manipulations that complicate
understanding of the program by both software engineers and tools.  In the
worst case, the preprocessor makes merely determing the program text as
difficult as determining the output of an ordinary program.  The designer
of C++, which shares C's preprocessor, also noted these problems:
``Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders.''~\cite[p.~424]{Stroustrup-DesignEvolution}

While much has been written about Cpp's potential pitfalls, no previous
work has examined actual use of the C preprocessor to determine whether it
presents a practical or merely theoretical obstacle to program
understanding, analysis, and transformation.  This paper fills that gap by
examining CPP use in {\numpackages} programs comprising {\numlines} lines
of source code.

We identified a number of potential pitfalls proceeding from preprocessor
use, including
\begin{description}
\item[high total use]  heavy use of either macro substitution or
  conditional compilation can overwhelm a human or tool, particularly for
  lines that depend on many macros or macros that control many lines
\item[complicated bodies]  a macro body need not expand to a complete
  C syntactic entities (like a statement or expression)
\item[extra-linguistic features]  a macro body may exploits features of
  the preprocessor not available in C, such as stringization, token
  pasting, or use of free variables
\item[multiple definitions]  uncertainty about the expansion of a macro
  prevents knowledge of the actual program text; even more problematically,
  two definitions of a macro may be incompatible, for instance if one is a
  statement and the other an expression or type
\item[macro pitfalls]  function-like macros may fail to swallow a following
  semicolon; macros may fail to parenthesize, or may side-effect, uses of
  formal variables; and more
\item[inconsistent usage]  a macro used both for conditional
  compilation and to expand code is harder to understand than one used just
  for one purpose or the other
\item[mixed tests]  a single conditional test may test conceptually
  distinct, unrelated conditions, making it difficult to perceive the
  intention
\item[variation in use]  if there is no clear pattern of use, or
  commonly-repeated paradigms, then no obvious point of attack presents
  itself
  %% No pattern according to package size, relative or absolute cppp use, etc.
\end{description}
We report in detail on each of these aspects of preprocessor use,
indicating which appear to be innocuous in practice (that is, the
problematic uses appear only infrequently) and which may prove problematic
for software engineers.  We also present new taxonomies of macro body
expansions, macro feature usage, macro pitfalls, and conditional
intentions.  These taxonomies improve on previous work by being more
detailed and more accurately reflecting actual use.

Sections~\ref{sec:first-content-section}--\ref{sec:first-content-section}
present the bulk of these results.  Section~\ref{sec:methodology} presents
our experimental methodology.  Section~\ref{sec:conclusion} discusses the relevance of the research,
suggests techniques for mitigating the negative impact of Cpp on program
understanding, and discusses avenues for future work, while
section~\ref{sec:related} discusses related work.
The remainder of this section [[does some stuff]].


% \subsection{Outline}
% 
% The remainder of this paper is organized as follows.
% 
% Section~\ref{sec:directives} reports the percentage of original C source
% code lines that are preprocessor directives, including a breakdown of the
% frequency of specific directives such as {\tt \#define}.  C programs
% commonly have preprocessor directives as over 10\% of their total lines,
% and over 20\% of the lines were directives in 3 of the {\numpackages}
% packages.
% 
% Section~\ref{sec:usage} reports how often each macro is defined and
% expanded.   Identifiers tend to be {\tt \#define}d relatively few times
% (96\% of macro identifiers had three or fewer definitions).  Many packages
% also have a significant number of macros that are never expanded, even
% disregarding system and library header files.
% 
% Section~\ref{sec:categorization} categorizes macro definitions according to
% their expansions; for example, macros may simply define a preprocessor
% symbol, define a literal, expand to a statement, etc.  We were particularly
% interested in determining the frequency of use of macros that are difficult
% to convert to other language features, such as those that string together
% characters as opposed to manipulating lexemes or syntactic units (less than
% one third of one percent of all macro definitions),
% those that expand to partial syntactic units such as unbalanced
% braces or partial declarations (half of one percent), and others not 
% directly expressible in the programming language (about four percent).
% 
% Section~\ref{sec:conclusion} discusses the relevance of the research,
% suggests techniques for mitigating the negative impact of Cpp on program
% understanding, and discusses avenues for future work, while
% section~\ref{sec:related} discusses related work.


[[Does anyone care about this?
Another niche already filled by our tool is that of a ``macro lint''
program which warns of potentially dangerous (or non-standard) uses of Cpp.
And, we wrote CPPP.]]



% In order to assess the practical difficulty of understanding uses of CPP
% (and the potential for replacement by other language constructs), 




[[Inane, content-free.  Must die.]]

Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of C
programming support tools.  On the other hand, the analysis also convinces
us that, by extending our analysis framework with some class type
inferencing techniques (similar to those used by Siff and Reps for C to C++
translation~\cite{Siff-fse96}, O'Callahan and Jackson for program
understanding~\cite{OCallahan-icse97}, and others), we can take significant
steps towards a tool that usefully converts a high percentage of Cpp code
into C++ language features.\footnote{Preliminary results indicate that many
  C++ packages rely heavily on Cpp, even when C++ supports a nearly
  identical language construct, probably due to a combination of trivial
  translations from C to C++ and of C programmers becoming C++ programmers
  without changing their habits.} We are interested not in translations
that merely allow a C program to be compiled by a C++ compiler (which is
usually easy, by intentional design of C++) but those that take advantage
of the added richness and benefits of C++ constructs.

In terms of the complexity of preprocessor usage, the results reported here
contain both good news and bad.  By far
the largest number of macro definitions and uses are relatively simple, of
the variety that a programmer could understand without undue effort (although
perhaps requiring tedious work) or that a relatively unsophisticated tool
could understand (although in practice very few even try).  Despite the
preponderance of innocuous macros, the preprocessor is so heavily used that
the remaining ones are numerically significant.  It is precisely these
macros that are mostly likely to cause difficulties, and there are enough
of them to be problematic in practice and to make the effort of
understanding, annotating, or eliminating them worthwhile.


\subsection{Coping with Cpp}

Tools\,---\,and, to a lesser degree, software engineers\,---\,have three
options for coping with Cpp.    They may ignore preprocessor directives
(including macro definitions) altogether, accept only post-processed code
(usually by running Cpp on their input), or attempt to emulate the
preprocessor.

Ignoring preprocessor directives is an option for approximate tools (such
as those based on lexical or approximate parsing techniques), but accurate
information about function extents, scopes, declared variables and
functions, and other aspects of a program requires addressing the
preprocessor.

Operating on post-processed code, the most common strategy, is simple to
implement, but then the tool's input differs from what the
programmer sees.  Even when line number mappings are maintained, other
information is lost in the mapping back to the original source code.
For instance, source-level debuggers have no symbolic names or types
for constants and functions introduced via {\tt \#define}, nor can tools
trace or set breakpoints in function macros, as they can for ordinary
functions (even those that have been inlined~\cite{Zellweger83:TR}).
As another example, Siff
and Reps describe a technique that uses type inference to produce
C++ function templates from C; however, the input is ``a C program
component that $\ldots$ has been preprocessed so that all include
files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
the readability and reusability of the resulting C++ templates.  As
yet another related example, call graph extractors generally work in
terms of the post-processed code, even when a human is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
leave the software engineer responsible for inferring the mapping between the
original and the post-processed source, which is an undesirable and
error-prone situation.

A tool that first preprocesses code, or takes already-preprocessed code as
input, cannot be run on a non-syntactic program or one that will not
preprocess on the platform on which the tool is being run.  These
constraints complicate porting and maintenance, two of the situations in
which program understanding and transformation tools are most likely to be
needed.  Additionally, a tool supplied with only one post-processed
instantiation of the source code cannot reason about the program as a
whole, only about that version that results from one particular set of
preprocessor variables.  For instance, a bug in one configuration may not
be discovered despite exhaustive testing of other configurations that do
not incorporate particular code or do not admit particular execution paths.

The third option, emulating the preprocessor, is fraught with difficulty.
Macro definitions consist of complete tokens but need not be complete
expressions or statements.  Conditional compilation and alternative macro
definitions lead to very different results from a single original program
text.  Preprocessing adds complexity to an implementation, which must trade
off performing preprocessing against maintaining the code in close to its
original form.  Extracting structure from macro-obfuscated source is not a
task for the faint-hearted.  Despite these problems, in many situations
only some sort of preprocessing or Cpp analysis can produce useful answers.

All three approaches would be unnecessary if programs did not use
preprocessor directives.  This is exactly what Stroustrup suggests:
\begin{quote}
  I'd like to see Cpp abolished.  However, the only realistic and
  responsible way of doing that is first to make it redundant, then
  encourage people to use the better alternatives, and {\em then\/}\,---\,years
  later\,---\,banish Cpp into the program development environment with the
  other extra-linguistic tools where it
  belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}
C++ contains features\,---\,such as constant variables, inline functions,
templates, and reference parameters\,---\,that obviate many uses of Cpp.
Thus, translation to C++ is a path for partial elimination of Cpp.
This study indicates the
feasibility\,---\,and our framework for analyzing preprocessor usage
provides a basis for the development\,---\,of an automatic translator with
two attractive properties.  It would take as input C programs complete with
preprocessor directives, and it would map many\,---\,preferably
most\,---\,uses of directives into C++ language features.  (It is not
practical to eliminate all uses of Cpp.  For example, C++ currently
provides no replacement for the {\tt \#include} directive, or for
stringization or pasting.  Macros that cannot be eliminated might be
annotated with their types or 
effects on parser or program state, so that even tools that do no Cpp
analysis can operate correctly on such programs.)

Another niche already filled by our tool is that of a ``macro lint''
program which warns of potentially dangerous (or non-standard) uses of Cpp.

And, we wrote CPPP.

%O'Callahan and Jackson also use type
%inference, although for program understanding rather than translation;
%they, too, apply their techniques to post-processed
%code~\cite{OCallahan-icse97}.


\subsection{Cpp: not all bad}

[[This section is out of place and horrible.  Where should the information
go?]]

Despite its evident shortcomings, Cpp is a useful and often necessary
adjunct to C, for it provides capabilities unavailable in the language or
its implementations.  Cpp permits definition of portable language
extensions that can define new syntax, abbreviate repetitive or complicated
constructs, or eliminate reliance on a compiler implementation to
open-code (inline) functions, propagate symbolic constants, eliminate dead
code, and short-circuit constant tests.  The latter guarantees are
especially valuable for compilers that do a poor job optimizing or when the
programmer wishes to override the compiler's heuristics.  Cpp also permits
system dependences to be made explicit and tested, resulting in a clearer
separation of concerns.  Finally, Cpp permits a single source to contain
multiple different dialects of C; a frequent use is to support both
K\&R-style and ANSI-style declarations.

%% NEED A REFERENCE TO DEBUGGER HERE!
%% also mention Emacs hide-ifdef mode

[[Expand:]]
A limited number of tools do exist to assist software engineers to
understand code with containing Cpp directives, such as debuggers that can
call {\tt \#define}d functions and editors that support viewing one
particular configuration of the code.

Our long-term goal is not to take these useful features away from
programmers, but to reduce Cpp use, making programs easier for humans to
understand and tools to analyze.






\section{Methodology}
\label{sec:methodology}

We analyzed {\numpackages} publicly-available software packages which
represent a mix of application domains, authors, programming styles, and
sizes.  Some are interactive, while others are not, and some are graphical
while others are text-based, command-line applications.
Figure~\ref{fig:packages} describes the packages and lists their sizes in
terms of physical lines (or newline characters) and non-comment, non-blank
(NCNB) lines.  The NCNB figure disregards lines consisting of only comments
or whitespace, as well as lines in a conditional that cannot evaluate to
true (such as {\#if 0}, which is frequently used to comment out code).  The
remainder of our analysis uses the NCNB length, which more accurately
reflects the amount of source code.

\begin{figure}
\centering
{% ``\small'' here has no effect; in a table, each cell is its own group.
 % It will require a hack to make the table smaller as I'd like it.
  \setlength{\tabcolsep}{.25em}
  \input{tbl-package-sizes.tex}
}
\caption{Analyzed packages and their sizes.  NCNB lines are non-comment,
  non-blank lines.}
\label{fig:packages}
\end{figure}

Before performing our analysis, we ran {\tt configure} or the equivalent on
each package in order to prepare it for compilation.  This creates various
header files (such as \file{configure.h}).  Our analysis works in the
absence of this step, but warns about uses of never-defined macros and
underreports some values related to those missing definitions.  We did
not, however, compile the packages; our analysis does not require that the
package be compilable (or even that the preprocessor be able to run on it).

We generated a list of all the C files in the package (both code and header
files).  In general, these have extensions like \file{.c}, \file{.h},
\file{.cc}, \file{.cpp}, \file{.hxx}, etc.  We removed files with such
extensions that don't actually contain valid C, and added others which were
{\tt \#included} by valid C files (some included files had extension
\file{.def}).

We analyzed all the C files in the package, as well as every file {\tt
\#include}d by any of those, primarily library header files.  We
took care to include as many libraries as possible so as not to overlook
those definitions.  However, the figures reported in this paper always omit
all macros defined only in libraries.  This prevents libraries from
swamping the characteristics of the package code, which is our focus in
this study.  The programmer generally has no control over libraries and
their header files, and may not even know whether a library symbol is
defined as a macro or a true function or variable.  Finally, we assume that
library macros are carefully written to behave correctly and robustly
(sadly, we discovered this not to always be true in practice).

Our analysis is a true whole-program analysis; rather than preprocessing the
code and examining just one configuration, we examine all possible code and
ignore no possible  conditional compilation conditions (though we do skip
over those which can be statically proven to be false).   As a result, this
analysis is more thoroughgoing than traditional ``whole-program'' analyses.

We perform approximate parsing because the input is not a valid C program;
as a result, we miss some constructs, but we can cope with uncompilable C
(and with different conditional compilation branches).  We include parsers
for expressions, statements, and declarations.

We performed our analysis via a collection of Perl scripts, totaling
approximately 11,000 lines (7,000 NCNB lines).  The raw data, which
includes considerable data not reported here, and the programs used to
generate and manipulate them, are available from the authors.



\section{Occurrence of preprocessor directives}
\label{sec:directives}

Figure~\ref{fig:directives-breakdown} shows how often preprocessor
directives appear in the programs we analyzed.  Each group of bars in the
figure represents the percentage of non-comment, non-blank (NCNB) lines
attributed to the specified category of directives, with each individual
bar showing the percentage for a specific package.  Conditional compilation
directives ({\tt \#if}, {\tt \#ifdef}, {\tt \#ifndef}, {\tt \#else}, {\tt
\#elif}, {\tt \#endif}) are grouped together.

\begin{figure}
\centerline{\epsfig{file=fig/directives-breakdown.eps,height=7.5in}}
\caption{Preprocessor directives as a fraction of non-comment,
  non-blank (NCNB) lines.}
\label{fig:directives-breakdown}
\end{figure}

The prevalence of preprocessor use makes understanding CPP constructs
crucial in any analysis of a program.  Overall, 9.6\% of non-comment,
non-blank program lines are preprocessor directives: about one in ten NCNB
lines is a notation to the preprocessor rather than C code.  Across
packages, the percentage varies from less than 4\% to more than 22\%.
(These figures do not include the 28\% of lines which expand a macro or the
38\% of lines whose inclusion is controlled by {\tt \#if}; see
section~\ref{sec:dependence}.)

% \#if 46\%, \#define 35\%, \#include 13\%, \#undef 3\%, \#line 2\%

Conditional compilation directives account for just under half (46\%) of
the total directives in all packages, macro definitions comprise another
35\%, and file inclusion makes up most of the rest.  Packages are not very
uniform in their mix of preprocessor directives, however.  (If they were,
each block of bars in figure Figure~\ref{fig:directives-breakdown} would be
a scaled version of the top block.)  Some packages rely much more heavily
on one set of Cpp features than another, so analyzing each is essential for
uniformly good performance.  In particular, the prevalence of {\tt
\#include} is essentially independent of incidence of other directives.
The percentage of conditional directives varies from 16\% to 74\%, the
percentage of {\tt \#define} varies from 14\% to 52\%, and the percentage
of {\tt \#include}s varies from 4\% to 60\%.


\subsection{{\tt \#line}, {\tt \#undef}, and other directives}

The definedness of a macro is often used as a boolean value.  However, {\tt
\#undef} is rarely used to set such macros to ``false''$\!$.  Most uses of
{\tt \#undef} immediately precede a definition of the just-undefined macro,
to avoid preprocessor warnings about incompatible macro redefinitions.

Every use of {\tt \#line} (in \pkg{bash}, \pkg{cvs}, \pkg{flex}, \pkg{fvwm},
\pkg{gawk}, \pkg{gcc}, \pkg{groff}, and \pkg{perl}) appears in lex or yacc
output that enables packages to build on systems lacking lex, yacc, or
their equivalents.  For instance, \pkg{flex} uses itself to parse its
input, but also includes an already-processed version of its input
specification (that is, C code corresponding to a {\tt .l} file) for
bootstrapping.

% , as are ``other'' directives (such as ).  
% as well as user-defined ones like {\tt \#module}

Rarely-appearing directives such as {\tt \#pragma}, {\tt \#assert}, and
{\tt \#ident}, and unrecognized directives, are omitted from
figure~\ref{fig:directives-breakdown}.

Among the packages we studied, these directives account for .017\%, or one
in six thousand, directives; their only significant user is \pkg{g77},
which contains 154 uses of {\tt \#error} (representing 1.5\% of its
preprocessor directives and 0.16\% of its NCNB lines) to check for
incompatible preprocessor flags.  We ignore the null command (``{\tt \#}''
followed by only whitespace), which produces no output.


\subsection{Packages with heavy preprocessor use}

The \pkg{gzip}, \pkg{remind}, and \pkg{bash} packages deserve
special attention for their heavy preprocessor usage\,---\,22\%, 21\%, and
16\%, respectively.

\pkg{gzip} {\tt \#define}s disproportionately many macros as literals and
uses them as arguments to system calls, enumerated values, directory
components, and more.  These macros act like {\tt const} variables and are
evidence of good programming style.  \pkg{gzip} also contains many
conditional compilation directives, since low-level file operations (such
as setting creation time and access control bits, accessing directories,
and so forth) are done differently on different systems; \pkg{gzip} is a
highly portable program.

\pkg{remind} supports speakers of ten different languages (and various
character sets) by using {\tt \#define}d constants for basically all user
output.  It also contains disproportionately many conditional compilation
directives; over half of these test the definedness of \verb|HAVE_PROTO|,
in order to provide both K\&R and ANSI prototypes.

Like \pkg{gzip}, \pkg{bash} is portable across a large variety of
systems, but \pkg{bash} uses even more operating system services.
Ninety-seven percent of \pkg{bash}'s conditional compilation directives
test the definedness of a macro whose presence or absence is a boolean
flag indicating whether the current system supports a specific feature.
The presence or absence of a feature requires different (or sometimes
additional) system calls or other code.


\section{Macro definition bodies}

This section examines features of macro definitions that may complicate
understanding the containing program.  With respect to individual macro
definitions, we report on how many expand to a partial or unidentifiable
syntactic entity, which take advantags of special Cpp features that lie
outside the programming language, and which contain other error-prone
constructs.  We then turn to multiple definitions of a particular macro
name.  Multiple definitions complicate understanding\,---\,even if they do
effectively the same thing; we report on incidence of redefinitions and of
differing redefinitions, especially redefinitions which expand to an
incompatible syntactic construct.  

\subsection{Macro body categorization}

We categorized macro bodies into 28 categories; for the purposes of
  this paper, we coalesce these into ten higher-level categories.
  Say what they are.

For each package, we report how many definitions create an
  expansion which falls in each category

There's no patern, again.  (Nor is there a pattern by package size
  or by type of application.)

\label{sec:categorization}

This section examines the purposes of macros and how they are intended to
be used, which requires heuristic categorization of macro definition bodies.  A
straightforward refinement that we are pursuing examines macro uses to aid
this categorization.  (For example, a macro used where a type should appear
can be inferred to expand to a type; a macro used before a function body is
probably expanding to a declarator.)

In addition to classifying each macro as taking arguments or not, our tool
identifies the following specific categories (and a number of more
rarely-used ones omitted for brevity;
figure~\ref{fig:subset-categories} contains a more complete list).  The examples
are chosen for clarity and brevity from the packages studied.

{\input{expl-categories.tex}}

[[Need to correct all these numbers.]]

Figure~\ref{fig:categorization} shows the percentage of macros that fit
into these categories for each package.  Overall, [[83\%]] of macros are
expressions\,---\,mostly constants; further analysis of the conditional
compilation structure (in the style of Krone and Snelting~\cite{Krone94})
and of the macros with free variables (essentially achieving dynamic
scoping) is needed to see which of the roughly [[33\%]] of expression macros
should be easy to convert to C++ language features such as constants or
enumerated values.  The [[7\%]] that are null
defines, should also be easy to understand and/or translate.  Another [[5\%]]
are statements, most of which are straightforward (complications include
scoping and semicolon swallowing).  That only [[0.2\%]] of macros exploit
stringization or pasting, the only truly extra-linguistic capabilities in
the C preprocessor, is encouraging.

Our tool failed to categorize less than [[2\%]] of the [[26701]] definitions;
performing even a single level of macro expansion in bodies would make most
of those failures categorizable.  Other straightforward improvements
include making a second pass after an initial categorization and using
dependence information to determine which definitions can be active at an
invocation site.  We have not pursued these enhancements, primarily because
our tool is already accurate enough for our purposes.


% The following isn't quite enough to get the columns lined up in this table.
% \newcolumntype{d}{D{.}{.}{2}}
% \begin{tabular}{|l|d|d|d|d|d|d|d|}\hline
\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{categories-tbl.tex}}%
% }
\centerline{\epsfig{file=fig/def-categories.eps,height=6in}}
\caption{Categorization of macro definition bodies.}
\label{fig:categorization}
\end{figure}



%In anticipation of the translator tool, the analysis tool infers
%types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
%and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
%type information is in the early stages, however, and we do not report
%on the preliminary results in this paper.

% [FIX: Benefits even from simple literal constant conversion -- exposes
% symbolic information to the debugger]

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.]



\subsection{Tricky cpp uses}

Things slightly out of the ordinary (ie, outside the
          scope of C, which is why the preprocessor was used), hard to translate

{\input{expl-properties.tex}}

{\input{tbl-subset-properties.tex}}

        About one in six macros (16.8\%) falls into at least one category

        We show an exact breakdown by subsets because, if we report by
          property, the numbers don't add to 100\%: sometimes a macro has
          multiple properties.

        Not as much use of stringization, pasting as we anticipated -- we
          had been particularly worried about them.

        We didn't analyze whether the free variables are globals or locals,
          which could substantially impact understanding/translation.

        The uses of types are going to be problematic.

[[These numbers aren't right any more -- not 10%.]]
One tenth of expression macros in our study use assignment operators, which
have potentially unexpected results.  A macro argument that is assigned to
is similar to a pass-by-reference function argument and need only be noted in the
macro's documentation.  A macro that assigns a global variable also
presents no difficulties in understanding or translation into a C++
inline function.
Assignment to a local variable that is free in the macro body, however,
demands that such a variable exist wherever the macro is invoked, and
assigns to different variables at different invocations.\footnote{By
  contrast, LCLint considers assignment to a macro argument dangerous but
  does not appear to check for assignments to local
  variables.~\cite{Evans:LCLint}} Such a macro implements a restricted form
of dynamic scoping by capturing the instance of a variable visible at
the point of macro invocation.


\subsection{CPP errors (or potential errors): lint output}

        We identified a number of potentially dangerous programming
          constructs -- those which may well be intentional, but which also
          might cause a macro not to behave as anticipated (and not to
          behave like an ordinary function).  One fourth of all definitions
          triggered at least one such warning; and across macro names,
          one fourth contain a definition which triggers a warning.

{\input{expl-lint.tex}}


{\input{tbl-lint.tex}}

        In some, but not all, C dialects there ways to declare a local
          variable so that each formal only need be used once.

        We didn't examine every use.  Besides, the point isn't just the
          current uses, but also future uses:  we want these to be easy for
          programmers to use.

        We also discovered a number of files which begin or end inside a
          brace scope or an {\tt \#if} scope.  Some of these were intentional, while
          others are bugs (such as, in one case, a failure to close a /* */
          style comment) which were apparently not discovered because testing
          did not build the package under all possible circumstances.

        Also warnings for unexpected indentation and more.

        Also some bad constructs:
          {\tt \#undef \verb|GO_IF_INDEXABLE_BASE|(X, ADDR)}
          {\tt \#module}

Assignment operators have potentially unexpected results.  A macro argument
that is assigned to is similar to a pass-by-reference function argument and
need only be noted in the macro's documentation.  A macro that assigns a
global variable also presents no difficulties in understanding or
translation into a C++ inline function.  Assignment to a local variable
that is free in the macro body, however, demands that such a variable exist
wherever the macro is invoked, and assigns to different variables at
different invocations.\footnote{By contrast, LCLint considers assignment to
  a macro argument dangerous but does not appear to check for assignments
  to local variables.~\cite{Evans:LCLint}} Such a macro implements a
restricted form of dynamic scoping by capturing the instance of a variable
visible at the point of macro invocation.

\subsection{Multiple definitions}

        When classifying a name (as opposed to a definition), "failed
          classification" includes "two different defs fell into different
          categories".  (We could also get that even if a single
          definition, earlier, if it expands to just such a macro.)
          Explain how we combine two different categories into a single one.

        Not C code tends to be for the benefit of Makefiles; compilation
          lines, library filenames, and such tend to differ for each
          operating system.  (We see those macros because: 1. we process
          all the .h files, and 2. some files are used for preprocessing
          both code and Makefiles, etc.  This latter use is problematic in
          itself, because by definition CPP is supposed to get a syntactic
          C program; it should err if what's between single quotes isn't a
          legal character constant, etc.  But many implementations don't
          perform any such checks.)

        Syntactic is parentheses, commas, semicolons, critical section
          delimiters, etc:  frequently-used.

        It stands to reason that "failed classification" has multiple
          definitions, because when determining the category for a macro
          name, we categorize each definition, and if any of them are
          either failures or have different categories, then the classification
          for the name is "failure".  (See below, "inconsistent definitions".)

        Few for unknown symbol, because if any def isn't unknown symbol,
          then neither is the entire macro.

        By package (not pictured):  most follow the "Mean" curve.

          Half of packages have no macros defined more than 14 times.

          Notable exceptions to the rule:
            bc:  all macros are defined either one or two times

            remind:  10\% of macros are defined between 11 and 15 times
                (none defined 16 or more)

            gcc: over 4\% of macros are defined more than 20 times
                gcc goes 99.5\% at 50 definitions
                i.e., 99.5\% of macro names are defined 50 or fewer times.
                1 macro, \verb|CPP_PREDEFINES|, is defined 181 times (and undefined
                  198 times)

        (Talk about the new table which summarizes def, ddf, and use
          frequency.  Not sure what to say about it yet.)

        % \section{Frequency of macro definition and usage}
        \label{sec:usage}

        % The second question we asked was: where and how often are macros
        % defined and used in practice?  

        % \begin{figure}
        % {\small
        %   \setlength{\tabcolsep}{.25em}
        %   \centerline{\input{freq_of_def-tbl.tex}}%
        % }
        % \caption{Number of definitions per Cpp identifier.  The numbers in the
        %   table represent the percentage of identifiers which are defined a given
        %   number of times or fewer.  For example, \pkg{bison} contains 4 or fewer
        %   definitions for 91.51\% of all Cpp macros it defines, and no
        %   macro is defined 5, 6, 7, 8, or more than 9 times.}
        % \label{fig:define_count}
        % \end{figure}

        \begin{figure}
        \centerline{\epsfig{file=fig/cat-def-frequency.eps,height=7.2in}}
        \caption{Number of definitions per Cpp identifier, graphed as
          the percentage of identifiers that are defined a given number of times
          or fewer.  Overall, 96\% of macros were defined three or
          fewer times; the other 4\% of macros had four or more distinct
          definitions ({\tt \#define} directives).}
        \label{fig:freq-def-cat}
        \end{figure}

The outlier, for which 10\% of
macros are defined more than eight times, is the \pkg{remind}
package, which uses macros for all user output.


% \begin{figure}
% \centerline{\epsfig{file=fig/def-frequency.eps,height=7.2in}}
% \caption{Number of definitions per Cpp identifier, graphed as
%   the percentage of identifiers that are defined a given number of times
%   or fewer.  Overall, 96\% of macros were defined three or
%   fewer times; the other 4\% of macros had four or more distinct
%   definitions ({\tt \#define} directives).  The outlier, for which 10\% of
%   macros are defined more than eight times, is the \pkg{remind}
%   package, which uses macros for all user output.}
% \label{fig:freq-def}
% \end{figure}

        [[We include definitions in library header files, if the symbol is defined
        both in the package and in the library.]]

        Figure~\ref{fig:freq-def} graphs the number of times each identifier is
        {\tt \#define}d in each of the packages.  No distinction is made between
        sequential redefinitions of a macro and multiple definitions that cannot
        take effect in a single configuration (say, because they appear in
        different branches of a Cpp conditional).

        While all macros defined by \pkg{bc} have only one or two different
        expansions, more than 10\% of macros defined by \pkg{remind} expand to more
        than eight different texts.

        In all but four packages, at least 93\% of all macros are defined three or
        fewer times.  For \pkg{bash}, \pkg{glibc}, and \pkg{dejagnu}, such macros account for 90\%,
        largely because these packages are highly portable and also quite dependent
        on system libraries.  The \pkg{remind} program uses macro definitions to provide
        localization support for ten different natural languages (and multiple
        character sets for some of them), accounting for its surprisingly large
        number of macros with many definitions.  All of \pkg{remind}'s macros are defined
        14 or fewer times, but 16 macros in the {\numpackages} packages are defined
        more than 16 times, including three with more than 30 distinct definitions.

        These data demonstrate that multiple definitions of symbols is not
        numerically frequent; even more importantly, the definitions of a symbol
        tend to be compatible, as shown in section~\ref{sec:categorization}.



\subsection{Multiple distinct definitions}

        \begin{figure}
        \centerline{\epsfig{file=fig/cat-ddf-frequency.eps,height=7.2in}}
        \caption{Number of distinct definitions per Cpp identifier, graphed as
          the percentage of identifiers that are defined a given number of times
          or fewer.  [[Say something to help interpret it.]]}
        \label{fig:freq-ddf-cat}
        \end{figure}

        This uses a less strict rule than that used by CPP when determining
          whether to issue a warning about redefinition.  We eliminate all
          comments and whitespace, canonically rename all formal arguments,
          and compare all character and string literals to be identical.
          Thus, it is a lower bound on the number of truly distinct
          definitions, much as the previous chart was an upper bound.

        Even when two distinct definitions are identical, if they appear in
          different locations then it is more likely that something used by
          that definition has changed.

\subsection{Inconsistent definitions}

        Even among macros with multiple definitions, those different
          definitions generally fall into the same, or compatible, categories.

        The arrows indicate macros for which the names would be classified
          as "failure".

        Two anomalies are due to the fact that "statement" category should
          really be called "statement-related":  it includes full
          statements (which comprise the majority, I think), statements
          missing a final semicolon, partial statements, multiple
          statements, multiple statements plus a partial one.
          * no arrow on one "expression + statement", at 1.1\%, because that
                was actually "expression + semicolonless statement" -- and
                an expression plus a semicolon also makes a statement.
                For instance, the expressions could have been function calls.
                The other, just below it, has slightly fewer occurrences
                (but both round to 1.1\%; it also contains "statement", which
                is incompatible with "expression").  Also compare the similar
                duplication at .26\% and .10\%.
          * arrow on just "statement" at .35\%:  was actually
                "semicolonless statement, semicolonless statements, statement"

        The "symbols+expression" results from expression + function name,
          also from expression + symbols (where first one is macro that expands
          to partial expression).  [This isn't common at .64\%, but is still
          one of the most frequently occurring failures.]


[[Where should this go?]]
\begin{itemize}
\item If the categories are the same, use that.

\item If one has no definition, is a null define, or is an unknown symbol, use
the other.

\item If one is a constant and the other is an expression, use expression.

\item If one is symbols and the other is reserved word or type, choose the latter.

\item If one is statement-related, choose the other if it's the corresponding plural.
\item If one is statement-related, sans semicolon, choose the other if it's partial.
\end{itemize}


        \begin{figure}
        {\small\centerline{\input{tbl-subset-categories.tex}}}
        \caption{Subset categorization of macros (not macro definitions).   Items
          less than one twentieth of a percent are omitted; such items appear
          fewer than ten times in the codebase.}
        \label{fig:subset-categories}
        \end{figure}

        Figure~\ref{fig:subset-categories} indicates that multiple definitions of a
        particular macro tend to be compatible.  It classifies macros rather than
        macro definitions, and uses a finer breakdown of categories.  Each macro is
        given a set of categorizations (corresponding to the categorizations of its
        definitions) and the incidence of each such is displayed.  For over [[97\%]] of
        macros, all of the macro's definitions are given the same
        categorization\,---\,even when categories such as literal, constant,
        expression, and expression with assignment are considered unrelated.  The
        most common ``conflict'', statement and null define, is also harmless in
        most contexts.  We expect that closer examination of most of the other
        conflicts will demonstrate that they present no real obstacles to
        understanding (even if they do complicate some details).


\section{Uses}

\subsection{high usage}

        Give average uses per defined macro name (see new summary table).

        In the chart, a higher line indicates less use.

        Half of all macros are used two or fewer times (!).  12\% not used
          at all (!).

        Syntactic macros tend to be used far more often; also type-related
          ones.  These make sense, as they are frequently-used constructs
          that permeate code (appear at every definition, for instance).

        Non-C-code, null defines (ie, every definition of the name is a
          null define), and constants are used least.  The latter is a bit
          surprising, but shows that macros are being used as a
          configuration mechanism rather than a linguistic mechanism.  (Do
          I buy that?  Maybe not. ??)

        All packages follow approximately the same curve; the outlier is
          gnuplot.  Gnuplot uses macros less, on average, than other packages
          do.  Over 40\% of the macros it defines are never used at all --
          say why.

% \section{Macro use}

% \begin{figure}
% % {\small
% %   \setlength{\tabcolsep}{.25em}
% %   \centerline{\input{freq_of_use-tbl.tex}}%
% % }
% \centerline{\epsfig{file=fig/use-frequency.eps,height=7.5in}}
% \caption{Number of expansions per Cpp macro.  The numbers in the
%   table represent the percentage of identifiers which are expanded a given
%   number of times or fewer.  For example, \pkg{g77} expands 65\% of its
%   macros two or fewer times.}
% \label{fig:freq-use}
% \end{figure}

\begin{figure}
\centerline{\epsfig{file=fig/cat-use-frequency.eps,height=7.5in}}
\caption{Number of expansions per Cpp macro.  The numbers in the
  table represent the percentage of identifiers which are expanded a given
  number of times or fewer.  For example, \pkg{g77} expands 65\% of its
  macros two or fewer times.}
\label{fig:freq-use-cat}
\end{figure}

Figure~\ref{fig:freq-use} is structured as the previous figure, but it
represents the number of times that a defined name is expanded in 
the package (not in system headers).  About 82\% of all macros were
expanded eight or fewer times.

It is notable that most packages contain a significant number of defined
macros that are never expanded\,---\,on average, 12\%.
(Figure~\ref{fig:freq-use} reports only on macros defined in a package, not
those defined in system or library header files, inclusion of which would
push the unused percentage well above 50\%.)

Macros with 10 or fewer uses
cover approximately [[85\%]] of the cases.

[[Double-check all these numbers.]]
The tail of this distribution is quite long, indicating that some macros
are used very heavily.  Ninety-nine percent of macros are expanded 147 or fewer
times, 99.5\% of macros are expanded 273 or fewer times, 99.9\% are
expanded 882 or fewer times, and \pkg{python} uses {\tt NULL} (which \pkg{python}
itself defines) 4233 times.  Figure~\ref{fig:freq-use} weights each macro
equally rather than weighting each macro use equally, which would weight
\pkg{python}'s {\tt NULL} 4233 times more heavily than a macro used only once
(and infinitely more than a macro never used at all).  Only macros defined
in a package, and uses in that package, are counted; system macros and uses
are excluded.

Summary:  \input{tbl-summarize-frequency.tex}


\subsection{number of arguments}

        You might wonder whether macros are used like functions (taking
        arguments) or like constants (taking no arguments).  We graphed
        that.  This seems irrelevant to me; I don't see where to fit it in,
        or what to say about it.

\subsection{inconsistent usage}

    Macros have two general purposes:  they can control the inclusion of
        lines of code (by appearing in a {\tt \#if} condition that controls that
        line) or can change the text of a line (by being expanded on that
        line).  Each of these uses can be modeled by C++ language features
        -- conditionals or (for many types of substitution) consts and
        inlines.  But when a macro is used in both ways, then there's no
        one C++ language feature.  (Additionally, it's harder to understand
        and to analyze a macro that's used in both ways than one that is
        only used in one way or the other.)

    We split macro uses into three categories:  
        * uses in C code, where the macro's expansion controls textual
            replacement
        * uses in {\tt \#if}, {\tt \#ifdef}, {\tt \#ifndef}, {\tt \#elsif} conditions
        * uses in a macro body, which eventually bottom out to one of the
            others (unless that macro is never used...)

      We discarded uses in CPP conditionals whose only purpose was to
        prevent redefinition.  (More specifically, if the condition tested
        only definedness, the next line defined the macro just tested, and
        the line after that ended the CPP conditional.  This is a bit
        overrestrictive, but conservative, and in practice fairly accurate.)

      Uses in a macro body generally are intended to be used however the
        containing macro is.  Since those uses also appear in this chart,
        we did not attempt to track each such use in a body to some macro's
        appearance in either code or a conditional.  Thus, the interesting
        categories are "code", "conditional", "macro", "code and
        conditional", and "no use".

      Macros are used more frequently to expand code than to control its
        inclusion, by a factor of ten to one.  (However, each inclusion use
        can control many lines of code; see the dependence section, below.)

      A surprising number -- nearly 12\% -- of macros defined in a package
        are never used at all.  Sometimes this is a result of shipping a
        standard set of headers with the package -- it's like a library for
        that development team, but one that can't be counted upon to exist
        everywhere, so it has to be provided.  For gnuplot, over 40\% of
        macros are never used because the package's support for several
        terminal types, such as tgif, is unfinished (and thus unused).
        Even discounting that package, though, the numbers are remarkably
        high.  We would be surprised if one in eight functions and
        variables in a package were never used, not even in testing code.
        [David, do you have any idea what fraction this is in practice?]
        (The percentage of macros defined in libraries/standard header
        files which are never used in the code is enormous, but that is
        expected.)

      The potentially problematic uses are those for which a macro appears
        both in code and in a conditional; these comprise only 3.4\% of all
        macro names.

      Across packages, there is very little variation.  Packages which use
        the preprocessor sparingly are as likely to have a high percentage
        of mixed usage as packages which make heavy use of CPP.  (There is
        a very slight tendency for the less aggressive packages to have
        more uses in code, fewer uses in conditionals, and fewer macros
        that are never used.)

\begin{figure}
{\small
  \setlength{\tabcolsep}{.25em}
  \input{tbl-where-used.tex}%
}
\caption{Where macros are used: in C code, in macro definition bodies, in
  conditional tests, or in some combination thereof.  The figure reflects
  only package macros and uses, not system files.  The numbers don't sum to
  100\% because of rounding.}
\label{fig:where-used}
\end{figure}

Figure~\ref{fig:where-used} breaks down macro usage according to whether
the macro invocation occurs in Cpp directives (which is further broken
down into conditional tests and definition bodies), in other C code, in
both, or in neither (i.e., no uses).

No package expanded all of its defined macros; two expanded fewer than 70\%
of the defined macros.  The dominant usage was in C code only; these uses
do not, therefore, have any affect on conditional compilation (for example).

In general, packages use macros either to direct conditional compilation or
to produce code, but not for both purposes; this separation of concerns
makes the source code easier to understand.  Only 3.1\% of macros expand in
both code and conditional contexts (the fourth and fifth categories in the
figure; the sixth, macro and conditional, accounts for only another 0.2\%
of macros). 
Conditional usage is rare in general; conditional compilation accounts for
half of Cpp directives but only 5.4\% of macros (plus the categories just
listed above).


\subsection{mixed usage in conditional (control)}

\begin{figure}
\centerline{\epsfig{file=fig/ccd-categories.eps,height=7.5in}}
\caption{CCD categories.}
\label{fig:ccd-categories}
\end{figure}


    Conditionals are used to check for a bunch of things (examples); but a
      single conditional generally checks just one of these categories.  We
      counted the number in each, and also the number that checked symbols
      falling in multiple categories.

    Few mixed categories.

    Lots of variation overall.

    The macros with mixed usage (especially here, but also in the previous
      section) are akin to Krone \& Snelting's anomalous macros that
      interfere in the lattice.

    What else to say about this chart?


{\input{expl-ccd-cat.tex}}


\section{Dependences}

 Two types of dependence:  control/inclusion, and substitution/expansion
     This includes macros which control its inclusion via conditional
     compilation, macros that are invoked on the line, and macros that
     control the definitions of, or are called by, directly invoked macros

 Explain this.  Explain how we compute it transitively.

 The dependence charts omit Emacs and Mosaic; the full dependence
   information for each overran the virtual memory available on the machine
   on which we ran our experiments.  This was in large part due to
   dependences in libraries; the Motif library is a particular problem.
   (We did compute dependence information for Plan, which is
   the third of our 30 packages which uses the Motif library.)  So these
   numbers are smaller than they might be with more complete information.

\subsection{line dependent on many macros}

\begin{figure}
\centerline{\epsfig{file=fig/dep-byline.eps,height=7.5in}}
\caption{dep-byline}
\label{fig:dep-byline}
\end{figure}


    [Gloss over what kind of line; I think it's a physical line, but each
      physical line is coalesced with other parts of its logical line for
      the purpose of computing dependence info.]

    We computed, for each line, the number of macros it depends on.

    Describe the matrix:  ExE=E, Cx?=C, ?xC=C; plus, we sum across
      conditional compilations and alternate definitions.

    The log scale is shifted by unity in order to place 0 on the scale; most
      58\% of lines have no dependence on macros at all.  (Most lines that
      don't expand any macros and appear in C files fall into this category.)

    Overall, 28\% of lines expand a macro -- that's quite a few.

    5\% of all lines depend on at least 134 macros.

    On average, each line is expansion-dependent on .040 macros,
      inclusion-dependent on .31 macros, and any-dependent on .34 macros.
      (These numbers don't add up because a line may be both expansion and
      inclusion dependent on a macro, but that macro is only counted once
      in the "any dependence" number.)

        One line of gcc is expansion-dependent on the
        expansions of 187 macros.  (This is an outlier:  of the 297760 lines in
        gcc, only 13 are expansion-dependent on more than 100 macros -- all of
        which fall in the range of 159 to 187 such dependences.)

        It's this one, whose mere appearance in the final source is
        dependent on 182 macros (not such an outlier: over 10,000 lines are
        inclusion-dependent on more than that many macros):

        gcc-2.7.2.1/explow.c:430: dependences incl=182; exp=187; either=356
              \verb|LEGITIMIZE_ADDRESS| (x, oldx, mode, win);

        \verb|LEGITIMIZE_ADDRESS| is defined 30 times in gcc, and one randomly-selected
        definition was 37 line long (and chock full of other macro invocations).

\subsection{macro controlling many lines}

    We combined these charts for the different packages, because the shapes
      of each were quite similar for each package.  In order to permit
      combining charts across quite different package sizes, we computed
      values that were percentages of package size.

    Each bar represents all macros that control less than the bar label
      percent of the lines in its package (but more than the previous
      label).  For instance, the .17 bar in the red chart indicates that
      there are 286 macros that each control between .11\% and .17\% of the
      entire package containing that macro.

   \subsubsection{expansion}

\begin{figure}
% This works, but the figures are upside-down.
% \centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=90,height=3.75in}}
% \centerline{\epsfig{file=fig/incl-dep-bymacro.eps,angle=90,height=3.75in}}
% Can't use ``height'' when rotating by -90 or +270; I don't know why.
\centerline{\epsfig{file=fig/exp-dep-bymacro.eps,angle=270,width=.9\linewidth}}
\bigskip
\centerline{\epsfig{file=fig/incl-dep-bymacro.eps,angle=270,width=.9\linewidth}}
\caption{dep-bymacro}
\label{fig:dep-bymacro}
\end{figure}

        This exponential decay indicates that, as expected, there are more
          macros which control just a few lines and fewer macros that
          control a lot of lines.

        Outliers (> 6\% of all lines expand):
          int in gcc (14.85\% !)
          NULL, ANY, object in python

        Above 5\%: 
          SvANY in Perl
          const in RCS
          ip in workman -- bogus, as defined just twice, then undefined;
                  most places it is a formal parameter and out of the scope
                  of the macro definition.
          ArgCount, Args in xfig
          rtx in gcc (defined to int or int*)

   \subsubsection{inclusion}

        For each package, the graph is bimodal (so much so that this even
          shows up a bit on the combined chart; it's more marked in the
          individual packages, I think).  Most macros control inclusion of
          no, or very few, lines; but quite a few control a substantial
          fraction of the package (around 10\%).

        All the headviest dependences are on header files (for instance,
          \verb|H_PERL| controls inclusion of over 53\% of Perl's lines).

        [It would have been intersting to run these numbers for everything
          but include file multiple inclusion prevention macros.]


\subsection{cppp experiment}

    [After the above, this experiment doesn't sound like such a smart thing
      to try any more; its results certainly aren't unexpected.]

    We had noticed in our programming that a particularly heavy use of the
      preprocessor is to handle mutiple dialects of a language.  These uses
      tend to be less structured:  they don't have a simple pattern.  And
      this work must be done in the preprocessor:  there's no hope of
      integrating it into the language.  So we performed an experiment to
      see whether standardizing on a single language would reduce
      dependences, failed classifications, etc.

    We built a CPPP partial evaluator (called cppp) which, given a set of
      macros known to be defined or undefined (and, optionally, their
      expansions), eliminates all possible CPP conditionals.  We defined
      all the macros that can be depended on if using ANSI standard C or
      C++ with POSIX-compliant libraries, preprocessed all the source (and
      all library header files), and reran our experiments.

    The results were disappointing:  while some numeric measures of how
      complicated the resulting program's dependences are decreased, most
      remained at about their previous level.  The number of multiple
      definitions of macros, and the number of failed classifications, did
      not decrease as much as anticipated.

    From this we can conclude that there is no one obvious single point of
      attack:  even eliminating what seems most prevalent to us doesn't
      make a sufficient difference.



\section{Conclusions}
\label{sec:conclusion}

% \subsection{Who cares?}
\subsection{Relevance of the results}

[[This is the place to recap the various sections, giving the highlights of
each.]]

The results of this research are of interest to language designers, tool
writers, programmers, and software engineers.

Language designers can examine uses of the macro system's extra-linguistic
capabilities to determine what programmers consider missing from the
language.  Future language specifications can support (or prevent!)\ such
practices in a more disciplined, structured way.

% [[[Think more about this:  Also, how do language choices lead to more/less
% tightly integrated (as opposed to open, component-based) environments?
% E.G., no need for \verb|__LINE__| in Java?]]]

Programming tool writers, too, need to understand how Cpp is used, for that
sheds insight on the sorts of inputs that will be provided to the tool.  By
coping with the most common constructs, the tool can provide relatively
good coverage for low effort.  By identifying problematic uses, much better
feedback can be given to the programmer, who can be more effective as a
result.  The analysis results also indicate the difficulty of processing [wc]
preprocessor directives; before these analyses, we did not know whether the
task was so trivial as to be uninteresting, so difficult as to be not worth
attempting, or somewhere in between.

The analyses are of interest to programmers who wish to make their code
cleaner and more portable, and can help them to avoid constructs that cause
tools (such as test frameworks and program understanding tools)
to give incomplete or incorrect results.

% Also, learn weird new Cpp tricks!

Finally, our results are of interest to software engineers for all of the
above reasons and more.  Since this is the first Cpp usage study of which
we are aware, it is worth performing simply to determine whether the
results were predictable a priori; we did in fact discover a number of
interesting features of our suite of programs.


\subsection{Making C programs easier to understand}

The combination of C and Cpp makes a source text unnecessarily difficult to
understand.  A good first step is to eliminate Cpp uses where an equivalent
C or C++ construct exists, and to apply tools to explicate the remaining
uses.  Here we discuss a few approaches to solving this problem by
eliminating the source of confusion rather than applying tools.  We do not
seriously consider simply eliminating the preprocessor, for it provides
conveniences and functionality not present in the base language.

Since many of the most problematic uses of Cpp provide portability across
different language dialects or different operating environments,
standardization can obviate many such uses.  Canonicalizing library
function names and calling conventions makes conditional compilation less
necessary and incidentally makes all programs more portable, even those
which have not gone to special effort to achieve portability.  This
proposal moves the responsibility for portability (really, conformance to a
specification) from the application program into the library or operating
system, which is a reasonable design choice since many application programs
rely on a much smaller number of libraries and run on relatively few
operating systems.

Likewise, the most common single cause [[Realy?  I'm not sure I believe
that without qualification]] for Cpp directives would be eliminated if the
C language and its dialects had only a single declaration syntax.  Because
most C compilers, and all C++ compilers, accept ANSI-style declarations,
much support for multiple declaration style may have outlived its
usefulness.  [[On the other hand, K\&R support was just *added* to some
mature, popular package (zsh?) recently.]]  We are investigating the effect
on our statistics (and program understandability) of ``partially
evaluating'' a program source by specifying the definedness and values of
some Cpp identifiers.

Some Cpp directives, such as {\tt \#include}, can be moved into the
language proper; this would also eliminate the need for Cpp constructs that
prevent multiple inclusion of header files.  Likewise, compilers that do a
good job of constant-folding and dead code elimination can encourage
programmers to use language constructs rather than relying on the
guarantees of an extra-linguistic tool like Cpp.\footnote{Interestingly,
  the issue seems to not be whether compilers do the appropriate
  optimizations, but whether programmers have confidence that the
  optimizations will be performed; if unsure, programmers will continue to
  resort to Cpp, since certainly a compiler cannot generate code for source
  that it does not ever even see (because Cpp has already stripped it
  away).}

Common Cpp constructs could be replaced by a special-purpose syntax.  For
instance, declarations or partial declarations could be made explicit
(perhaps first-class) objects; similar support could be provided for
repetitive constructs and dynamic scoping.  Manipulations of these objects
would then be performed through a clearly-specified interface rather than
via string and token concatenation, easing the understanding burden on the
programmer.  Such uses would also be visible to the compiler and could be
checked and reasonable error messages provided.  The downside of this
approach is the introduction of a new syntax or new library functions which
may not simplify the program text and which cannot cover all cases, only a
few specified ones.

[[Add a hygenic macros reference somewhere.]]

An alternative approach which avoids the clumsiness of a separate language
of limited expressiveness is to make the macro language more
powerful\,---\,perhaps even using the language itself via constructs
evaluated at compile time rather than run time.  (The macro systems of
Common Lisp and Scheme, and their descendants~\cite{WeiseC93}, take this
approach.)  An extreme example would be to provide a full-fledged
reflection capability.  Such an approach is highly general, powerful, and
theoretically clean; it circumvents many of the limitations of Cpp,
though it does not necessarily support all of Cpp's low-level features.
However, this approach may degrade rather than improve programs
understanding.  As difficult as it may be to determine what output a
macroless program produces, it can be just as difficult simply to determine
the text of a program which uses such macros.  (This is also a problem with
meta-object protocols, aspect-oriented programming, and intentional
programming, all of which permit the programmer to specify transformations
on other parts of the source code.  In practice, such systems are used in
fairly restricted ways, perhaps because other uses would be too
complicated.)  A dialog among users, compiler writers, tool writers, and
language theorists is necessary when introducing a feature in order to
prevent unforeseen consequences from turning it into a burden.


%% See slide 15 and its notes from Mike's quals talk.
% \subsection{Future work}
% 
% These results suggest a wide variety of future avenues for research, both
% in terms of expanding our understanding of uses of the preprocessor in
% practice and in addressing the issues identified by this study.


\section{Related work}
\label{sec:related}

We could find no other empirical study of the use of the C preprocessor nor
any other macro processor.  However, we did find guidance on using C macros
effectively and tools for checking macro usage.

Carroll and Ellis state that ``almost all uses of macros can be eliminated
from C++ libraries''~\cite[p.~146]{Carroll95}.  They list eight categories
of macro usage and explain how to convert them into C++ mechanisms.  They
do not discuss automatic conversion, but focus on instructing the software
engineer on better ways to do Cpp-like things.

Similarly, a number of organizations provide hints about effective ways to
the use the C preprocessor.  The GNU documentation discusses
a set of techniques including simple macros, argument macros, predefined
macros, stringization macros, concatenation macros, and undefining and
redefining macros.  It also identifies a set of ``pitfalls and subtleties
of macros''; these are much like some of the problems our analysis tool
identifies.

We discovered that these categorizations sometimes focussed on constructs
that don't happen very often or missed ones that are actually frequent.
Our effort not only categorizes problems, but it also determines the
frequency of appearance of those problems and discovers other idiosyncratic
uses.

A number of tools check whether specific C programs satisfy particular
constraints.  The lint program checker [[reference]]
checks for potentially problematic uses of C\@.  
[[This is irrelevant, unless you're going to claim that it does a worse job
than one would like.]]
The implementation of lint
is complicated by the fact that it tries to replicate significant functions
of both the C compiler and the preprocessor.

LCLint performs many of lint's checks and also
allows the programmer to add annotations which enable additional
checks~\cite{Evans-pldi96,Evans-fse94}.
LCLint optionally checks function-like
macros\,---\,that is, those which take arguments\,---\,for
macro arguments on the left hand side of assignments, for statements
playing the role of expressions, and for consistent return types.
%\begin{quote}
%$\ldots$ a parameter to a macro may not be used as the left hand side
%of an assignment expression $\ldots$, a macro definition must be
%syntactically equivalent to a statement when it is invoked followed by
%a semicolon $\ldots$, the type of the macro body must match the return
%type of the corresponding function $\ldots$\footnote{From Section 8 of
%David Evans's LCLint User's Guide, Version 2.2 (August 1996); larch-www.lcs.mit.edu:8001/\discretionary{}{}{}larch/\discretionary{}{}{}lclint/\discretionary{}{}{}guide/\discretionary{}{}{}guide.html}
%[FIX: That footnote should really be a reference instead.]
%\end{quote}
%[Fix: that quotation is really easy to nitpick:
% 1) assignment parameters is fine (just turn into a reference argument) but
%    assignment of non-parameters that aren't at global scope is quite bad.
% 2) in x=foo(); we do NOT want foo() to be a statement
% 3) a macro doesn't have a single type, but may have many polymorphic types
%Should we mention these things?  I don't want to seem nitpicky or petty.]
LCLint's approach is prescriptive: programmers are encouraged not to use
constructs that might be dangerous, or to change code that contains such
constructs.  We are more interested in analyzing, describing, and
automatically removing such uses so that tools can better process existing
code without requiring human interaction or producing misleading results.

%% FIX: If we could also list the platforms for which each can compile,
%% that would be great, but I doubt the benefit is worth the effort for now

Krone and Snelting use mathematical concept analysis to determine the
conditional compilation structure of code~\cite{Krone94}.  They determine,
for each line, which preprocessor macros it depends upon, and display that
information in a lattice.  They do not determine how macros depend upon one
another directly, only by their nesting in {\tt \#if}, and the information
conveyed is about the program as a whole.


\begin{figure}
\centerline{\epsfig{file=fig/cat-numargs.eps,height=7.5in}}
\caption{Where to put this?  Is it worth including at all?}
\label{fig:cat-numargs}
\end{figure}


% Not really right:  Don't want the ``References'' section head to be small.
{\small \bibliography{evil}}

\end{document}
