Introduction

The C preprocessor is viewed as a source of difficulty for understanding
and transforming C programs -- most tools just preprocess, which has
undesirable consequences.
Indeed, in the worst case, this makes determing the program text as
difficult as determining the output of an ordinary program.
In order to assess the practical difficulty of understanding uses of CPP
(and the potential for replacement by other language constructs), we
examined actual CPP use in 30 programs comprising approximately 2 million
lines of source code.

We identified and investigated a number of potential sources of complexity
in understanding and translating CPP, including:
 * high total use
 * complicated bodies
    * categories -- what it expands to (ie, the syntactic/parsing category)
    * properties -- what it uses internally (eg, stringization, pasting)
 * multiple definitions per name (and potential inconsistency)
 * high usage
 * where used (control, replacement)
 * mixed usage in conditional (control)
 * highly dependent
    * line dependent on many macros
    * macro controlling many lines
    * cppp experiment

One orthogonal point:
 * variation in use (no obvious point of attack) -- true throughout.
     No pattern according to package size, relative or absolute cppp use, etc.

Methodology
  30 packages, mix of purposes, styles, projects, authors, sizes,
	interactive vs. not, graphical vs. not.  Nearly 2M loc;
	nearly 1.4 M ncnb loc (this omits anything in #if 0, also)
  Ran "configure" or equivalent on each one, in order to set up for
	compilation (but did not attempt to compile).  This creates certain
	header files (such as configure.h).
  Examined all C files in a package, plus anything they included.  In
	general, the C files are those with .c and
	.h (and other...) extensions, but some such files don't actually contain
	C code, and some files had other extensions (such as .def for some
	include files).
  We also analyzed all available headers, but we report only on the macros
	defined in the package, on the theory that macros in libraries are
	well-behaved and need no special attention from the programmer.
	(Sadly, we discovered this not to be true in practice; but the
	programmer generally has no control over libraries and their header
	files; so we concentrate on what the programmer can control.)  Most
	figures report only on package-defined macros; most numbers increase
	dramatically if all macros are included.
  Collection of scripts, approximately 11,000 lines of Perl (7,000 NCNB lines)
  The raw data, and the programs used to generate and manipulate them, are
	available from the authors.

Total usage
 high total use: 9.6% cpp use is very high
   one in ten noncomment, nonblank lines is a preprocessor directive
	(this doesn't count the 28% of lines which expand a macro or the
	38% of lines whose inclusion is controlled by #if)
 for gzip, remind, and bash, the numbers are 22, 21, and 16%
	say a few words about these
 about 1/3 of directives are definitions
 about 1/2 of directives are conditionals
 The remainder are mostly inclusion
 "line" comes only from post-processed source included for users who don't
	have lex, yacc, etc.  
 All other directives account for .017%, or one in six thousand, directives.
 Notice the lack of uniformity; if every package relied equally on
	definitions vs. conditionals, then each group of bars would look
	just like the "total" group.  In particular, use of #include is
	independent of overall use.

Definitions
 * Complicated bodies
    * What it expands to
	We categorized macro bodies into 28 categories; for the purposes of
	  this paper, we coalesce these into ten higher-level categories.
	  Say what they are.
	For each package, we report how many definitions create an
	  expansion which falls in each category
	There's no patern, again.  (Nor is there a pattern by package size
	  or by type of application.)
    * Tricky cpp uses: things slightly out of the ordinary (ie, outside the
	  scope of C, which is why the preprocessor was used), hard to translate
	About one in six macros (16.8%) falls into at least one category
	We show an exact breakdown by subsets because, if we report by
	  property, the numbers don't add to 100%: sometimes a macro has
	  multiple properties.
	Not as much use of stringization, pasting as we anticipated -- we
	  had been particularly worried about them.
	We didn't analyze whether the free variables are globals or locals,
	  which could substantially impact understanding/translation.
	The uses of types are going to be problematic.
    * CPP errors (or poential errors): lint output
	We identified a number of potentially dangerous programming
	  constructs -- those which may well be intentional, but which also
	  might cause a macro not to behave as anticipated (and not to
	  behave like an ordinary function).  One fourth of all definitions
          triggered at least one such warning; and across macro names,
	  one fourth contain a definition which triggers a warning.
	In some, but not all, C dialects there ways to declare a local
	  variable so that each formal only need be used once.
	We didn't examine every use.  Besides, the point isn't just the
	  current uses, but also future uses:  we want these to be easy for
	  programmers to use.
	We also discovered a number of files which begin or end inside a
	  brace scope or an #if scope.  Some of these were intentional, while
	  others are bugs (such as, in one case, a failure to close a /* */
	  style comment) which were apparently not discovered because testing
	  did not build the package under all possible circumstances.
	Also warnings for unexpected indentation and more.
	Also some bad constructs:
	  #undef GO_IF_INDEXABLE_BASE(X, ADDR)
	  #module

 * Multiple (potentially inconsistent) definitions per name
    * Multiple definitions
	When classifying a name (as opposed to a definition), "failed
	  classification" includes "two different defs fell into different
	  categories".  (We could also get that even if a single
	  definition, earlier, if it expands to just such a macro.)
	  Explain how we combine two different categories into a single one.
	Not C code tends to be for the benefit of Makefiles; compilation
	  lines, library filenames, and such tend to differ for each
	  operating system.  (We see those macros because: 1. we process
	  all the .h files, and 2. some files are used for preprocessing
	  both code and Makefiles, etc.  This latter use is problematic in
	  itself, because by definition CPP is supposed to get a syntactic
	  C program; it should err if what's between single quotes isn't a
	  legal character constant, etc.  But many implementations don't
	  perform any such checks.)
	Syntactic is parentheses, commas, semicolons, critical section
	  delimiters, etc:  frequently-used.
	It stands to reason that "failed classification" has multiple
	  definitions, because when determining the category for a macro
	  name, we categorize each definition, and if any of them are
	  either failures or have different categories, then the classification
	  for the name is "failure".  (See below, "inconsistent definitions".)
	Few for unknown symbol, because if any def isn't unknown symbol,
	  then neither is the entire macro.
	By package (not pictured):  most follow the "Mean" curve.
	  Half of packages have no macros defined more than 14 times.
	  Notable exceptions to the rule:
	    bc:  all macros are defined either one or two times
	    remind:  10% of macros are defined between 11 and 15 times
		(none defined 16 or more)
	    gcc: over 4% of macros are defined more than 20 times
		gcc goes 99.5% at 50 definitions
		i.e., 99.5% of macro names are defined 50 or fewer times.
		1 macro, CPP_PREDEFINES, is defined 181 times (and undefined
		  198 times)

	(Talk about the new table which summarizes def, ddf, and use
	  frequency.  Not sure what to say about it yet.)

    * Multiple distinct definitions
	This uses a less strict rule than that used by CPP when determining
	  whether to issue a warning about redefinition.  We eliminate all
	  comments and whitespace, canonically rename all formal arguments,
	  and compare all character and string literals to be identical.
	  Thus, it is a lower bound on the number of truly distinct
	  definitions, much as the previous chart was an upper bound.
	Even when two distinct definitions are identical, if they appear in
	  different locations then it is more likely that something used by
	  that definition has changed.

    * Inconsistent definitions
	Even among macros with multiple definitions, those different
	  definitions generally fall into the same, or compatible, categories.
	The arrows indicate macros for which the names would be classified
	  as "failure".
	Two anomalies are due to the fact that "statement" category should
	  really be called "statement-related":  it includes full
	  statements (which comprise the majority, I think), statements
	  missing a final semicolon, partial statements, multiple
	  statements, multiple statements plus a partial one.
	  * no arrow on one "expression + statement", at 1.1%, because that
		was actually "expression + semicolonless_statement" -- and
		an expression plus a semicolon also makes a statement.
		For instance, the expressions could have been function calls.
		The other, just below it, has slightly fewer occurrences
		(but both round to 1.1%; it also contains "statement", which
		is incompatible with "expression").  Also compare the similar
		duplication at .26% and .10%.
          * arrow on just "statement" at .35%:  was actually
		"semicolonless_statement semicolonless_statements statement"
	The "symbols+expression" results from expression + function_name,
	  also expression + symbols (where first one is macro that expands
	  to partial expression).  [This isn't common at .64%, but is still
	  one of the most frequently occurring failures.]

Uses
 * high usage
	Give average uses per defined macro name (see new summary table).
	In the chart, a higher line indicates less use.
	Half of all macros are used two or fewer times (!).  12% not used
	  at all (!).
	Syntactic macros tend to be used far more often; also type-related
	  ones.  These make sense, as they are frequently-used constructs
	  that permeate code (appear at every definition, for instance).
	Non-C-code, null defines (ie, every definition of the name is a
	  null define), and constants are used least.  The latter is a bit
	  surprising, but shows that macros are being used as a
	  configuration mechanism rather than a linguistic mechanism.  (Do
	  I buy that?  Maybe not. ??)
	All packages follow approximately the same curve; the outlier is
	  gnuplot.  Gnuplot uses macros less, on average, than other packages
	  do.  Over 40% of the macros it defines are never used at all --
	  say why.
 * number of arguments
	You might wonder whether macros are used like functions (taking
	arguments) or like constants (taking no arguments).  We graphed
	that.  This seems irrelevant to me; I don't see where to fit it in,
	or what to say about it.
 * inconsistent usage
    Macros have two general purposes:  they can control the inclusion of
	lines of code (by appearing in a #if condition that controls that
	line) or can change the text of a line (by being expanded on that
	line).  Each of these uses can be modeled by C++ language features
	-- conditionals or (for many types of substitution) consts and
	inlines.  But when a macro is used in both ways, then there's no
	one C++ language feature.  (Additionally, it's harder to understand
	and to analyze a macro that's used in both ways than one that is
	only used in one way or the other.)
    We split macro uses into three categories:  
	* uses in C code, where the macro's expansion controls textual
	    replacement
	* uses in #if, #ifdef, #ifndef, #elsif conditions
	* uses in a macro body, which eventually bottom out to one of the
	    others (unless that macro is never used...)
      We discarded uses in CPP conditionals whose only purpose was to
	prevent redefinition.  (More specifically, if the condition tested
	only definedness, the next line defined the macro just tested, and
	the line after that ended the CPP conditional.  This is a bit
	overrestrictive, but conservative, and in practice fairly accurate.)
      Uses in a macro body generally are intended to be used however the
	containing macro is.  Since those uses also appear in this chart,
	we did not attempt to track each such use in a body to some macro's
	appearance in either code or a conditional.  Thus, the interesting
	categories are "code", "conditional", "macro", "code and
	conditional", and "no use".
      Macros are used more frequently to expand code than to control its
	inclusion, by a factor of ten to one.  (However, each inclusion use
	can control many lines of code; see the dependence section, below.)
      A surprising number -- nearly 12% -- of macros defined in a package
	are never used at all.  Sometimes this is a result of shipping a
	standard set of headers with the package -- it's like a library for
	that development team, but one that can't be counted upon to exist
	everywhere, so it has to be provided.  For gnuplot, over 40\% of
	macros are never used because the package's support for several
	terminal types, such as tgif, is unfinished (and thus unused).
	Even discounting that package, though, the numbers are remarkably
	high.  We would be surprised if one in eight functions and
	variables in a package were never used, not even in testing code.
	[David, do you have any idea what fraction this is in practice?]
	(The percentage of macros defined in libraries/standard header
	files which are never used in the code is enormous, but that is
	expected.)
      The potentially problematic uses are those for which a macro appears
        both in code and in a conditional; these comprise only 3.4% of all
        macro names.
      The numbers don't sum to 100% because of rounding.

      Across packages, there is very little variation.  Packages which use
        the preprocessor sparingly are as likely to have a high percentage
        of mixed usage as packages which make heavy use of CPP.  (There is
        a very slight tendency for the less aggressive packages to have
        more uses in code, fewer uses in conditionals, and fewer macros
        that are never used.)
 * mixed usage in conditional (control)
    Conditionals are used to check for a bunch of things (examples); but a
      single conditional generally checks just one of these categories.  We
      counted the number in each, and also the number that checked symbols
      falling in multiple categories.
    Few mixed categories.
    Lots of variation overall.
    The macros with mixed usage (especially here, but also in the previous
      section) are akin to Krone & Snelting's anomalous macros that
      interfere in the lattice.
    What else to say about this chart?

Dependences
 Two types of dependence:  control/inclusion, and substitution/expansion
     This includes macros which control its inclusion via conditional
     compilation, macros that are invoked on the line, and macros that
     control the definitions of, or are called by, directly invoked macros
 Explain this.  Explain how we compute it transitively.
 The dependence charts omit Emacs and Mosaic; the full dependence
   information for each overran the virtual memory available on the machine
   on which we ran our experiments.  This was in large part due to
   dependences in libraries; the Motif library is a particular problem.
   (We did compute dependence information for Plan, which is
   the third of our 30 packages which uses the Motif library.)  So these
   numbers are smaller than they might be with more complete information.

 * line dependent on many macros
    [Gloss over what kind of line; I think it's a physical line, but each
      physical line is coalesced with other parts of its logical line for
      the purpose of computing dependence info.]
    We computed, for each line, the number of macros it depends on.
    Describe the matrix:  ExE=E, Cx?=C, ?xC=C; plus, we sum across
      conditional compilations and alternate definitions.
    The log scale is shifted by unity in order to place 0 on the scale; most
      58% of lines have no dependence on macros at all.  (Most lines that
      don't expand any macros and appear in C files fall into this category.)
    Overall, 28% of lines expand a macro -- that's quite a few.
    5% of all lines depend on at least 134 macros.
    On average, each line is expansion-dependent on .040 macros,
      inclusion-dependent on .31 macros, and any-dependent on .34 macros.
      (These numbers don't add up because a line may be both expansion and
      inclusion dependent on a macro, but that macro is only counted once
      in the "any dependence" number.)

	Greg wondered which line of gcc is expansion-dependent on the
	expansions of 187 macros.  (This is an outlier:  of the 297760 lines in
	gcc, only 13 are expansion-dependent on more than 100 macros -- all of
	which fall in the range of 159 to 187 such dependences.)

	It's this one, whose mere appearance in the final source is
	dependent on 182 macros (not such an outlier: over 10,000 lines are
	inclusion-dependent on more than that many macros):

	gcc-2.7.2.1/explow.c:430: dependences incl=182; exp=187; either=356
	      LEGITIMIZE_ADDRESS (x, oldx, mode, win);

	LEGITIMIZE_ADDRESS is defined 30 times in gcc, and one randomly-selected
	definition was 37 line long (and chock full of other macro invocations).

 * macro controlling many lines
    We combined these charts for the different packages, because the shapes
      of each were quite similar for each package.  In order to permit
      combining charts across quite different package sizes, we computed
      values that were percentages of package size.
    Each bar represents all macros that control less than the bar label
      percent of the lines in its package (but more than the previous
      label).  For instance, the .17 bar in the red chart indicates that
      there are 286 macros that each control between .11% and .17% of the
      entire package containing that macro.

    * expansion
	This exponential decay indicates that, as expected, there are more
	  macros which control just a few lines and fewer macros that
	  control a lot of lines.
	Outliers (> 6% of all lines expand):
	  int in gcc (14.85% !)
	  NULL, ANY, object in python
	Above 5%: 
	  SvANY in Perl
	  const in RCS
	  ip in workman -- bogus, as defined just twice, then undefined;
		  most places it is a formal parameter and out of the scope
		  of the macro definition.
	  ArgCount, Args in xfig
	  rtx in gcc (defined to int or int*)
    * inclusion
	For each package, the graph is bimodal (so much so that this even
	  shows up a bit on the combined chart; it's more marked in the
	  individual packages, I think).  Most macros control inclusion of
	  no, or very few, lines; but quite a few control a substantial
	  fraction of the package (around 10%).
	All the headviest dependences are on header files (for instance,
	  H_PERL controls inclusion of over 53% of Perl's lines).
	[It would have been intersting to run these numbers for everything
	  but include file multiple inclusion prevention macros.]


 * cppp experiment
    [After the above, this experiment doesn't sound like such a smart thing
      to try any more; its results certainly aren't unexpected.]
    We had noticed in our programming that a particularly heavy use of the
      preprocessor is to handle mutiple dialects of a language.  These uses
      tend to be less structured:  they don't have a simple pattern.  And
      this work must be done in the preprocessor:  there's no hope of
      integrating it into the language.  So we performed an experiment to
      see whether standardizing on a single language would reduce
      dependences, failed classifications, etc.
    We built a CPPP partial evaluator (called cppp) which, given a set of
      macros known to be defined or undefined (and, optionally, their
      expansions), eliminates all possible CPP conditionals.  We defined
      all the macros that can be depended on if using ANSI standard C or
      C++ with POSIX-compliant libraries, preprocessed all the source (and
      all library header files), and reran our experiments.
    The results were disappointing:  while some numeric measures of how
      complicated the resulting program's dependences are decreased, most
      remained at about their previous level.  The number of multiple
      definitions of macros, and the number of failed classifications, did
      not decrease as much as anticipated.
    From this we can conclude that there is no one obvious single point of
      attack:  even eliminating what seems most prevalent to us doesn't
      make a sufficient difference.

Discussion
  Related work
    LCLint: core dumped
      describe methodology

Conclusion
  Future work
    Compare uses to definitions, to see if the lint problems are real or not.
    Do classification based on expansion, which can eliminate some failures
	(because after a few levels of expansion, what "symbols" means may
        become obvious).
    Do classification based on uses (eg, if every call passes a type, then
	treat that parameter as a type even if it might not be.

  Advice for cppp users

===========================================================================

Take a stand about whether it's hard or not, and whether it's worthwhile or
not.  (We should conclude that it's not as hard as we thought it would be,
though we had to do a fair amount of engineering and examination of data to
find that what initially appeared problematic really wasn't after all.)
