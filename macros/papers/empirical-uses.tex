% $Id$
\documentclass[11pt]{article}
\usepackage{epsfig}
\usepackage{dcolumn}

\def\numpackages{27}
\def\numlines{1.2 million}

% the "fullpage" package does almost the same thing
% as the below lines-- it doesn't make things quite as
% tall or wide, but is generally what I use
% \usepackage{fullpage}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt
\topmargin   0pt
\textwidth   6.5 in
\textheight  8.5 in

\renewcommand{\floatpagefraction}{.8} %default .5
% Avoid putting all figures at end of text.
\renewcommand{\textfraction}{.1}  % .2 is the default
\renewcommand{\topfraction}{.9}   % .7 is the default

\begin{document}
% \bibliographystyle{plain}
\bibliographystyle{alpha}

\title{An Empirical Analysis of C Preprocessor Use}

\author{Michael Ernst%
  \and Greg J. Badros%
  \thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.}
  \and David Notkin}

\date{Department of Computer Science and Engineering \\
University of Washington \\
Box 352350 \\
Seattle, WA  98195-2350  USA \\
{\small \{{\tt mernst},{\tt gjb},{\tt notkin}\}{\tt @cs.washington.edu}} \\
\today}  

\maketitle

\begin{abstract}

  The C programming language is intimately connected to its macro
  preprocessor.  This relationship affects, indeed generally hinders, both
  the tools (compilers, debuggers, call graph extractors, etc.)\ built to
  engineer C programs and also the ease of translating to other languages
  such as C++.  This paper analyzes {\numpackages} packages comprising
  {\numlines} lines of publicly available C code, determining how
  the preprocessor is used in practice.  We developed a framework for
  analyzing preprocessor usage and used it to extract information about the
  incidence of preprocessor directives, the frequency of macro use and
  redefinition, the purposes of macros (in terms of both definitions and
  uses), and expressibility of macros in terms of other C or C++
  language features.  We particularly note
  data that are material to the development of tools for C or C++,
  including translating from C to C++ to reduce preprocessor usage.
  The results are of interest to language designers, tool writers,
  programmers, and software engineers.
\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is intimately connected to its macro
preprocessor, Cpp~\cite[Ch.~3]{Harbison91}.   C is incomplete without the
preprocessor, which supplies essential 
facilities such as file inclusion, definition of constants and macros, and
conditional compilation.
While disciplined use of  the preprocessor can reduce programmer effort and 
improve portability, performance, or readability, Cpp also lends itself to
arbitrary source code manipulations that complicate understanding of the
program by both software engineers and tools.  The designer of C++, which
shares C's preprocessor, also noted these problems:
``Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders.''~\cite[p.~424]{Stroustrup-DesignEvolution}

\subsection{Coping with Cpp}

Tools\,---\,and, to a lesser degree, software engineers\,---\,have three
options for coping with Cpp.    They may ignore preprocessor directives
(including macro definitions) altogether, accept only post-processed code
(usually by running Cpp on their input), or attempt to emulate the
preprocessor.

Ignoring preprocessor directives is an option for approximate tools (such
as those based on lexical or approximate parsing techniques), but accurate
information about function extents, scope nesting, declared variables and
functions, and other aspects of a program requires addressing the
preprocessor.

Operating on post-processed code, the most common strategy, is simple to
implement, but then the tool's input differs from what the
programmer sees.  Even when line number mappings are maintained, other
information is lost in the mapping back to the original source code.
For instance, source-level debuggers have no symbolic names or types
for constants and functions introduced via {\tt \#define}, nor can tools
trace or set breakpoints in function macros, as they can for ordinary
functions (even those that have been inlined~\cite{Zellweger83:TR}).
As another example, Siff
and Reps describe a technique that uses type inferencing to produce
C++ function templates from C; however, the input is ``a C program
component that $\ldots$ has been preprocessed so that all include
files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
the readability and reusability of the resulting C++ templates.  As
yet another related example, call graph extractors generally work in
terms of the post-processed code, even when a human is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
leave the software engineer responsible for inferring the mapping between the
original and the post-processed source, which is an undesirable and
error-prone situation.

A tool that first preprocesses code, or takes already-preprocessed code as
input, cannot be run on a non-syntactic program or one that will not
preprocess on the platform on which the tool is being run.  These
constraints complicate porting and maintenance, two of the situations in
which program understanding and transformation tools are most likely to be
needed.  Additionally, a tool supplied with only one post-processed
instantiation of the source code cannot reason about the program as a
whole, only about that version that results from one particular set of
preprocessor variables.  For instance, a bug in one configuration may not
be discovered despite exhaustive testing of other configurations that do
not incorporate particular code or do not admit particular execution paths.

The third option, emulating the preprocessor, is fraught with difficulty.
Macro definitions consist of complete tokens but need not be complete
expressions or statements.  Conditional compilation and alternative macro
definitions lead to very different results from a single original program
text.  Preprocessing adds complexity to an implementation, which must trade
off performing preprocessing against maintaining the code in close to its
original form.  Extracting structure from macro-obfuscated source is not a
task for the faint-hearted.  Despite these problems, in many situations
only some sort of preprocessing or Cpp analysis can produce useful answers.

All three approaches would be unnecessary if programs did not use
preprocessor directives.  This is exactly what Stroustrup suggests:
\begin{quote}
  I'd like to see Cpp abolished.  However, the only realistic and
  responsible way of doing that is first to make it redundant, then
  encourage people to use the better alternatives, and {\em then\/}\,---\,years
  later\,---\,banish Cpp into the program development environment with the
  other extra-linguistic tools where it
  belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}
C++ contains features\,---\,such as constant variables, inline functions,
templates, and reference parameters\,---\,that obviate many uses of Cpp.
Thus, translation to C++ is another path for partial elimination of Cpp.
This study indicates the
feasibility\,---\,and our framework for analyzing preprocessor usage
provides a basis for the development\,---\,of an automatic translator with
two attractive properties.  It would take as input C programs complete with
preprocessor directives, and it would map many\,---\,preferably
most\,---\,uses of directives into C++ language features.  (It is not
practical to eliminate all uses of Cpp.  For example, C++ currently
provides no replacement for the {\tt \#include} directive, or for
stringization or pasting.  Macros that cannot be eliminated might be
annotated with their types or 
effects on parser or program state, so that even tools that do no Cpp
analysis can operate correctly on such programs.)

Another niche already filled by our tool is that of a ``macro lint''
program which warns of potentially dangerous uses of Cpp.

%O'Callahan and Jackson also use type
%inference, although for program understanding rather than translation;
%they, too, apply their techniques to post-processed
%code~\cite{OCallahan-icse97}.

\subsection{Cpp: not all bad}

Despite its evident shortcomings, Cpp is a useful and often necessary
adjunct to C, for it provides capabilities unavailable in the language or
its implementations.  Cpp permits definition of portable language
extensions that can define new syntax, abbreviate repetitive or complicated
constructs, or eliminate reliance on a compiler implementation to
open-code (inline) functions, propagate symbolic constants, eliminate dead
code, and short-circuit constant tests.  The latter guarantees are
especially valuable for compilers that do a poor job optimizing or when the
programmer wishes to override the compiler's heuristics.  Cpp also permits
system dependences to be made explicit and tested, resulting in a clearer
separation of concerns.  Finally, Cpp permits a single source to contain
multiple different dialects of C; a frequent use is to support both
K\&R-style and ANSI-style declarations.

%% NEED A REFERENCE TO DEBUGGER HERE!
%% also mention Emacs hide-ifdef mode

A limited number of tools do exist to assist software engineers to
understand code with containing Cpp directives, such as debuggers that can
call {\tt \#define}d functions and editors that support viewing one
particular configuration of the code.

Our long-term goal is not to take these useful features away from
programmers, but to reduce Cpp use, making programs easier for both humans
and tools to understand.


\subsection{The analyses}

To build a better understanding of how the preprocessor is used,
we wrote tools to analyze preprocessor usage and ran them on {\numpackages}
C packages comprising {\numlines} lines of code.  Figure~\ref{fig:packages}
describes the packages and lists their sizes in terms of physical lines (or
newline characters) and non-comment, non-blank (NCNB) lines, which
disregards lines consisting of only comments or whitespace.  The remainder
of the analysis uses only the NCNB length, which more accurately reflects
the amount of source code.

\begin{figure}
\centering
{\small
  \setlength{\tabcolsep}{.25em}
  \input{package_and_sizes-tbl.tex}
}
\caption{Analyzed packages and their sizes}
\label{fig:packages}
\end{figure}


Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of C
programming support tools.  On the other hand, the analysis also convinces
us that, by extending our analysis framework with some class type
inferencing techniques (similar to those used by Siff and Reps for C to C++
translation~\cite{Siff-fse96}, O'Callahan and Jackson for program
understanding~\cite{OCallahan-icse97}, and others), we can take significant
steps towards a tool that usefully converts a high percentage of Cpp code
into C++ language features.\footnote{Preliminary results indicate that many
  C++ packages rely heavily on Cpp, even when C++ supports a nearly
  identical language construct, probably due to a combination of trivial
  translations from C to C++ and of C programmers becoming C++ programmers
  without changing their habits.} We are interested not in translations
that merely allow a C program to be compiled by a C++ compiler (which is
usually easy, by intentional design of C++) but those that take advantage
of the added richness and benefits of C++ constructs.

In terms of the complexity of preprocessor usage, the results reported here
contain both good news and bad.  By far
the largest number of macro definitions and uses are relatively simple, of
the variety that a programmer could understand without undue effort (although
perhaps requiring tedious work) or that a relatively unsophisticated tool
could understand (although in practice very few even try).  Despite the
preponderance of innocuous macros, the preprocessor is so heavily used that
the remaining ones are numerically significant.  It is precisely these
macros that are mostly likely to cause difficulties, and there are enough
of them to be problematic in practice and to make the effort of
understanding, annotating, or eliminating them worthwhile.

%% The tool is completely uninteresting; why would anyone care?
% We wrote a collection of Perl scripts to produce and analyze these data.
% As we developed them, we slowly shifted from a lexical analysis of the
% source to a syntactic analysis of the source.  For example, we now properly
% parse virtually all declarations in every package.  We're pursuing even
% more analysis, pushing into semantics.  For instance, we can now determine
% which identifiers are free variables within a defined macro.  We are also
% close to inferring the types of macro bodies, one of several steps
% necessary before we can attempt to build a converter from C to C++ which
% eliminates most macro usages (except for {\tt \#include}).  Such a
% translator would both enable programmers to shift to C++ and its constructs
% more easily and simplify the development of source-level debuggers,
% call graph extractors, and other programming support tools.



%[FIX: We catch errors that you miss when you look at post-processed code
%since we look at all branches.]
%
%[FIX: Harsh macro examples?  They're in the categorization section, but
%maybe give some hints here too.]


\subsection{Who cares?}

The results of this research are of interest to language designers, tool
writers, programmers, and software engineers.

Language designers can examine uses of the macro system's extra-linguistic
capabilities to determine what programmers consider missing from the
language.  Future language specifications can support (or prevent!)\ such
practices in a more disciplined, structured way.

[[[Think more about this:  Also, how do language choices lead to more/less
tightly integrated (as opposed to open, component-based) environments?
E.G., no need for \verb|__LINE__| in Java?]]]

Programming tool writers, too, need to understand how Cpp is used, for that
sheds insight on the sorts of inputs that will be provided to the tool.  By
coping with the most common constructs, the tool can provide relatively
good coverage for low effort.  By identifying problematic uses, much better
feedback can be given to the programmer, who can be more effective as a
result.  The analysis results also indicate the difficulty of processing
preprocessor directives; before these analyses, we did not know whether the
task was so trivial as to be uninteresting, so difficult as to be not worth
attempting, or somewhere in between.

The analyses are of interest to programmers who wish to make their code
cleaner and more portable, and can help them to avoid constructs that cause
tools (such as test frameworks and program understanding tools)
to give incomplete or incorrect results.

% Also, learn weird new Cpp tricks!

Finally, our results are of interest to software engineers for all of the
above reasons and more.  Since this is the first Cpp usage study of which
we are aware, it is worth performing simply to determine whether the
results were predictable a priori; we did in fact discover a number of
interesting features of our suite of programs.


\subsection{Outline}

The remainder of this paper is organized as follows.

Section~\ref{sec:directives} reports the percentage of original C source
code lines that are preprocessor directives, including a breakdown of the
frequency of specific directives such as {\tt \#define}.  C programs
commonly have preprocessor directives as over 10\% of their total lines,
and over 20\% of the lines were directives in 3 of the {\numpackages}
packages.

Section~\ref{sec:usage} reports how often each macro is defined and
expanded.  In general identifiers are {\tt \#define}d relatively few times
(96\% of macro identifiers had three or fewer definitions).  Many packages
also have a significant number of macros that are never expanded, even
disregarding system and library header files.

Section~\ref{sec:categorization} categorizes macro definitions according to
their expansions; for example, macros may simply define a preprocessor
symbol, define a literal, expand to a statement, etc.  We were particularly
interested in determining the frequency of use of macros that are difficult
to convert to other language features, such as those that string together
characters as opposed to manipulating lexemes or syntactic units (less than
one third of one percent of all macro definitions),
those that expand to partial syntactic units such as unbalanced
braces or partial declarations (half of one percent), and others not 
directly expressible in the programming language (about four percent).

Section~\ref{sec:related} discusses related and future work and concludes.


\section{Occurrence of preprocessor directives}
\label{sec:directives}

Figure~\ref{fig:directives-breakdown} shows how often preprocessor
directives appear in the programs we analyzed.  Each group of bars in the
figure represents the percentage of NCNB lines attributed to the specified
category of directives, with each individual bar showing the percentage for
a specific package.  Conditional compilation directives are grouped together, as
are ``other'' directives (such as {\tt \#error} and {\tt \#pragma}).  These
numbers do not include Cpp directives discovered in system header files,
only in files included in the package.

% Conditional compilation =  ({\tt \#ifdef},
% {\tt \#ifndef}, {\tt \#if}, {\tt \#endif}, {\tt \#else}, and {\tt \#elif})

\begin{figure}
\centerline{\epsfig{file=directives-breakdown.eps,height=7.5in}}
\caption{Preprocessor directives as a fraction of non-comment,
  non-blank (NCNB) lines.}
\label{fig:directives-breakdown}
\end{figure}

Overall, more than 10\% of NCNB program lines are preprocessor directives;
the percentage varies by a factor of five across packages.  Half of
the packages
have directives for over 9\% of their lines, and one in nine exceed 21\%,
indicating quite heavy use of the preprocessor.

% Figure~\ref{fig:total_directives} presents the same data arranged by
% package, to show the percentage of the total directive count attributed to
% each category of directives.

% package other   line    cond.   include undef   define  total
% Average 0.028   0.169   4.661   1.944   0.202   3.080   10.084
% (mapcar (lambda (x) (/ x 10.084)) '(0.028 0.169 4.661 1.944 0.202 3.080))

Conditional compilation directives account for just under half (46\%) of
the total directives in all packages, macro definitions comprise another
31\%, file inclusion is 19\%, macro undefinition makes up 2\%, and the
other directives are in the noise.  The directive breakdown varies by quite
a bit across packages: the percentage of {\tt \#define} varies from 14\% to
51\%, the percentage of {\tt \#include}s varies from 4\% to 60\%, and the
percentage of conditional directives varies from 16\% to 74\%.



\subsection{{\tt \#line}, {\tt \#undef}, and ``other'' directives}

The definedness of a macro is often used as a boolean value.  However, our
study shows that {\tt \#undef} is rarely used to set such macros to
``false''$\!$.  Most uses of {\tt \#undef} immediately precede a definition of
the just-undefined macro, to avoid preprocessor warnings about incompatible
macro redefinitions.  (About 90\% of glibc's {\tt \#undef}s are used this
way, and 216 of the 614 {\tt \#undef}s appear in a single file which
consists of a long series of {\tt \#undef}s followed by a single {\tt
\#include}.)

Every use of {\tt \#line} (in bash, cvs, flex, fvwm, gawk, groff, and perl)
appears in lex or yacc output that enables packages build on systems
lacking lex, yacc, or their equivalents.  For instance, flex uses itself to
parse its input, but also includes an already-processed version of its
input specification (that is, C code corresponding to a {\tt .l} file) for
bootstrapping.

The only significant user of ``other'' directives is the g77 package, which
contains 154 uses of {\tt \#error} (representing 1.5\% of all preprocessor
directives and 0.16\% of all lines) to check for incompatible preprocessor
flags.


\subsection{Packages with heavy preprocessor use}

Four packages\,---\,gzip, glibc, remind, and bash\,---\,deserve special
attention for their heavy preprocessor usage.  The first three have
preprocessor directives as 21--23\% of their lines.

gzip {\tt \#define}s disproportionately many macros as literals used as
arguments to system calls, enumerated values, directory components, and
more.  These macros act like {\tt const} variables and are evidence of good
programming style.  gzip also contains many conditional compilation
directives, since low-level file operations (such as setting creation time
and access control bits, accessing directories, and so forth) are done
differently on different systems; gzip is a highly portable program.

glibc's heavy preprocessor use is largely accounted for by {\tt \#include}
directives.  Its files average just 42 NCNB lines each, and most contain
several {\tt \#include} directives.  Of the 1684 files, 182 are header
files consisting of a single {\tt \#include} line, relieving glibc users of
the need to know in which directory a service really resides.

remind supports speakers of many different languages by using {\tt
\#define}d constants for basically all user output.  It also contains
disproportionately many conditional compilation directives; over half of
these test the definedness of \verb|HAVE_PROTO|, in order to provide both
K\&R and ANSI prototypes.

Like gzip, bash is portable across a large variety of systems, but bash
uses even more operating system services.  Ninety-seven percent of bash's
conditional compilation directives test the definedness of a macro whose
presence or absence is a boolean flag indicating what features the current
system supports.  The presence or absence of a feature requires different
(or sometimes additional) system calls or other code.


\section{Frequency of macro definition and usage}
\label{sec:usage}

% The second question we asked was: where and how often are macros
% defined and used in practice?  

%Specifically, for each macro we
%determined:
%\begin{itemize}\itemsep 0pt \parskip 0pt
%
%\item How many times it was {\tt \#define}d.
%\item How many times it was {\tt \#undef}ed.
%\item How many arguments (if any) it takes.
%\item How many times (and where) it was mentioned in C source code.
%\item How many times (and where) it was mentioned in other
%preprocessor directives.
%
%\end{itemize}
%[FIX: gjb: This is redundant with the same information in paragraph
%form, above]

%[FIX: gjb: I'd also like to see info about how often define-d to the same
%expansion, or not, etc.]

% \begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{freq_of_def-tbl.tex}}%
% }
% \caption{Number of definitions per Cpp identifier.  The numbers in the
%   table represent the percentage of identifiers which are defined a given
%   number of times or fewer.  For example, bison contains 4 or fewer
%   definitions for 91.51\% of all Cpp macros it defines, and no
%   macro is defined 5, 6, 7, 8, or more than 9 times.}
% \label{fig:define_count}
% \end{figure}

\begin{figure}
\centerline{\epsfig{file=def-frequency.eps,height=7.5in}}
\caption{Number of definitions per Cpp identifier, graphed as
  the percentage of identifiers that are defined a given number of times
  or fewer.  Overall, 96\% of macros were defined three or
  fewer times; the other 4\% of macros had four or more distinct
  definitions ({\tt \#define} directives).}
\label{fig:freq-def}
\end{figure}

Figure~\ref{fig:freq-def} graphs the number of times each identifier is
{\tt define}d in each of the packages.  No distinction is made between
sequential redefinitions of a macro and multiple definitions that cannot
take effect in a single configuration (say, because they appear in
different branches of a Cpp conditional).

This graph shows a great deal of variation.  For example, all macros
defined by bc have only one or two different expansions, but more than 10\%
of macros defined by remind expand to more than eight different texts.

In all but four packages, at least 93\% of all macros are defined three or
fewer times.  For bash, glibc, and dejagnu, such macros account for 90\%,
largely because these packages are highly portable and also quite dependent
on system libraries.  The remind program uses macro definitions to provide
localization support for ten different natural languages (and multiple
character sets for some of them), accounting for its surprisingly large
number of macros with many definitions.  All of remind's macros are defined
14 or fewer times, but 16 macros in the {\numpackages} packages are defined
more than 16 times, including three with more than 30 distinct definitions.

These data demonstrate that multiple definitions of symbols is not
numerically frequent; even more importantly, the definitions of a symbol
tend to be compatible, as shown in section~\ref{sec:categorization}.


\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{freq_of_use-tbl.tex}}%
% }
\centerline{\epsfig{file=use-frequency.eps,height=7.5in}}
\caption{Number of expansions per Cpp macro.  The numbers in the
  table represent the percentage of identifiers which are expanded a given
  number of times or fewer.  For example, g77 expands 65\% of its
  macros two or fewer times.}
\label{fig:freq-use}
\end{figure}

Figure~\ref{fig:freq-use} is structured as the previous figure, but it
represents the number of times that a defined name is expanded in either
the package (not in system headers).  About 82\% of all macros were
expanded eight or fewer times.

It is notable that most packages contain a significant number of defined
macros that are never expanded.  (Figure~\ref{fig:freq-use} reports only on
macros defined in a package, not those defined in system or library header
files, inclusion of which would push the unused percentage well above
50\%.)  Most packages are in the 4-12\% range, while gnuplot exceeds 40\%.
Although it's difficult to fully account for the larger numbers,
contributing factors include a lot of cut-and-paste and a lack of attention
to implementations for specific platforms in some packages.  Again with
respect to bounding analysis, covering macros with 10 or fewer uses covers
approximately 85\% of the cases.

The tail of this distribution is quite long, indicating that some macros
are used very heavily.  Ninety-nine percent of macros expanded 147 or fewer
times, 99.5\% of macros are expanded 273 or fewer times, 99.9\% are
expanded 882 or fewer times, and python uses {\tt NULL} (which it defines
itself) 4233 times.  Figure~\ref{fig:freq-use} weights each macro equally
rather than weighting each macro use equally, which would weight python's
{\tt NULL} 4233 times more heavily than a macro used only once (and
infinitely more than a macro never used at all).  Only macros defined in a
package, and uses in that package, are counted; system macros and uses are
excluded.


\begin{figure}
\centerline{\epsfig{file=where-used.eps,height=6in}}
%{\small
%  \setlength{\tabcolsep}{.25em}
%  \input{where_uses-tbl.tex}%
%}
\caption{Where macros are used: in C code, in macro definition bodies, in
  conditional tests, or in some combination thereof.  The figure reflects
  only package macros and uses, not system files.}
\label{fig:where-used}
\end{figure}

Figure~\ref{fig:where-used} breaks down macro usage according to whether
the macro invocation occurs in C code, in Cpp code (which is further broken
down into conditional tests and definition bodies), in both, or in neither
(i.e., no uses).  

No package expanded all of its defined macros; two expanded fewer than 70\%
of the defined macros.  The dominant usage was in C code only; these uses
don't, therefore, have any affect on conditional compilation (for example).

In general, packages use macros either to direct conditional compilation or
to produce code, but not for both purposes; this separation of concerns
makes the source code easier to understand.  Only 3.1\% of macros expand in
both code and conditional contexts (the fourth and fifth categories in the
figure; the sixth, macro and code, accounts for only another 0.2% of macros).
Conditional usage is rare in general; conditional compilation accounts for
half of Cpp directives but only 5.4\% of macros (plus the categories just
listed above).



\section{Categorization}
\label{sec:categorization}

This section examines the purposes of macros and how they are intended to
be used, which requires heuristic categorization of macro bodies.  A
straightforward refinement that we are pursuing examines macro uses to aid
this categorization.  (For example, a macro used where a type should appear
can be inferred to expand to a type; a macro used before a function body is
probably expanding to a declarator.)

In addition to classifying each macro as taking arguments or not, our tool
identifies the following specific categories (and a number of more
rarely-used ones omitted for reasons of space;
figure~\ref{fig:subset-categories} contains a fuller list).  The examples
are chosen for clarity and brevity from the packages studied.

\begin{description}
  \sloppy
  \emergencystretch=2em

\item[Null define]  The {\tt \#define} gives only an
  identifier name but no macro body, as in {\tt \#define
  \verb|HAVE_PROTO|}\@.  Such macros appear most frequently in Cpp
directives (such as {\tt \#ifdef}), where they are used as boolean
variables by the preprocessor, but may also appear in code.  For instance,
macro {\tt private} may expand either to {\tt static} or to nothing,
depending on whether a debugging mode is set.

\item[Constant] The macro is defined to be either a literal or an operator
  applied to constant values.  For instance, {\tt \#define NULL 0}, {\tt \#define
  \verb|ARG_MAX| 131072}, and {\tt \#define ETCHOSTS "/etc/hosts"} define
literals, while {\tt \#define \verb|RE_DUP_MAX| ((1<<15)-1)} and {\tt
\#define \verb|RED_COLS| (1 << \verb|RED_BITS|)} (where \verb|RED_BITS| is
a constant, possibly a literal) define constants.  Such macros act like
{\tt const} variables.

\item[Expression]  The macro body is an expression, as in {\tt \#define
  sigmask(x) (1 << ((x)-1))}.  This expression might have a single constant
value everywhere (the usual case for expression macros without arguments,
most of which are classified as constants, above) or might have a
different value on each use (the usual case for expression macros with
arguments).

One tenth of expression macros in our study use assignment operators, which
have potentially unexpected results.  A macro argument that is assigned to
is like a pass-by-reference function argument and need only be noted in the
macro's documentation.  A macro that assigns a global variable also
presents no difficulties in understanding or translation into a C function.
Assignment to a local variable that is free in the macro body, however,
demands that such a variable exist wherever the macro is invoked, and
assigns to different variables at different invocations.\footnote{By
  contrast, LCLint considers assignment to a macro argument dangerous but
  does not appear to check for assignments to local
  variables.~\cite{Evans:LCLint}} Such a macro implements a restricted form
of dynamic scoping by capturing the version of a variable at the point of
macro invocation.

% Computed ``10% use assignment'' from CATEGORIES_NI lines of *.stat files.

% Problem: can't figure out how to get typewriter curly braces in footnote.
\item[Statement]  The macro body is a complete statement such as
  ``{\tt x = 3;}'', ``{\tt if (s) free(s);}'' or ``{\tt \verb|{| int x =
    y*y; printf("\%d", x); \verb|}|}''.  Such a macro is like a function
    returning {\tt void}, except that uses should not be followed by a
    semicolon.\footnote{Since the body is already a complete statement, the
      extra semicolon can cause problems such as mis-parsing of nested {\tt
      if} statements.  Such macros can be confusing to use, because
    programmers are inclined to add a semicolon after invocations that look
    like functions; wrapping the body in {\tt do \{\ldots\} while (0)}, a
    partial statement which requires a trailing semicolon, solves this
    problem.  To our surprise, we found few uses of that construct, but
    many error-prone instances of a macro that expanded to a statement like
    \{\ldots\} in which a call to the macro was immediately followed by a
    semicolon.}

\item[Stringization and pasting]  The macro body contains {\tt \#} or
  {\tt \#\#}, which treat the macro argument not as a token but as a
  string.  Examples include {\tt \#define spam1(OP,DOC) \verb|{|\#OP, OP,
    1, DOC\verb|}|,}, {\tt \#define REG(xx) register long int xx asm
    (\#xx)}, and {\tt \#define \verb|__CONCAT|(x,y) x \#\# y}.  No C or C++
  language mechanism can replace such macros.

\item[Other syntactic macros]  Like stringization and pasting, these
  macros make essential use of the unique features of the preprocessor.
  Our framework separately categorizes a number of such macros, including
  those that expand to a reserved word (such as {\tt \#define private
  static}, mentioned above), those that expand to a delimiter (such as
{\tt \#define AND ;}), and those with mismatched parentheses, brackets, or
braces.  The latter are often used to create a block and perform actions
that must occur at its beginning and end, as for \verb|BEGIN_GC_PROTECT|
and \verb|END_GC_PROTECT|.

\item[Type-related macros]  These macros either take a type as an argument, pass
  a type to another macro, expand to a type or partial type, or use such a
  macro.  Examples include {\tt \#define \verb|__ptr_t| void *}, {\tt
  \#define \verb|__INLINE| extern inline}, {\tt \#define \verb|ALIGN_SIZE|
sizeof(double)}, and {\tt \#define PTRBITS \verb|__BITS|(char*)}.  Since
types are not first-class in C, they may not be passed to functions or
returned as results; additionally, these macros may produce or use only
part of a type (such as a storage class).  As a result, these macros may be
tricky to understand, and cannot be eliminated via straightforward
translation.

\item[Recursive]  The ISO C standard permits macros to be recursively
  defined (the preprocessor performs only one level of expansion), as in
  {\tt \#define LBIT vcat(LBIT)}.  This mechanism permits already-defined
  or to-be-defined macros to be extended or modified.

\item[Classification failure]  Multiple adjacent identifiers\,---\,as in
  {\tt \#define EXFUN(name, proto) name proto} and {\tt \#define
  \verb|DO_OP|(OP,a,b) (a OP b)}\,---\,caused most failures of our
classification heuristics.  Of the 1025 classification failures in the
{\numpackages} packages, 496 were caused by a single definition in gnuplot,
{\tt \#define CUR \verb|cur_term->type.|}, and uses of that macro, as in
{\tt \#define \verb|acs_plus| CUR Strings[408]}.  Four
packages\,---\,bison, gnuchess, remind, and workman\,---\,had no macro
classification failures.  These packages contain 93, 297, 932, and 58 macro
definitions, respectively.

% and illegal identifier
%   names ({\tt \#define \verb|FAT$C_VFC| 3} for VMS compilation), tokens %$HACK
%   ({\tt \#define \verb|LIB_PATH| /usr/ucblib}), and constants ({\tt 1ULL} for
%   {\tt unsigned long long})

%   gnuplot
%   contains only 13 other failures, ghostscript has 130, and no other
%   package stands out with many.

% half appeared in gnuplot;
% gnuplot, cvs, and groff accounted for over 75\% of the failures, many of
% which could be eliminated by slightly relaxing the parsing rules.
\end{description}


Figure~\ref{fig:categorization} shows the percentage of macros that fit
into these categories for each package.  Overall, 83\% of macros are
expressions\,---\,mostly constants; further analysis of the conditional
compilation structure (in the style of Krone and Snelting~\cite{Krone94})
and of the macros with free variables (essentially achieving dynamic
scoping) is needed to see which of the roughly 33\% of expression macros
should be easy to handle in a conversion process.  The 7\% that are null
defines, should also be easy to understand and/or translate.  Another 5\%
are statements, most of which are straightforward (complications include
scoping and semicolon swallowing).  That only 0.2\% of macros exploit
stringization or pasting, the only truly extra-linguistic capabilities in
the C preprocessor, is encouraging.

Our tool failed to categorize less than 2\% of the 26701 definitions;
performing even a single level of macro expansion in bodies would make most
of those failures categorizable.  Other straightforward improvements
include making a second pass after an initial categorization and using
dependence information to determine which definitions can be active at an
invocation site.  We have not pursued these enhancements, primarily because
our tool is already accurate enough for our purposes.


% The following isn't quite enough to get the columns lined up in this table.
% \newcolumntype{d}{D{.}{.}{2}}
% \begin{tabular}{|l|d|d|d|d|d|d|d|}\hline
\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{categories-tbl.tex}}%
% }
\centerline{\epsfig{file=def-categories.eps,height=6in}}
\caption{Categorization of macro definition bodies.}
\label{fig:categorization}
\end{figure}



%In anticipation of the translator tool, the analysis tool infers
%types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
%and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
%type information is in the early stages, however, and we do not report
%on the preliminary results in this paper.

% [FIX: Benefits even from simple literal constant conversion -- exposes
% symbolic information to the debugger]

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.]


\begin{figure}
{\small\centerline{\input{tbl-subset-categories.tex}}}
\caption{Subset categorization of macros (not macro definitions).   Items
  less than one twentieth of a percent are omitted; such items appear
  fewer than ten times in the codebase.}
\label{fig:subset-categories}
\end{figure}
 
Figure~\ref{fig:subset-categories} indicates that multiple definitions of a
particular macro tend to be compatible.  It classifies macros rather than
macro definitions, and uses a finer breakdown of categories.  Each macro is
given a set of categorizations (corresponding to the categorizations of its
definitions) and the incidence of each such is displayed.  For over 97\% of
macros, all of the macro's definitions are given the same
categorization\,---\,even when categories such as literal, constant,
expression, and expression with assignment are considered unrelated.  The
most common ``conflict'', statement and null define, is also harmless in
most contexts.  We expect that closer examination of most of the other
conflicts will demonstrate that they present no real obstacles to
understanding (even if they do complicate some details).


\section{Related work}
\label{sec:related}

We could find no other empirical study of the use of the C preprocessor nor
any other macro processor.  However, we did find guidance on using C macros
effectively and tools for checking macro usage.

Carroll and Ellis state that ``almost all uses of macros can be eliminated
from C++ libraries''~\cite[p.~146]{Carroll95}.  They list eight categories
of macro usage and explain how to convert them into C++ mechanisms.  They
do not discuss automatic conversion, but focus on instructing the software
engineer on better ways to do Cpp-like things.

Similarly, a number of organizations provide hints about effective ways to
the use the C preprocessor.  The GNU documentation, for example, discusses
a set of techniques including simple macros, argument macros, predefined
macros, stringization macros, concatenation macros, undefining and
redefining macros.  It also identifies a set of ``pitfalls and subtleties
of macros''; these are much like some of the problems our analysis tool
identifies.  We discovered that these categorizations sometimes focussed on
constructs that don't happen very often or missed ones that are actually
frequent.  Our effort not only categorizes problems, but it also determines
the frequency of appearance of those problems and discovers other
idiosyncratic uses.

A number of tools check whether specific C programs satisfy particular
constraints.  The lint program checker, distributed with most Unix systems,
checks for potentially problematic uses of C\@.  The implementation of lint
is complicated by the fact that it tries to replicate significant functions
of both the C compiler and the preprocessor.

LCLint performs many of lint's checks and also
allows the programmer to add annotations which enable additional
checks~\cite{Evans-pldi96,Evans-fse94}.
LCLint optionally checks function-like
macros\,---\,that is, those which take arguments\,---\,for
macro arguments on the left hand side of assignments, for statements
playing the role of expressions, and for consistent return types.
%\begin{quote}
%$\ldots$ a parameter to a macro may not be used as the left hand side
%of an assignment expression $\ldots$, a macro definition must be
%syntactically equivalent to a statement when it is invoked followed by
%a semicolon $\ldots$, the type of the macro body must match the return
%type of the corresponding function $\ldots$\footnote{From Section 8 of
%David Evans's LCLint User's Guide, Version 2.2 (August 1996); larch-www.lcs.mit.edu:8001/\discretionary{}{}{}larch/\discretionary{}{}{}lclint/\discretionary{}{}{}guide/\discretionary{}{}{}guide.html}
%[FIX: That footnote should really be a reference instead.]
%\end{quote}
%[Fix: that quotation is really easy to nitpick:
% 1) assignment parameters is fine (just turn into a reference argument) but
%    assignment of non-parameters that aren't at global scope is quite bad.
% 2) in x=foo(); we do NOT want foo() to be a statement
% 3) a macro doesn't have a single type, but may have many polymorphic types
%Should we mention these things?  I don't want to seem nitpicky or petty.]
LCLint's approach is prescriptive: programmers are encouraged not to use
constructs that might be dangerous, or to change code that contains such
constructs.  We are more interested in analyzing, describing, and
automatically removing such uses so that tools can better process existing
code without requiring human interaction or producing misleading results.

%% FIX: If we could also list the platforms for which each can compile,
%% that would be great, but I doubt the benefit is worth the effort for now

Krone and Snelting use mathematical foundations analysis to determine the
conditional compilation structure of code~\cite{Krone94}.  They determine,
for each line, which preprocessor macros it depends upon, and display that
information in a lattice.  They do not determine how macros depend upon one
another directly, only by their nesting in {\tt \#if}, and the information
conveyed is about the program as a whole.


% Not really right:  Don't want the ``References'' section head to be small.
{\small \bibliography{evil}}

\end{document}
