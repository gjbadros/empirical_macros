% $Id$
% macros-paper.tex
\documentclass[11pt]{article}
\usepackage{epsfig}
\usepackage{dcolumn}

\def\numpackages{27}
\def\numlines{1.2 million}

% the "fullpage" package does almost the same thing
% as the below lines-- it doesn't make things quite as
% tall or wide, but is generally what I use
% \usepackage{fullpage}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt
\topmargin   0pt
\textwidth   6.5 in
\textheight  8.5 in

\renewcommand{\floatpagefraction}{.8} %default .5
% Avoid putting all figures at end of text.
\renewcommand{\textfraction}{.1}  % .2 is the default
\renewcommand{\topfraction}{.9}   % .7 is the default

\begin{document}
% \bibliographystyle{plain}
\bibliographystyle{alpha}

\title{An Empirical Analysis of C Preprocessor Use}

\author{Michael Ernst%
  \and Greg J. Badros%
  \thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.}
  \and David Notkin}

\date{Department of Computer Science and Engineering \\
University of Washington \\
Box 352350 \\
Seattle, WA  98195-2350  USA \\
{\small \{{\tt mernst},{\tt gjb},{\tt notkin}\}{\tt @cs.washington.edu}} \\
\today}  

\maketitle

\begin{abstract}

  The C programming language is intimately connected to its macro
  preprocessor.  This relationship affects, indeed generally hinders, both
  the tools (compilers, debuggers, call graph extractors, etc.)\ built to
  engineer C programs and also the ease of translating to other languages
  such as C++.  In this paper we analyze {\numpackages} packages comprising
  {\numlines} lines of publicly available C code, determining the ways in
  which the preprocessor is used in practice.  We developed a framework for
  analyzing preprocessor usage and used it to extract information about the
  indicence of preprocessor directives, the frequency of macro use and
  redefinition, the purposes of macros (in terms of both definitions and
  uses), and how many macros are expressible in terms of other C or C++
  language features.  We report on the analysis (and the supporting
  framework), in particular illustrating data that are material to the
  development of tools for C or C++ such as a tool to reduce usage of the
  preprocessor by translating from C to C++.  The results are of interest
  to language designers, tool writers, programmers, and software engineers.
\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is intimately connected to its macro
preprocessor, Cpp~\cite[Ch.~3]{Harbison91}.   C is incomplete without the
preprocessor, which supplies essential 
facilities such as file inclusion, defining constants and macros, and
conditional compilation which are present in essentially every C program.
While disciplied use of  the preprocessor reduces programmer effort and 
improves portability, performance, or readability, it also lends itself to
arbitrary source code manipulations that complicate understanding of the
program by both software engineers and tools.  The designer of C++, which
inherits C's preprocessor, also noted these problems:
``Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders.''~\cite[p.~424]{Stroustrup-DesignEvolution}

\subsection{Coping with Cpp}

Tools have three options for coping with Cpp.  (Humans usually adopt one of
these approaches, as well.)  They may ignore preprocessor directives
(including macro definitions) altogether, accept only post-processed code
(perhaps by running Cpp on their input), or attempt to emulate the
preprocessor.

Ignoring preprocessor directives is an option for approximate tools (such
as those based on lexical or approximate parsing techniques), but accurate
information about function extents, scope nesting, declared variables and
functions, and other aspects of a program requires addressing the
preprocessor.

Operating on post-processed code, the most common strategy, is simple to
implement, but then the tool's input differs from what the
programmer sees.  Even when line number mappings are maintained, other
information is lost in the mapping back to the original source code.
For instance, source-level debuggers have no symbolic names or types
for constants and functions introduced via {\tt \#define}, nor can tools
trace or set breakpoints in function macros, as they can for ordinary
functions, even those that have been inlined~\cite{Zellweger83:TR}.
As another example, Siff
and Reps describe a technique that uses type inferencing to produce
C++ function templates from C; however, the input is ``a C program
component that $\ldots$ has been preprocessed so that all include
files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
the readability and reusability of the resulting C++ templates.  As
yet another related example, call graph extractors generally work in
terms of the post-processed code, even when a human programmer is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
leave the software engineer responsible for inferring the mapping between the
original and the post-processed source, which is an undesirable and
error-prone situation.

A tool that first preprocesses code, or insists on preprocessed code as
input, cannot be run on a non-syntactic program or one that will not
preprocess (or produces nonsense) on the platform on which the tool is being
run.  These constraints complicate porting and maintenance, two of the
situations in which program understanding and transformation tools are most
likely to be needed.  Additionally, a tool supplied with only one post-processed
instantiation of the source code cannot reason about the program as a
whole, only about that version that results from one particular set of
preprocessor variables.  (For instance, a bug in one configuration may not
be discovered despite exhaustive testing of other configurations that do
not incorporate particular code or do not admit particular paths through
the code.)

[[[EXPAND THIS:]]]

The third option, emulating the preprocessor, is fraught with difficulty.
Macro definitions consist of complete tokens but need not be complete
expressions or statements.  Conditional compilation and alternative macro
definitions lead to very different results from a single original program
text.  Preprocessing also adds complexity to an implementation, which must
trade off performing preprocessing and maintaining the code in close to its
original form.  Extracting structure from macro-obfuscated source is not a
task for the faint-hearted.  Despite these problems, in many situations
only some sort of preprocessing or Cpp analysis can produce useful answers.

All three approaches would be unnecessary if programs did not use
preprocessor directives.  This is exactly what Stroustrup suggests:
\begin{quote}
  I'd like to see Cpp abolished.  However, the only realistic and
  responsible way of doing that is first to make it redundant, then
  encourage people to use the better alternatives, and {\em then\/}\,---\,years
  later\,---\,banish Cpp into the program development environment with the
  other extra-linguistic tools where it
  belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}
C++ contains features\,---\,such as constant variables, inline functions,
templates, and reference parameters\,---\,that obviate many uses of Cpp.  Thus,
translation to C++ is another path for partial elimination of Cpp.  (Macros
that cannot be eliminated might be annotated with their types or effects on
parser or program state, so that even tools which do no Cpp analysis can
operate correctly on such programs.)  This study indicates the feasibility,
and, our framework for analyzing preprocessor usage provides a basis for
the development, of an automatic translator with two attractive properties:
it will take as input C programs complete with preprocessor directives, and
it will map many\,---\,preferably most\,---\,uses of directives into C++ language
features.  (It is not practical to eliminate all uses of Cpp.  For example,
C++ currently provides no replacement for the {\tt \#include} directive, or
for stringization or pasting.)

Another niche already filled by our tool is that of a ``macro lint''
program which warns of potentially dangerous uses of Cpp.

%O'Callahan and Jackson also use type
%inference, although for program understanding rather than translation;
%they, too, apply their techniques to post-processed
%code~\cite{OCallahan-icse97}.

\subsection{Cpp: not all bad}

[[[WRITE TEXT FOR THIS.]]]

Language extensions
Can define entirely new syntax, avoid repetition, do other things the
language doesn't let you do.  [more examples here!]

No dependence on compiler:
\begin{itemize}
\item inlining (probably -- watch out for recursion, etc.)
\item constant-folding
\item fold away branches
\end{itemize}
Should trust the compiler more than the programmer most (not all) of the time

Some tools exist:
Tool examples: Emacs hide-ifdef mode

There are killer apps for Cpp: 
\begin{itemize}
 \item conditional compilation (esp. system dependencies)
 \item declarations (writing two different languages in one source code)
 \item repetitive constructs
\end{itemize}


\subsection{Outline}

In order to build a better understanding of how the preprocessor is used,
we analyzed preprocessor usage in {\numpackages} C packages comprising
{\numlines} lines of code.  Figure~\ref{fig:packages} describes the
packages and lists their sizes in terms of physical lines (or newline
characters) and non-comment, non-blank (NCNB) lines, which disregards lines
consisting of only comments or whitespace.  The remainder of the analysis
uses only the NCNB length, which more accurately reflects the amount of
source code.

\begin{figure}
\centering
{\small
  \setlength{\tabcolsep}{.25em}
  \input{package_and_sizes-tbl.tex}
}
\caption{Analyzed packages and their sizes}
\label{fig:packages}
\end{figure}

[[[DISCUSS THE CONTENTS SECTION BY SECTION, WITHOUT BULLETS.]]]
[[[This hasn't been read yet.]]]

\begin{itemize}\itemsep 0pt \parskip 0pt

\item We computed the percentage of original C source code lines that
were preprocessor directives, including a breakdown of the frequency
of specific directives such as {\tt \#define} (see
Section~\ref{sec:directives}). It was not unusual to find C programs 
in which over 10\% of the total lines were preprocessor directives, and 
over 20\% of the lines were directives in
three of the {\numpackages} packages.

\item We computed how often each macro was defined and expanded (see
Section~\ref{sec:usage}).  In general we found that identifiers are in
general {\tt \#define}d relatively few times (almost always there were
five or fewer definitions for over 90\% of the macro identifiers).  We
also found that many packages have a significant number of macros that are
never expanded, even disregarding system and library header files.


\item We categorized macro definitions according to their expansions;
for example, we determined which macros simply defined a preprocessor
symbol, which defined literals, etc.~(see
Section~\ref{sec:categorization}).  We were particularly interested in
determining the frequency of use of macros that are difficult to convert to
other language features, such as those that string together characters
(as opposed to manipulating lexemes or syntactic units) and those that
expand to partial syntactic units (for instance, unbalanced braces or
partial declarations).
We found a
small number of such macros (less than one third of a percent of all macro
definitions and less than five percent, respectively).
\end{itemize}

Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of C
programming support tools.  On the other hand, the analysis also
convinces us that, by extending our analysis framework with some class
type inferencing techniques (similar to those used by Siff and Reps for
C to C++ translation~\cite{Siff-fse96}, O'Callahan and Jackson for
program understanding~\cite{OCallahan-icse97}, and others), we can take
significant steps towards a tool that usefully converts a high
percentage of Cpp code into C++ language features.\footnote{Preliminary
  results using our tool on C++ code demonstrates that many C++ packages
  still rely on Cpp, even when C++ provides mechanisms within the
  language to support a nearly identical construct, probably due to a
  combination of trivial translations from C to C++ and of C programmers
  becoming C++ programmers without changing their habits.} We are
interested not in translations that merely allow a C program to be
compiled by a C++ compiler (which is usually easy, by intentional design
of C++) but those that take advantage of the added richness and benefits
of C++ constructs.

[[[There's both good news and bad:  most uses are ``good'', but numerically
many are ``bad''.]]]


\subsection{Who cares?}

[[[Change this into text.]]]

Language designers: see what's missing, see what people do via macros and
Cpp, figure out how to support it (or prevent it!) \\
Think more about this:  Also, how do language choices lead to more/less
tightly integrated (as opposed to open, component-based) environments?
E.G., no need for \verb|__LINE__| in Java?

Tool writers: understand how people use Cpp; learn to recognize and cope
with those constructs. \\
Translator \& other macro tool writers:  Is this a reasonable task?  Is it a
non-trivial task?

Programmers: see how to avoid constructs that give tools fits.  Or, learn
weird new tricks!

Software engineers: This has never been done before -- try it and see
what's interesting.  This may seem straightforward, but we discovered some
interesting results; both good and bad news.  [Briefly mention (lack of)
related work here?]


\section{Occurrence of Preprocessor Directives}
\label{sec:directives}

Figure~\ref{fig:directives-breakdown} shows how often preprocessor
directives appear in the programs we analyzed.  Each group of bars in the
figure represents the percentage of NCNB lines attributed to the specified
category of directives, with each individual bar showing the percentage for
a specific package.  Conditional compilation directives are grouped together, as
are ``other'' directives (such as {\tt \#error} and {\tt \#pragma}).  These
numbers do not include Cpp directives discovered in system header files,
only in files included in the package.\footnote{Only
  Figures~\ref{fig:freq-use} and~\ref{fig:where-used}, which relate to
  macro expansion rather than macro definition or general preprocesor
  usage, include macros defined in system files\,---\,though even in those
  figures, only expansions in user code, not in system headers, are counted.}

% Conditional compilation =  ({\tt \#ifdef},
% {\tt \#ifndef}, {\tt \#if}, {\tt \#endif}, {\tt \#else}, and {\tt \#elif})

\begin{figure}
\centerline{\epsfig{file=directives-breakdown.eps,height=7.5in}}
\caption{Preprocessor directives as a fraction of non-comment,
  non-blank (NCNB) lines.}
\label{fig:directives-breakdown}
\end{figure}

Overall, more than 10\% of NCNB program lines are preprocessor directives;
the percentage varies by a factor of five across packages.  Half of
the packages
have directives for over 9\% of their lines, and one in nine exceed 21\%,
indicating quite heavy use of the preprocessor.

% Figure~\ref{fig:total_directives} presents the same data arranged by
% package, to show the percentage of the total directive count attributed to
% each category of directives.

Conditional compilation directives account for just under half (47\%) of
the total directives in all packages, macro definitions comprise another
30\%, file inclusion is 19\%, macro undefinition makes up 2\%, and the
other directives are in the noise.  [[[Check these numbers; automatically
generate?]]] The directive breakdown varies by quite a bit across
packages: the percentage of {\tt \#define}s varies by a factor of 14 across
the packages, {\tt
\#include}s by a factor of 25, and conditional compilation directives by a
factor of about 16.



\subsection{{\tt \#line}, {\tt \#undef}, and ``other'' directives}

The definedness of a macro can be used as a boolean value.  However, {\tt
\#undef} is very rarely used to set
such macros to ``false''.  Most uses of {\tt \#undef} immediately precede a
definition of the just-undefined macro, in order to avoid preprocessor
warnings about incompatible macros redefinitions.  (About 90\% of glibc's
{\tt \#undef}s are used for this purpose, and 216 of the 614 {\tt \#undef}s
appear in a single file which constists of a long series of {\tt \#undef}
followed by a single {\tt \#include}.)

Every use of {\tt \#line} (in bash, cvs, flex, fvwm, gawk, groff, and perl)
appears in lex or yacc output which enables packages build on systems which
don't have lex, yacc, or their equivalents installed.  For instance, flex
uses itself to parse its input, but also includes an already-procesed
version of its input specification (that is, C code corresponding to a {\tt
.l} file) for bootstrapping.

The only significant user of ``other'' directives is the g77 package, which
contains 157 uses of {\tt \#error} to check for incompatible preprocessor
flags.


[[[FIX THIS:]]]

Flex is also unusual in its very low percentage (0.4\% of NCNB lines) of
{\tt \#include}s; zsh and rcs share this characteristic, with percentages
of 0.4\% and 0.3\% respectively.  The percentages of {\#include} with
respect to total preprocessor directives is about 4\% for flex and rcs and
about 6\% for zsh.


\subsection{Packages with heavy preprocessor use}

gzip, glibc, remind, and bash use the preprocessor most; the first three
have preprocessor directives as 21--23\% of NCNB lines.

glibc's heavy preprocessor use is largely accounted for by {\tt \#include}
directives.  Its files average just 42 NCNB lines each, and most contain
several {\tt \#include} directives.  Of the 1684 files, 182 are header files
consisting of a single {\tt \#include} line, which relieves glibc users of
the need to know in which directory a service really resides.

gzip {\tt \#define}s disproportionately many macros as literals (these
macros act like {\tt const} variables) used as arguments to system calls,
enumerated values, directory components, and more.  (This is evidence of
good programming style.)  gzip also contains many conditional compilation
directives, since low-level file operations (such as setting creation time
and access control bits, accessing directories, and so forth) are done
differently on different systems; gzip is a highly portable program.

In order to support speakers of many different languages, remind uses {\tt
\#define}d constants for basically all user output.  It also contains
disproportionately many conditional compilation directives; over half of
these test the definedness of \verb|HAVE_PROTO|, in order to provide both
K\&R and ANSI prototypes.  

Like gzip, bash is portable across a large variety of systems, but bash
uses even more operating system services.  Ninety-seven percent of bash's
conditional compilation directives test the definedness of a macro whose
presence or absence is a boolean flag indicating what features the current
system supports.  The presence or absence of a feature requires different
(or sometimes additional) system calls or other code.


\section{Frequency of Defined Macro Usage}
\label{sec:usage}

% The second question we asked was: where and how often are macros
% defined and used in practice?  

%Specifically, for each macro we
%determined:
%\begin{itemize}\itemsep 0pt \parskip 0pt
%
%\item How many times it was {\tt \#define}d.
%\item How many times it was {\tt \#undef}ed.
%\item How many arguments (if any) it takes.
%\item How many times (and where) it was mentioned in C source code.
%\item How many times (and where) it was mentioned in other
%preprocessor directives.
%
%\end{itemize}
%[FIX: gjb: This is redundant with the same information in paragraph
%form, above]

%[FIX: gjb: I'd also like to see info about how often define-d to the same
%expansion, or not, etc.]

% \begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{freq_of_def-tbl.tex}}%
% }
% \caption{Number of definitions per Cpp identifier.  The numbers in the
%   table represent the percentage of identifiers which are defined a given
%   number of times or fewer.  For example, bison contains 4 or fewer
%   definitions for 91.51\% of all Cpp macros it defines, and no
%   macro is defined 5, 6, 7, 8, or more than 9 times.}
% \label{fig:define_count}
% \end{figure}

\begin{figure}
\centerline{\epsfig{file=freq-def.eps,height=7.5in}}
\caption{Number of definitions per Cpp identifier, graphed as the
  the percentage of identifiers which are defined a given number of times
  or fewer.  [[[Check when real total data is available:]]] 
  In all the packages, 90\% of all macros were defined four or
  fewer times; the other 10\% of macros had five or more distinct
  definitions ({\tt \#define} directives).  All counts ten or greater are
  collapsed into a single bin.}
\label{fig:freq-def}
\end{figure}

Figure~\ref{fig:freq-def} graphs the number of times each identifier is
{\tt define}d in each of the packages.  No distinction is made between
sequential redefinitions of a macro and mulitple definitons that cannot
take effect in a single configuration (because they appear in different
branches of a Cpp conditional or are otherwise guarded to prevent conflicts).

This graph shows a great deal of variation.  All macros defined by bc have
only one or two different expansions, but more than 50\% of all macros
defined by remind expand to more than eight different texts.

In all but three packages, at least 87\% of all macros are defined five or
fewer times.  In gawk and glibc, such macros represent 83\% and 81\% of all
macros, largely because both packages are highly portable and also quite
dependent on system libraries.  The remind program uses macro definitions
to provide localization support for ten different natural languages (and
multiple character sets for some of them), accounting for the surprisingly
large number of macros with many definitions.  All of remind's macros are
defined 14 or fewer times, but 16 macros in the {\numpackages} packages are
defined more than 16 times, including three with more than 30 distinct
definitions.

That there is a fairly limited use of multiple definitions of symbols
allows us to more reasonably bound the analysis needed to determine
the interactions among the definitions for a given symbol.

\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{freq_of_use-tbl.tex}}%
% }
\centerline{\epsfig{file=freq-use.eps,height=7.5in}}
\caption{Number of expansions per Cpp macro.  The numbers in the
  table represent the percentage of identifiers which are expanded a given
  number of times or fewer.  For example, fvwm expands 49.85\% of its
  macros two or fewer times.}
\label{fig:freq-use}
\end{figure}

Figure~\ref{fig:freq-use} is structured as the previous figure, but it
represents the number of times that a defined name is expanded in either C
code or preprocessor directives.  About 80\% of all macros were expanded
eight or fewer times.

It is notable that most packages contain a significant number of defined
macros that are never expanded.  (Figure~\ref{fig:freq-use} reports only
on macros defined in a package, not those defined in system or library
header files, inclusion of which pushes the unused percentage well above 50\%.)
Most packages are in the 3-10\% range, while gnuplot
reaches nearly 40\%.  Although it's difficult to fully account for the
larger numbers, contributing factors include a lot of cut-and-paste
and a lack of attention to implementations for specific platforms in
some packages.  Again with respect to bounding analysis, covering
macros with 10 or fewer uses covers approximately 75-80\% of the cases.

\begin{figure}
\centerline{\epsfig{file=where-used.eps,height=6in}}
%{\small
%  \setlength{\tabcolsep}{.25em}
%  \input{where_uses-tbl.tex}%
%}
\caption{Where macros are used.  This figure includes uses of
  system-defined macros as well as those defined in the package.}
\label{fig:where-used}
\end{figure}

Figure~\ref{fig:where-used} shows the percentage of defined macros
that are used in C code, in Cpp code, in both, or in neither (i.e., no
uses).  No package expanded all of its defined macros; two did not
expand as many as 40\% of the defined macros.  The dominant usage was
in C code only; these uses don't, therefore, have any affect on
conditional compilation (for example).  


\section{Categorization}
\label{sec:categorization}

The third question we asked was: in what ways are the macros used in
practice?  In contrast to the earlier analyses, this requires an
analysis and categorization of the bodies of the macros to determine
how they can be used.  This analysis depends in part on some
heuristics we defined to interpret the bodies of the macros; by
studying our analyses, we have refined (and continue to refine) the
heuristics to do a better job of categorization.  A straightforward
refinement that we are pursuing looks at the specific uses of macros
to help in this categorization.

In addition to classifying each macro as taking arguments or not, our tool
identifies the following specific categories (and a few more rarely-used
ones omitted for reasons of space):
\begin{description}
  \sloppy
  \emergencystretch=2em

\item[Null define]  The {\tt \#define} gives only an
identifier name.  Such macros appear most frequently in Cpp directives
(such as {\tt \#ifdef}), but may also appear in code.  For instance,
macro {\tt private} may expand either to {\tt static} or to nothing,
depending on whether a debugging mode is set, or macro {\tt then} might
expand to nothing for programmers who like {\tt else} to be offset by
{\tt then}.\footnote{Recall that we are empirically studying the way
in which macros are used rather than assessing the desirability of any
particular usage style.}  Macros with null definitions are often used as boolean
variables by the preprocessor.

\item[Literal] The macro is defined to be a specific constant value;
  for instance, {\tt \#define NULL 0} or 
  {\tt \#define ETCHOSTS "/etc/hosts"}.  Such macros act like {\tt const}
  variables, albeit 
  without type information, debugging information in the symbol table,
  and so forth.  Macros with constant expression bodies, such as
  \verb|#define| \verb|RE_DUP_MAX| \verb|((1<<15)-1)|, are included, but
  not macros referencing other literal or constant macros, so this number
  underestimates the number of macros with constant values.

\item[Expression]  The macro body is an expression.  This expression
  might have a single constant value everywhere (the usual case for
  expression macros without arguments), or might have a different value on
  each use (the usual case for expression macros with arguments).  Of
  expression macros, 6.31\% use assignment operators,
  which have potentially unexpected results.  A macro argument that is
  assigned to is like a pass-by-reference function argument and need only
  be noted in the macro's documentation.  A global variable that is
  assigned to also presents no difficulties in understanding.  Assignment
  to a local variable, however, demands that such a variable exist wherever
  the macro is invoked, and assigns to different variables at different
  invocations.\footnote{By contrast, LCLint considers assignment to a macro
  argument dangerous but does not appear to check for assignments to local
  variables.}

%    * expression sans assignment
%    * expression with assignment

\item[Statement]  The macro body is a statement such as ``{\tt x = 3;}'' or
  ``{\tt \verb|{| int x = y*y; printf("\%d", x); \verb|}|}''.  A use of such
  a macro should not be followed by a semicolon; since the body is
  already a complete statement, the extra semicolon can cause problems such as 
  mis-parsing of nested {\tt if} statements.  Such macros can be
  confusing to use, because programmers are inclined to add a semicolon
  after invocations that look like functions;   the
  well-known \verb|do {|\ldots\verb|} while (0)| partial statement trick
  solves this problem.  To our surprise, we found very few uses of that
  construct, but many instances of a macro that expanded to a statement
  like \verb|{|\ldots\verb|}| in which a call to the macro was immediately
  followed by a semicolon.

\item[Stringization and pasting]  The macro body contains {\tt \#} or
  {\tt \#\#}, which treat the macro argument not as a token but as a
  string.  No existing language mechanism can replace such macros.  Our
  implementation separates these categories, but we combine them to
  simplify our figures.

\item[Other syntactic macros]  Like stringization and pasting, these
  macros make essential use of the unique features of the preprocessor.
  Our framework separately categorizes a number of such macros, including
  those that expand to a reserved word (such as {\tt \#define private
  static}, mentioned above), those which expand to a delimiter such as
  {\tt ,} or {\tt ;}, and those with mismatched parentheses, brackets, or
  braces.  (The latter are often used to create a block and perform actions
  that must occur at its beginning and end, such as \verb|BEGIN_GC_PROTECT| and
  \verb|END_GC_PROTECT|.)

\item[Classification failure]  Frequent reasons that our
classification heuristics failed
  include multiple adjacent identifiers
  ({\tt \#define EXFUN(name, proto) name proto},
  {\tt \#define \verb|DO_OP|(OP,a,b) (a OP b)}) and illegal identifier
  names ({\tt \#define \verb|FAT$C_VFC| 3} for VMS compilation), tokens %$HACK
  ({\tt \#define \verb|LIB_PATH| /usr/ucblib}), and constants ({\tt 1ULL} for
  {\tt unsigned long long}).  Of the fewer than 1000 classification
  failures in the {\numpackages} packages, over half appeared in gnuplot; gnuplot, cvs,
  and groff accounted for over 75\% of the failures, many of which could be
  eliminated by slightly relaxing the parsing rules.  Only five packages had no
  macro classification failures at all.


% \item {\em Constants\/}, where the {\tt \#define} gives only an
% identifier name and a value.
% 
% \item {\em Null defines\/}, where the {\tt \#define} gives only an
% identifier name.
% 
% \item {\em Functions\/}, where the {\tt \#define} has one or more
% parameters.  The functions are broken down into several sub-categories:
% \begin{itemize}
% 
% \item {\em Functions/Cpp\/}, where the body of the macro invokes one
% or more other macros.  [FIX: This is currently ``function, macro as
% function'', I think.]
% 
% \item {\em Functions/C-expression\/}, where the body of the macro is
% defined as a well-defined C expression.  [FIX: This is currently
% ``function, some constant'', I think.]
% 
% \item {\em Functions/Essential\/}, [FIX: bad name?] where the body of the
% macro is not defined as a C expression; specific reasons include
% ...
% 
% These are of special interest with respect to translation.  Some of
% them, for instance ones that define C statements rather than
% expressions, might yield to translation if further analysis shows that
% they are used in particular, disciplined ways.
% 
% \end{itemize}
% 
% \item{\em Uncategorized\/}, where we could not characterize the macro
% as any of the above categories.

\end{description}

Figure~\ref{fig:categorization} shows the percentage of macros that
fit into these categories for each package.  The null define and
literal categories together account for nearly 60\% of all
macro definitions; converting these into C++ constructs
should be simple.  The extremely small percentage (0.33\%) of
``essential'' macros\,---\,those such as stringization and pasting that
have no simple mapping into any programming language\,---\,is encouraging;
these would have to be studied individually, but the overall cost
may well be low.  Further analysis of the conditional compilation
structure (in the style of Krone and Snelting~\cite{Krone94}) and of
the macros with free variables (essentially achieving dynamic scoping)
is needed to see which of the roughly 33\% of expression macros should
be easy to handle in a conversion process.

% The following isn't quite enough to get the columns lined up in this table.
% \newcolumntype{d}{D{.}{.}{2}}
% \begin{tabular}{|l|d|d|d|d|d|d|d|}\hline
\begin{figure}
% {\small
%   \setlength{\tabcolsep}{.25em}
%   \centerline{\input{categories-tbl.tex}}%
% }
\centerline{\epsfig{file=def-categories.eps,height=6in}}
\caption{Categorization of macro definition bodies.}
\label{fig:categorization}
\end{figure}



%In anticipation of the translator tool, the analysis tool infers
%types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
%and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
%type information is in the early stages, however, and we do not report
%on the preliminary results in this paper.

% [FIX: Benefits even from simple literal constant conversion -- exposes
% symbolic information to the debugger]

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.]


\begin{figure}
\centerline{\input{tbl-subset-categories.tex}}
\caption{Subset categorization of macros (not macro definitions).   Items
  less than one twentieth of a percent are omitted; such items appear
  fewer than ten times in the codebase.}
\label{fig:subset-categories}
\end{figure}
 



\section{Related Work}
\label{sec:related}

We could find no other empirical study of the use of the C preprocessor nor
any other macro processor.  However, a number of other efforts provide
guidance on how to use C macros effectively, provide tools for
checking macro usage for given programs, etc.

Carroll and Ellis state that ``almost all uses of macros can be
eliminated from C++ libraries''~\cite[p.~146]{Carroll95}. 
They list eight categories of macro usage and explain how to convert
them into C++ mechanisms.  They do not
discuss automatic conversion, but  focus on instructing the
software engineer on better ways to do Cpp-like things.

Similarly, a number of organizations provide hints about effective
ways to the use the C preprocessor.  The GNU documentation, for example,
discusses a set of techniques including simple macros, argument
macros, predefined macros, stringification macros, concatenation
macros, undefining and redefining
macros.
It also identifies a set of ``pitfalls and subtleties of
macros''; these are much like some of the problems our analysis tool
identifies, as discussed below.  Our effort not only categorizes
problems, but it also determines the frequency of appearance of those
problems and discovers other idiosyncratic uses.

A number of tools check whether specific C programs
satisfy particular constraints.  The lint program checker, distributed
with most Unix systems, checks for potentially problematic uses of C\@.
The implementation of lint is complicated by the fact
that it tries to replicate significant functions of both the C
compiler and the preprocessor.

LCLint performs many of lint's checks and also
allows the programmer to add annotations which enable additional
checks~\cite{Evans-pldi96,Evans-fse94}.
LCLint's  optionally checks  on function-like
macros\,---\,that is, those which take arguments\,---\,include checking for
macro arguments on the left hand side of assignments, checking for statements
playing the role of expressions, and checking for consistent return types.
%\begin{quote}
%$\ldots$ a parameter to a macro may not be used as the left hand side
%of an assignment expression $\ldots$, a macro definition must be
%syntactically equivalent to a statement when it is invoked followed by
%a semicolon $\ldots$, the type of the macro body must match the return
%type of the corresponding function $\ldots$\footnote{From Section 8 of
%David Evan's LCLint User's Guide, Version 2.2 (August 1996); larch-www.lcs.mit.edu:8001/\discretionary{}{}{}larch/\discretionary{}{}{}lclint/\discretionary{}{}{}guide/\discretionary{}{}{}guide.html}
%[FIX: That footnote should really be a reference instead.]
%\end{quote}
%[Fix: that quotation is really easy to nitpick:
% 1) assignment parameters is fine (just turn into a reference argument) but
%    assignment of non-parameters that aren't at global scope is quite bad.
% 2) in x=foo(); we do NOT want foo() to be a statement
% 3) a macro doesn't have a single type, but may have many polymorphic types
%Should we mention these things?  I don't want to seem nitpicky or petty.]
LCLint's approach is prescriptive: programmers are encouraged not to use
constructs that might be dangerous, or to change code that contains such
constructs.  We are more interested in analyzing, describing, and
automatically removing such uses so that tools can better process existing
code without requiring human interaction or producing misleading results.

%% FIX: If we could also list the platforms for which each can compile,
%% that would be great, but I doubt the benefit is worth the effort for now

Krone and Snelting use mathematical foundations analysis to determine the
conditional compilation structure of code~\cite{Krone94}.  They determine,
for each line, which preprocessor macros it depends upon, and display that
information in a lattice.  They do not determine how macros depend upon one
another directly, only by their nesting in {\tt \#if}, and the information
conveyed is about the program as a whole.


\section{Conclusion}
\label{sec:discussion}

Even with significant expertise in using C, C++, and the Cpp macro
preprocessor, these data showed us a far broader and diverse use of
preprocessing than we had anticipated.  

We wrote a collection of Perl scripts to produce and analyze these data.
As we developed them, we slowly shifted from a lexical analysis of the
source to a syntactic analysis of the source.  For example, we now properly
parse virtually all declarations in every package.  We're pursuing even
more analysis, pushing into semantics.  For instance, we can now determine
which identifiers are free variables within a defined macro.  We are also
close to inferring the types of macro bodies, one of several steps
necessary before we can attempt to build a converter from C to C++ which
eliminates most macro usages (except for {\tt \#include}).  Such a
translator would both enable programmers to shift to C++ and its constructs
more easily and simplify the development of source-level debuggers,
call graph extractors, and other programming support tools.

%[FIX: We catch errors that you miss when you look at post-processed code
%since we look at all branches.]
%
%[FIX: Harsh macro examples?]

% Not really right:  Don't want the ``References'' section head to be small.
{\small \bibliography{evil}}

\end{document}
