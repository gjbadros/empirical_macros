% $Id$
% macros-paper.tex
% FIX: Switch to LaTeX2e, not 209
\documentstyle[11pt,epsf]{article}

\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin 0pt
\marginparsep 0pt

\topmargin   0pt

\textwidth   6.5 in
\textheight  8.5 in

\begin{document}
\bibliographystyle{plain}

\title{An Empirical Analysis of the Use of the C Preprocessor}

\author{Michael Ernst\thanks{Email 
addresses: {\tt \{mernst, gjb, notkin\}@cs.washington.edu}.  Contact author:
mernst@cs.washington.edu.}
\and Greg J. Badros\thanks{Supported by a National Science Foundation
    Graduate Fellowship. Any opinions, findings, conclusions, or
    recommendations expressed in this publication are those of the
    author, and do not necessarily reflect the views of the National
    Science Foundation.} \and David Notkin}

\date{Department of Computer
Science and Engineering\\
University of Washington\\
Box 352350\\
Seattle, WA  98195-2350\\
\today}  

\maketitle


\begin{abstract}

  The C programming language is intimately connected to its macro
  preprocessor.  This relationship affects, indeed generally hinders, both
  the tools (compilers, debuggers, call graph extractors, etc.)\ built to
  engineer C programs and also the ease of translating to other languages
  such as C++.  In this paper we analyze over 20 packages comprising almost
  one million lines of publicly available C code, determining the ways in
  which the preprocessor is used in practice.  We developed a framework for
  analyzing preprocessor usage, using it to extract information about the
  percentage of preprocessor directives in C programs, the frequency of use
  of defined macros, the relationship between C functions and their use of
  macros, and the macros that are difficult to express in terms of other C
  or C++ language features.  We report on the analysis, in particular
  illustrating data that are material when considering the definition of
  tools for C or C++.  The results (and the supporting framework) lay the
  foundation for developing a tool to reduce usage of the preprocessor by
  translating from C to C++.

\end{abstract}

\bigskip

\section{Introduction}

The C programming language~\cite{ansi} is intimately connected to its macro
preprocessor, Cpp~\cite[Ch.3]{Harbison91}.  Preprocessor facilities such as
file inclusion, defining constants and macros, and conditional compilation
are present in essentially every C program.   
While the preprocessor can be used in a simple or well-structured way to
improve portability, performance, or readability, it also lends itself to 
arbitrary source code manipulations that complicate understanding of the
program by both software engineers and tools.
This relationship
between the C language and the preprocessor has a number of consequences,
many of which were probably not anticipated by the original designers.

Stroustrup puts the consequences of this relationship in perspective:
\begin{quote}
Occasionally, even the most extreme uses of Cpp are useful, but its
facilities are so unstructured and intrusive that they are a constant
problem to programmers, maintainers, people porting code, and tool
builders~\cite[p.~424]{Stroustrup-DesignEvolution}.
\end{quote}

Tools may ignore macro definitions altogether, accept only preprocessed
code (perhaps by running Cpp on their input), or attempt to emulate the
preprocessor.

Ignoring preprocessor directives is an option for approximate tools, but
accurate information about function extents, scope nesting, declared
variables and functions, and so forth requires addressing the preprocessor.

Operating on preprocessed code, the most common strategy, is simple to
implement, but then the tool's input differs from what the
programmer sees.  Even when line number mappings are maintained, other
information is lost in the mapping back to the original source code.
For instance, source-level debuggers have no symbolic names or types
for constants introduced via \verb|#define|.  As another example, Siff
and Reps describe a technique that uses type inferencing to produce
C++ function templates from C; however, the input is ``a C program
component that $\ldots$ has been preprocessed so that all include
files are incorporated and all macros
expanded~\cite[p.~145]{Siff-fse96}.''  Such preprocessing may limit
the readability and reusability of the resulting C++ templates.  As
yet another related example, call graph extractors generally work in
terms of the post-processed code, even when a human programmer---as
opposed to an optimizing compiler, for example---is the intended
consumer of the call graph~\cite{Murphy-icse18}.  Some tools even
leave the software engineer responsible for inferring the mapping between the
original and the post-processed source, which is an undesirable and
error-prone situation.

A tool that first preprocesses code, or insists on preprocessed code as
input, cannot be run on a non-syntactic program or one that will not
preprocess (or produces nonsense) on the system on which the tool is being
run.  These constraints complicate porting and maintenance, two of the
situations in which program understanding and transformation tools are most
likely to be desired.  Additionally, a tool supplied with only one preprocessed
instantiation of the source code cannot reason about the program as a
whole, only that version that results from one particular set of
preprocessor variables.  (For instance, a bug in one configuration may not
be discovered despite exhaustive testing of other configurations that do
not incorporate particular code or do not admit particular paths through
the code.)

The third option, emulating the preprocessor, is fraught with difficulty.
Macro definitions consist of complete tokens but need not be complete
expressions or statements.  Conditional compilation and alternative macro
definitions lead to very different results from a single original program
text.  Tradeoffs must be made between performing preprocessing and
maintaining the code in as close to its original form as possible.
Preprocessing also adds complexity to an implementation.  Despite these
problems, in many
situations only performing some sort of preprocessing can produce correct
answers.

All three approaches would be unnecessary if programs did not use
preprocessor directives.  This is exactly what Stroustrup suggests:
\begin{quote}
  I'd like to see Cpp abolished.  However, the only realistic and
  responsible way of doing that is first to make it redundant, then
  encourage people to use the better alternatives, and {\em then\/}---years
  later---banish Cpp into the program development environment with the
  other extra-linguistic tools where it
  belongs~\cite[p.~426]{Stroustrup-DesignEvolution}.
\end{quote}
C++ contains features---such as constant variables, inline functions,
templates, and reference arguments---that obviate many uses of Cpp.  Thus,
translation to C++
is another path for partial elimination of Cpp.  

%O'Callahan and Jackson also use type
%inference, although for program understanding rather than translation;
%they, too, apply their techniques to post-processed
%code~\cite{OCallahan-icse97}.

As a step toward such preprocessor elimination, we empirically analyzed
preprocessor usage in 23 C packages comprising nearly one million lines of
code.  Section~\ref{sec:gathering} lists the packages and their sizes.  The
analysis is intended to build a better understanding of how the
preprocessor is used.  In turn, this will help us to build a C to C++
conversion tool with two attractive properties: it will take as input C
programs complete with preprocessor directives, and it will map
many---preferably most---uses of directives into C++ language
features.\footnote{It is not practical to eliminate all uses of Cpp.  For
  example, C++ currently provides no replacement for the \verb+\#include+
  directive.} Our framework for analyzing preprocessor usage provides a
basis for the development of such a conversion tool.

[fix: is this list complete?]
This paper reports on several specific analyses we performed.
\begin{itemize}\itemsep 0pt \parskip 0pt

\item We computed the percentage of original C source code lines that
were preprocessor directives, including a breakdown of the frequency
of specific directives such as \verb+#define+ (see
Section~\ref{sec:directives}). It was not unusual to find C programs 
in which over 10\% of the total lines were preprocessor directives, and 
three of the 23 packages had over 20\% directives.

\item We computed how often each macro was
defined,  undefined, and
were expanded both from other preprocessor directives and also from
true C source code (see Section~\ref{sec:usage}).
[FIX: pithy observation goes here]

\item We categorized macro definitions according to their expansions; 
      for example, we determined which macros simply
      defined a preprocessor symbol, which defined constants, etc.~(see
      Section~\ref{sec:categorization}).  We
      were particularly interested in determining which macros are
      difficult to convert to other language
      features.  [FIX: pithy observation goes here]

\end{itemize}

Overall, our analysis confirms that the C preprocessor is used in
exceptionally broad and diverse ways, complicating the development of
C programming support tools.  On the other hand, the analysis also
convinces us that, by extending our analysis framework with some class
type inferencing techniques (similar to those used by Siff and Reps
for C to C++ translation~\cite{Siff-fse96}, O'Callahan and Jackson for
program understanding~\cite{OCallahan-icse97}, and others), we can
take significant steps towards a tool that usefully converts a high
percentage of Cpp code into C++ language
features.\footnote{Preliminary results using our tool on C++ code
demonstrates that many C++ packages still rely on Cpp, even when C++
provides mechanisms within the language to support a nearly identical
construct, probably due to a combination of trivial
translations from C to C++ and of C programmers becoming C++
programmers without changing their habits.}  We are
interested not in translations that merely allow a C program to be
compiled by a C++ compiler (which is usually easy, by intentional
design of C++) but those that take advantage of the added richness of C++
constructs.

%[FIX: Two important points to address: C code almost *is*
%C++ code, so what do we gain from a ``conversion''?  If we can't get all
%the code, can we identify the parts we can't get?
%gjb thinks this is more for the next paper-- we don't need to get into
%it here]

\section{Related Work}\label{sec:related}

We could find no empirical study of the use of the C preprocessor nor
any other macro processor.  However, a number of other efforts provide
guidance on how to use C macros effectively, provide tools for
checking macro usage for given programs, etc.

Carroll and Ellis state that ``almost all uses of macros can be
eliminated from C++ libraries.''~\cite[p.146]{Carroll95} 
They list eight categories of macro usage and explain how to convert
these types of constructs into C++ mechanisms.  They do not
discuss automatic conversion, but  focus on instructing the
software engineer on better ways to do Cpp-like things.

Similarly, a number of organizations provide hints about effective
ways to the use the C preprocessor.  The GNU documentation, for example,
discusses a set of techniques including simple macros, argument
macros, predefined macros, stringification macros, concatentation
macros, undefining and redefining
macros.
It also identifies a set of ``pitfalls and subtleties of
macros''; these are much like some of the problems our analysis tool
identifies, as discussed below.  Our effort not only categorizes
problems, but it also determines the frequency of appearance of those
problems and discovers other idiosyncratic uses.

A number of tools check whether specific C programs
satisfy particular constraints.  The lint program checker, distributed
with most Unix systems, checks for uses of C that are likely to be
problematic.  The implementation of lint is complicated by the fact
that it tries to replicate significant functions of both the C
compiler and also the preprocessor.

LCLint, a recently developed tool
at MIT, handles many of the checks that lint does; furthermore, LCLint
allows the programmer to add annotations that then permit additional
checks to be performed~\cite{Evans-fse94,Evans-pldi96}.
Examples of checks that LCLint can optionally perform on function-like
macros---that is, those which take arguments---include checking for
macro arguments on the left hand side of assignments, checking for statements
playing the role of expressions, checking for consistent return types, etc.
%\begin{quote}
%$\ldots$ a parameter to a macro may not be used as the left hand side
%of an assignment expression $\ldots$, a macro definition must be
%syntactically equivalent to a statement when it is invoked followed by
%a semicolon $\ldots$, the type of the macro body must match the return
%type of the corresponding function $\ldots$\footnote{From Section 8 of
%David Evan's LCLint User's Guide, Version 2.2 (August 1996); larch-www.lcs.mit.edu:8001/\discretionary{}{}{}larch/\discretionary{}{}{}lclint/\discretionary{}{}{}guide/\discretionary{}{}{}guide.html}
%[FIX: That footnote should really be a reference instead.]
%\end{quote}
%[Fix: that quotation is really easy to nitpick:
% 1) assignment parameters is fine (just turn into a reference argument) but
%    assignment of non-parameters that aren't at global scope is quite bad.
% 2) in x=foo(); we do NOT want foo() to be a statement
% 3) a macro doesn't have a single type, but may have many polymorphic types
%Should we mention these things?  I don't want to seem nitpicky or petty.]
LCLint's approach is prescriptive: programmers are encouraged not to use
constructs that might be dangerous, or to change code that contains such
constructs.  We are more interested in analyzing, describing, and
automatically removing such uses so that tools can better process existing
code without requiring human interaction or producing misleading results.


\section{The Programs}\label{sec:gathering}

Figure~\ref{fig:packages} shows the packages we analyzed, along with their
sizes.  For each package we also report the physical line count---the
number of lines (or newline characters) in the file---and the non-comment,
non-blank (NCNB) line count, which disregards lines consisting of only
comments or whitespace.  The remainder of the analysis uses only the NCNB
length, since that focuses more clearly on the essential source text of a
package.

\begin{figure}
\input{package_and_sizes-tbl.tex}
\caption{Analyzed Packages and Sizes\label{fig:packages}}
\end{figure}

%% FIX: If we could also list the platforms for which each can compile,
%% that would be great, but I doubt the benefit is worth the effort for now

\section{Occurrence of Preprocessor Directives}\label{sec:directives}

[fix: I hate these detailed explanations of figures.  It would go better in
a caption, but may not be needed at all.]
[fix: this is a really bizarre ordering.  Should go in approximate size
order: define, conditionals, etc.]

The first question we asked was simple: how often do
preprocessor directives appear in C programs?
Figure~\ref{fig:directives} shows the data for the packages we
analyzed.  There are five rows in the figure, each of which has a
separate bar for each of the packages we analyzed.  The topmost row
shows the total percentage of lines in the package that represent
preprocessor directives.  The rows below the topmost one show the
percentage of lines in the package attributed to a subset of
preprocessor directives: (a) \verb+#undef+ is the next row down, (b)
\verb+#include+ is the next row down, (c) \verb+#define+ is the next
row down, (d) the conditional compilation directives (\verb+#ifdef+,
\verb+#ifndef+, \verb+#if+, \verb+#endif+, \verb+#else+, and
\verb+#elif+) are combined into a single group shown in the next row,
and (e) any other directives in the bottom row. 

\begin{figure}
goes here [FIX: blank page for insertion?]
\caption{Percentages of Directives\label{fig:directives}}
\end{figure}

Perhaps the most surprising information here is the percentage of the
NCNB program lines that are preprocessor directives: the number varies
by about a factor of five across packages.  About one third of the
packages have directives for over 10\% of their lines, and about one
out of eight exceed 20\%.  On average, conditional compilation
directives account for just under half (47.4\%) of the total
directives in all packages, \verb+#define+s account for another
29.5\%, \verb+#include+s account for 18.9\%, and the other directives
are in the noise.\footnote{Only the flex package had a significant
``other'' component, accounted for by its aggressive use of the
\verb+\#line+ directive.} The percentage of \verb+#define+s varies by a factor of 14
across the packages, \verb+#include+s by a factor of 25, and
conditional compilation directives by a factor of about 16.

\section{Frequency of Defined Macro Usage}\label{sec:usage}

The second question we asked was: where and how often are macros
defined and used in practice?  Specifically, for each macro we
determined:
\begin{itemize}\itemsep 0pt \parskip 0pt

\item How many times it was \verb+#define+d.
\item How many times it was \verb+#undef+ed.
\item How many arguments (if any) it takes.
\item How many times (and where) it was mentioned in C source code.
\item How many times (and where) it was mentioned in other
preprocessor directives.

\end{itemize}
[FIX: gjb: This is redundant with the same information in paragraph
form, above]

[FIX: gjb: I'd also like to see info about how often define-d to the same
expansion, or not, etc.]


We display some of these data here, in several different forms.
Figure~\ref{fig:define_count} shows a histogram with cumulative
percentages of the number of times each identifier is \verb+define+d
in each of the packages.  The leftmost column lists the number of
times an identifier is \verb+define+d, with the one through ten counts
listed explicitly and all counts greater than ten collapsed into a
single bin.  For example, looking at the \verb+bc+ package, 98.72\% of
the identifiers were \verb+#define+d between one and six times.

\begin{figure}
[FIX]
%\input{freq_of_def-tbl.tex}
\caption{Frequency of Definition\label{fig:define_count}}
\end{figure}

The \verb+#undef+ counts are generally extremely low; the maximum
number of undefines seen in any package was [FIX: fill in here].

Figure~\ref{fig:use_count} is structured as the last figure, but it
represents the number of times that a defined name is expanded (in
either C code or preprocessor directives).

\begin{figure}
x
%\input{freq_of_use-tbl.tex}
\caption{Frequency of Macro Expansions\label{fig:use_count}}
\end{figure}

Figure~\ref{fig:define_usage} shows the percentage of defined macros
that are used in C code, in Cpp code, in both, or in neither (i.e. no
uses).

\begin{figure}
x
%\input{where_uses-tbl.tex}
\caption{Where Macro Uses Occur\label{fig:define_usage}}
\end{figure}


\section{Categorization}\label{sec:categorization}

The third question we asked was: how are the macros used in practice?
In contrast to the earlier analyses, this requires a analysis and
categorization of the bodies of the macros to determine how they can
be used.  This analysis depends in part on some heuristics we defined
to interpret the bodies of the macros; by studying our analyses, we
have refined (and continue to refine) the heuristics to do a better
job of categorization.  (This improvement is important in helping us
produce the C to C++ translation tool we have mentioned.)

[FIX: We should note that we plan to look at uses of macros to help in
this categorizations (though we don't do any of this now)]

In addition to classifying each macro as taking arguments or not, our tool
identifies the following specific categories (and a few more rarely-used
ones omitted for reasons of space):
\begin{itemize}

\item[{\em Null define\/}]  The \verb+#define+ gives only an
identifier name.  Such macros appear most frequently in Cpp directives
(such as \verb|#ifdef|), but may also appear in code.  For instance,
macro \verb|private| to expand either to \verb|static| or to nothing,
depending on whether a debugging mode was set, or macro \verb|then| might
expand to nothing for programmers who like \verb|else| to be offset by
\verb|then|.  Macros with null definitions are often used as boolean
variables by the preprocessor.

\item[{\em Literal\/}]  The macro is defined to be a specific constant
  value; for instance, \verb|#define NULL 0| or \verb|#define ETCHOSTS
  "/etc/hosts"|.  Such macros act like \verb|const| variables, albeit
  without type information, debugging information in the symbol table, and
  so forth.  Macros with constant expression bodies, such as \verb|#define
  RE_DUP_MAX ((1<<15)-1)|, are included here, but not macros with arguments
  that are literals or constants, so this number is an underestimate.

COPY_BUFFER_SIZE (32 * 512)

\item[{\em Expression}]  The macro body is an expression.  This expression
  might be 

A constant value appears 

* null defines
* expressions
   * literal, constant expression
   * expression
%    * expression sans assignment
%    * expression with assignment
* statements
* other:
   * stringization, pasting
   * syntactic entity, reserved word, mismatched parentheses
%    * stringization
%    * pasting
%    * syntactic entity
%    * reserved word
%    * mismatched parentheses
* classification failure

\item {\em Constants\/}, where the \verb+#define+ gives only an
identifier name and a value.

\item {\em Null defines\/}, where the \verb+#define+ gives only an
identifier name.

\item {\em Functions\/}, where the \verb+#define+ has one or more
parameters.  The functions are broken down into several sub-categories:
\begin{itemize}

\item {\em Functions/Cpp\/}, where the body of the macro invokes one
or more other macros.  [FIX: This is currently ``function, macro as
function'', I think.]

\item {\em Functions/C-expression\/}, where the body of the macro is
defined as a well-defined C expression.  [FIX: This is currently
``function, some constant'', I think.]

\item {\em Functions/Essential\/}, [FIX: bad name?] where the body of the
macro is not defined as a C expression; specific reasons include
...

These are of special interest with respect to translation.  Some of
them, for instance ones that define C statements rather than
expressions, might yield to translation if further analysis shows that
they are used in particular, disciplined ways.

\end{itemize}

\item {\em Uncategorized\/}, where we could not characterize the macro
as any of the above categories.

\end{itemize}

In anticipation of the translator tool, the analysis tool infers
types, using techniques similar to those of Siff and Reps~\cite{Siff-fse96}
and O'Callahan and Jackson~\cite{OCallahan-icse97}.  Our use of the
type information is in the early stages, however, and we do not report
on the preliminary results in this paper.

[FIX: Benefits even from simple literal constant conversion -- exposes
symbolic information to the debugger]

%[FIX: Should this also include Mike's manual breakdown into categories
%for gzip.  --not tonight!]

\section{Discussion}\label{sec:discussion}

Even with significant expertise in using C, C++, and the Cpp macro
preprocessor, these data showed us a far broader and diverse use of
preprocessing than we had anticipated.  

We wrote a collection of Perl scripts to produce and analyze these
data.  As we developed them, we slowly shifted from a lexical analysis
of the source to a syntactic analysis of the source.  For example, we
now properly parse virtually all declarations in every package.  We're
pursuing even more analysis, pushing into semantics.  For instance, we
can now determine which identifiers are free variables within a defined
macro.  We are also close to being able to infer the types of the bodies
of defined macros, one of several steps necessary before we can attempt
to build a converter from C to C++.

[FIX: We catch errors that you miss when you look at post-processed code
since we look at all branches.]

[FIX: Harsh macro examples?]

\section{Conclusion}\label{sec:conclusion}

\small \bibliography{evil}

\end{document}
